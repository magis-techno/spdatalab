# Build Dataset 性能优化

## 🎯 优化目标
解决 `build_dataset` 处理400+数据集文件耗时过长的问题。

## ✨ 优化内容

### 1. **并行处理** - 核心优化
- 使用 `ThreadPoolExecutor` 替代串行处理
- 最多16个线程同时处理文件
- 适合I/O密集型的文件读取任务

### 2. **文件级缓存** - 避免重复计算
- scene_id提取结果本地缓存到 `.cache/scene_ids/`
- 基于文件路径的MD5哈希作为缓存键
- 重复构建时直接从缓存读取

### 3. **进度显示** - 改善用户体验
- 集成 `tqdm` 进度条（可选依赖）
- 实时显示处理进度
- 无tqdm时优雅降级到日志输出

## 📊 性能提升预期

| 场景 | 优化前 | 优化后 | 提升倍数 |
|------|--------|--------|----------|
| 400个文件首次构建 | ~3小时 | ~15分钟 | **12x** |
| 400个文件重复构建 | ~3小时 | <1分钟 | **200x+** |
| 100个文件首次构建 | ~45分钟 | ~4分钟 | **11x** |

## 🚀 使用方法

### 基本用法（不变）
```bash
# JSON格式输入
python -m spdatalab.cli build-dataset \
    --training-dataset-json training_dataset.json \
    --output datasets/dataset.json

# txt格式输入
python -m spdatalab.cli build-dataset \
    --index-file data/index.txt \
    --dataset-name "Dataset" \
    --output datasets/dataset.json
```

### 推荐安装（提升体验）
```bash
# 安装进度条支持
pip install tqdm
```

## 🔧 技术细节

### 代码改动量
- **总改动**: ~50行代码
- **新增导入**: 4个模块
- **新增方法**: 1个辅助方法
- **修改方法**: 2个核心方法

### 缓存机制
- **缓存位置**: `.cache/scene_ids/`
- **缓存格式**: pickle序列化
- **缓存键**: MD5(文件路径)
- **缓存策略**: 永久缓存，手动清理

### 并行策略
- **线程池大小**: min(16, 文件数量)
- **适用场景**: I/O密集型文件读取
- **内存使用**: 线性增长，可控范围

## 🧪 测试验证

运行性能测试脚本：
```bash
python test_build_dataset_performance.py
```

## 📝 注意事项

### 优势
- ✅ 代码改动最小
- ✅ 向后兼容
- ✅ 优雅降级（可选依赖）
- ✅ 显著性能提升

### 限制
- ⚠️ 缓存需要磁盘空间（通常很小）
- ⚠️ 首次运行仍需完整处理
- ⚠️ 并行线程数受限制（避免过载）

### 清理缓存
如需清理缓存（例如源文件更新）：
```bash
rm -rf .cache/scene_ids/
```

## 🎯 后续优化建议

如果需要进一步优化，可以考虑：
1. **流式处理** - 减少内存占用
2. **增量更新** - 基于文件修改时间的智能缓存
3. **分布式处理** - 多机并行处理
4. **预建索引** - scene_id预建索引表

## 📞 问题反馈

如果遇到问题或需要进一步优化，请提供：
- 实际文件数量和大小
- 具体耗时对比
- 错误日志（如有）
