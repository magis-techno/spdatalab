# SPDataLab CLI ä½¿ç”¨æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨ `spdatalab` çš„å‘½ä»¤è¡Œæ¥å£ï¼ˆCLIï¼‰è¿›è¡Œæ•°æ®é›†ç®¡ç†å’Œè¾¹ç•Œæ¡†å¤„ç†ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…
```bash
pip install -e .  # ä»æºç å®‰è£…
# æˆ–è€…
pip install spdatalab  # ä»PyPIå®‰è£…ï¼ˆå¦‚æœå¯ç”¨ï¼‰
```

### åŸºæœ¬å‘½ä»¤ç»“æ„
```bash
python -m spdatalab.cli <command> [options]
```

## ğŸ“‹ å¯ç”¨å‘½ä»¤

### 1. `build-dataset` - æ„å»ºæ•°æ®é›†

ä»ç´¢å¼•æ–‡ä»¶æ„å»ºæ•°æ®é›†ç»“æ„ã€‚

```bash
python -m spdatalab.cli build-dataset \
  --index-file data/train_index.txt \
  --dataset-name "training_v1" \
  --description "è®­ç»ƒæ•°æ®é›†ç‰ˆæœ¬1" \
  --output output/train_dataset.json \
  --format json
```

**å‚æ•°è¯´æ˜ï¼š**
- `--index-file`: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œæ ¼å¼ä¸º `obs_path@duplicateN`
- `--dataset-name`: æ•°æ®é›†åç§°
- `--description`: æ•°æ®é›†æè¿°ï¼ˆå¯é€‰ï¼‰
- `--output`: è¾“å‡ºæ–‡ä»¶è·¯å¾„
- `--format`: è¾“å‡ºæ ¼å¼ï¼Œ`json` æˆ– `parquet`

### 2. `process-bbox` - å¤„ç†è¾¹ç•Œæ¡† â­

ä»æ•°æ®é›†æ–‡ä»¶ä¸­æå–åœºæ™¯IDå¹¶ç”Ÿæˆè¾¹ç•Œæ¡†æ•°æ®ï¼Œæ”¯æŒæ™ºèƒ½è¿›åº¦è·Ÿè¸ªå’Œå¤±è´¥æ¢å¤ã€‚

#### åŸºæœ¬ç”¨æ³•
```bash
python -m spdatalab.cli process-bbox \
  --input output/train_dataset.json \
  --batch 1000 \
  --insert-batch 500 \
  --work-dir ./logs/bbox_import
```

#### è¿›åº¦è·Ÿè¸ªå’Œæ¢å¤
```bash
# å¤§å‹æ•°æ®é›†å¤„ç†ï¼ˆæ¨èï¼‰
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --batch 1000 \
  --insert-batch 1000 \
  --work-dir ./logs/large_import_20231201

# ç¨‹åºä¸­æ–­åï¼Œé‡æ–°è¿è¡Œç›¸åŒå‘½ä»¤å³å¯è‡ªåŠ¨ç»­ä¼ 
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --batch 1000 \
  --work-dir ./logs/large_import_20231201

# åªé‡è¯•å¤±è´¥çš„æ•°æ®
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --retry-failed \
  --work-dir ./logs/large_import_20231201

# æŸ¥çœ‹å¤„ç†ç»Ÿè®¡ä¿¡æ¯
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --show-stats \
  --work-dir ./logs/large_import_20231201
```

**å‚æ•°è¯´æ˜ï¼š**

*åŸºæœ¬å‚æ•°ï¼š*
- `--input`: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒJSON/Parquet/æ–‡æœ¬æ ¼å¼ï¼‰
- `--batch`: å¤„ç†æ‰¹æ¬¡å¤§å°ï¼ˆæ¯æ‰¹ä»æ•°æ®åº“è·å–å¤šå°‘ä¸ªåœºæ™¯ï¼‰
- `--insert-batch`: æ’å…¥æ‰¹æ¬¡å¤§å°ï¼ˆæ¯æ‰¹å‘æ•°æ®åº“æ’å…¥å¤šå°‘æ¡è®°å½•ï¼‰
- `--create-table`: æ˜¯å¦åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ï¼Œé»˜è®¤å¯ç”¨

*è¿›åº¦è·Ÿè¸ªå‚æ•°ï¼š*
- `--work-dir`: å·¥ä½œç›®å½•ï¼Œç”¨äºå­˜å‚¨è¿›åº¦æ–‡ä»¶ï¼ˆé»˜è®¤ï¼š`./bbox_import_logs`ï¼‰
- `--retry-failed`: åªé‡è¯•å¤±è´¥çš„æ•°æ®ï¼Œè·³è¿‡å·²æˆåŠŸå¤„ç†çš„æ•°æ®
- `--show-stats`: æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡ä¿¡æ¯å¹¶é€€å‡ºï¼Œä¸æ‰§è¡Œå®é™…å¤„ç†

*æ€§èƒ½ä¼˜åŒ–å‚æ•°ï¼ˆå¦‚æœä½¿ç”¨åŸç‰ˆå¸¦ç¼“å†²åŒºåŠŸèƒ½ï¼‰ï¼š*
- `--buffer-meters`: ç¼“å†²åŒºå¤§å°ï¼ˆç±³ï¼‰ï¼Œç”¨äºç‚¹æ•°æ®çš„è¾¹ç•Œæ¡†æ‰©å±•
- `--precise-buffer`: ä½¿ç”¨ç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒºï¼ˆé€šè¿‡æŠ•å½±è½¬æ¢å®ç°ï¼‰

#### å·¥ä½œç›®å½•ç»“æ„
```
work_dir/
â”œâ”€â”€ successful_tokens.parquet  # æˆåŠŸå¤„ç†çš„åœºæ™¯IDï¼ˆé«˜æ•ˆæŸ¥è¯¢ï¼‰
â”œâ”€â”€ failed_tokens.parquet      # å¤±è´¥è®°å½•è¯¦æƒ…ï¼ˆé”™è¯¯åˆ†æï¼‰
â””â”€â”€ progress.json              # æ€»ä½“è¿›åº¦ä¿¡æ¯ï¼ˆäººç±»å¯è¯»ï¼‰
```

#### ä½¿ç”¨åœºæ™¯ç¤ºä¾‹

**åœºæ™¯1ï¼šå¤„ç†å¤§å‹æ•°æ®é›†**
```bash
# 400ä¸‡åœºæ™¯çš„æ•°æ®é›†å¤„ç†
python -m spdatalab.cli process-bbox \
  --input output/huge_dataset.parquet \
  --batch 1000 \
  --insert-batch 1000 \
  --work-dir ./logs/huge_import_$(date +%Y%m%d)
```

**åœºæ™¯2ï¼šå¤±è´¥æ¢å¤**
```bash
# æŸ¥çœ‹ä¸Šæ¬¡å¤„ç†çš„ç»“æœ
python -m spdatalab.cli process-bbox \
  --input output/dataset.json \
  --show-stats \
  --work-dir ./logs/previous_import

# é‡è¯•å¤±è´¥çš„æ•°æ®
python -m spdatalab.cli process-bbox \
  --input output/dataset.json \
  --retry-failed \
  --batch 500 \
  --work-dir ./logs/previous_import
```

**åœºæ™¯3ï¼šåˆ†é˜¶æ®µå¤„ç†**
```bash
# é˜¶æ®µ1ï¼šå¿«é€Ÿå¤„ç†ä¸»è¦æ•°æ®
python -m spdatalab.cli process-bbox \
  --input output/dataset.json \
  --batch 2000 \
  --work-dir ./logs/phase1

# é˜¶æ®µ2ï¼šå¤„ç†å‰©ä½™å¤±è´¥æ•°æ®
python -m spdatalab.cli process-bbox \
  --input output/dataset.json \
  --retry-failed \
  --batch 500 \
  --work-dir ./logs/phase1
```

### 3. `build-dataset-with-bbox` - ä¸€é”®å¼å®Œæ•´å·¥ä½œæµç¨‹ â­

æ„å»ºæ•°æ®é›†å¹¶è‡ªåŠ¨å¤„ç†è¾¹ç•Œæ¡†ï¼Œæä¾›æœ€ä¾¿æ·çš„ä½¿ç”¨æ–¹å¼ã€‚

```bash
python -m spdatalab.cli build-dataset-with-bbox \
  --index-file data/train_index.txt \
  --dataset-name "complete_training_v1" \
  --description "å®Œæ•´è®­ç»ƒæ•°æ®é›†" \
  --output output/complete_dataset.json \
  --format json \
  --batch 1000 \
  --insert-batch 500 \
  --buffer-meters 50 \
  --precise-buffer
```

**è·³è¿‡è¾¹ç•Œæ¡†å¤„ç†ï¼š**
```bash
# åªæ„å»ºæ•°æ®é›†ï¼Œä¸å¤„ç†è¾¹ç•Œæ¡†
python -m spdatalab.cli build-dataset-with-bbox \
  --index-file data/test_index.txt \
  --dataset-name "test_dataset" \
  --output output/test_dataset.json \
  --skip-bbox
```

### 4. `dataset-info` - æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯

æ˜¾ç¤ºæ•°æ®é›†çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ã€‚

```bash
python -m spdatalab.cli dataset-info \
  --dataset-file output/train_dataset.json
```

### 5. `list-scenes` - åˆ—å‡ºåœºæ™¯ID

åˆ—å‡ºæ•°æ®é›†ä¸­çš„åœºæ™¯IDã€‚

```bash
# è¾“å‡ºåˆ°æ§åˆ¶å°
python -m spdatalab.cli list-scenes \
  --dataset-file output/train_dataset.json

# ä¿å­˜åˆ°æ–‡ä»¶
python -m spdatalab.cli list-scenes \
  --dataset-file output/train_dataset.json \
  --output output/scene_ids.txt

# åªåˆ—å‡ºç‰¹å®šå­æ•°æ®é›†çš„åœºæ™¯
python -m spdatalab.cli list-scenes \
  --dataset-file output/train_dataset.json \
  --subdataset "lane_change_1"
```

### 6. `generate-scene-ids` - ç”ŸæˆåŒ…å«å€å¢çš„åœºæ™¯ID

ç”ŸæˆåŒ…å«å€å¢å› å­çš„å®Œæ•´åœºæ™¯IDåˆ—è¡¨ã€‚

```bash
python -m spdatalab.cli generate-scene-ids \
  --dataset-file output/train_dataset.json \
  --output output/scene_ids_with_duplicates.txt
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### è¿›åº¦è·Ÿè¸ªå’Œå¤±è´¥æ¢å¤

#### å¤§è§„æ¨¡æ•°æ®å¤„ç†ï¼ˆ400ä¸‡+åœºæ™¯ï¼‰
```bash
# å¤„ç†å¤§å‹Parquetæ•°æ®é›†ï¼ˆæ¨èï¼‰
python -m spdatalab.cli process-bbox \
  --input output/huge_dataset.parquet \
  --batch 1000 \
  --insert-batch 1000 \
  --work-dir ./logs/production_import_$(date +%Y%m%d_%H%M%S)
```

#### æ–­ç‚¹ç»­ä¼ å’Œæ¢å¤
```bash
# 1. å¼€å§‹å¤„ç†å¤§å‹æ•°æ®é›†
python -m spdatalab.cli process-bbox \
  --input output/dataset.parquet \
  --batch 1000 \
  --work-dir ./logs/import_job_001

# 2. å¦‚æœç¨‹åºä¸­æ–­ï¼Œé‡æ–°è¿è¡Œç›¸åŒå‘½ä»¤å³å¯ç»­ä¼ 
python -m spdatalab.cli process-bbox \
  --input output/dataset.parquet \
  --batch 1000 \
  --work-dir ./logs/import_job_001

# 3. æŸ¥çœ‹å¤„ç†ç»Ÿè®¡å’Œè¿›åº¦
python -m spdatalab.cli process-bbox \
  --input output/dataset.parquet \
  --show-stats \
  --work-dir ./logs/import_job_001

# 4. é‡è¯•å¤±è´¥çš„æ•°æ®
python -m spdatalab.cli process-bbox \
  --input output/dataset.parquet \
  --retry-failed \
  --batch 500 \
  --work-dir ./logs/import_job_001
```

#### å·¥ä½œç›®å½•ç®¡ç†
```bash
# ä¸ºä¸åŒä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„å·¥ä½œç›®å½•
mkdir -p logs/imports/{training,validation,test}

# è®­ç»ƒæ•°æ®å¤„ç†
python -m spdatalab.cli process-bbox \
  --input output/train_dataset.parquet \
  --work-dir ./logs/imports/training \
  --batch 1500

# éªŒè¯æ•°æ®å¤„ç†
python -m spdatalab.cli process-bbox \
  --input output/val_dataset.parquet \
  --work-dir ./logs/imports/validation \
  --batch 1000
```

### æ€§èƒ½è°ƒä¼˜

#### ç³»ç»Ÿèµ„æºé…ç½®å»ºè®®

**é«˜æ€§èƒ½æœåŠ¡å™¨ï¼ˆ32GB+ å†…å­˜ï¼‰**
```bash
python -m spdatalab.cli process-bbox \
  --input dataset.parquet \
  --batch 2000 \
  --insert-batch 2000 \
  --work-dir ./logs/high_perf
```

**æ ‡å‡†é…ç½®ï¼ˆ16GB å†…å­˜ï¼‰**
```bash
python -m spdatalab.cli process-bbox \
  --input dataset.parquet \
  --batch 1000 \
  --insert-batch 1000 \
  --work-dir ./logs/standard
```

**èµ„æºå—é™ï¼ˆ8GB å†…å­˜ï¼‰**
```bash
python -m spdatalab.cli process-bbox \
  --input dataset.parquet \
  --batch 500 \
  --insert-batch 500 \
  --work-dir ./logs/limited
```

#### æ•°æ®é›†æ ¼å¼é€‰æ‹©

**å¤§å‹æ•°æ®é›†ï¼ˆ100ä¸‡+åœºæ™¯ï¼‰** - æ¨èParquetæ ¼å¼ï¼š
```bash
# æ„å»ºæ—¶ç›´æ¥ä½¿ç”¨Parquet
python -m spdatalab.cli build-dataset \
  --index-file large_index.txt \
  --dataset-name "large_dataset" \
  --output output/large_dataset.parquet \
  --format parquet

# å¤„ç†æ—¶æ€§èƒ½æ›´ä½³
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --batch 1500 \
  --work-dir ./logs/large_import
```

**ä¸­å°å‹æ•°æ®é›†ï¼ˆ<50ä¸‡åœºæ™¯ï¼‰** - JSONæ ¼å¼ä¾¿äºè°ƒè¯•ï¼š
```bash
python -m spdatalab.cli build-dataset \
  --index-file small_index.txt \
  --dataset-name "small_dataset" \
  --output output/small_dataset.json \
  --format json
```

### å¹¶å‘å’Œåˆ†ç‰‡å¤„ç†

#### æ•°æ®é›†åˆ†ç‰‡å¤„ç†
```bash
# å¯¹äºè¶…å¤§æ•°æ®é›†ï¼Œå¯ä»¥è€ƒè™‘åˆ†ç‰‡å¤„ç†
# 1. æŒ‰å­æ•°æ®é›†åˆ†å‰²ç´¢å¼•æ–‡ä»¶
split -l 1000 large_index.txt index_part_

# 2. åˆ†åˆ«å¤„ç†å„ä¸ªåˆ†ç‰‡
for part in index_part_*; do
    python -m spdatalab.cli build-dataset-with-bbox \
        --index-file "$part" \
        --dataset-name "dataset_$(basename $part)" \
        --output "output/dataset_$(basename $part).parquet" \
        --format parquet \
        --batch 1000 \
        --work-dir "./logs/$(basename $part)"
done
```

### ç›‘æ§å’Œè°ƒè¯•

#### å®æ—¶ç›‘æ§å¤„ç†è¿›åº¦
```bash
# åœ¨å¦ä¸€ä¸ªç»ˆç«¯ç›‘æ§è¿›åº¦æ–‡ä»¶
watch -n 30 'cat ./logs/import/progress.json | jq .'

# æˆ–è€…å®šæœŸæŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
while true; do
    python -m spdatalab.cli process-bbox \
        --input dataset.json \
        --show-stats \
        --work-dir ./logs/import
    sleep 300  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
done
```

#### æ•…éšœæ’æŸ¥
```bash
# æŸ¥çœ‹å¤±è´¥è®°å½•çš„è¯¦ç»†ä¿¡æ¯
python -c "
import pandas as pd
df = pd.read_parquet('./logs/import/failed_tokens.parquet')
print('å¤±è´¥æ­¥éª¤ç»Ÿè®¡:')
print(df['step'].value_counts())
print('\næœ€è¿‘å¤±è´¥è®°å½•:')
print(df.tail(10)[['scene_token', 'step', 'error_msg', 'failed_at']])
"
```

### ç¼“å†²åŒºé…ç½®ï¼ˆå¦‚æœä½¿ç”¨å¸¦ç¼“å†²åŒºçš„ç‰ˆæœ¬ï¼‰

#### å¿«é€Ÿæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
é€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯ï¼Œä½¿ç”¨åº¦æ•°è¿‘ä¼¼è½¬æ¢ï¼š
```bash
python -m spdatalab.cli process-bbox \
  --input dataset.json \
  --buffer-meters 50 \
  --work-dir ./logs/quick_mode
```

#### ç²¾ç¡®æ¨¡å¼
ä½¿ç”¨æŠ•å½±åæ ‡ç³»è¿›è¡Œç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒºè®¡ç®—ï¼š
```bash
python -m spdatalab.cli process-bbox \
  --input dataset.json \
  --buffer-meters 100 \
  --precise-buffer \
  --work-dir ./logs/precise_mode
```

## ğŸ“‚ æ–‡ä»¶æ ¼å¼æ”¯æŒ

### è¾“å…¥æ ¼å¼

#### ç´¢å¼•æ–‡ä»¶ (*.txt)
```
obs://bucket/path1/file1.jsonl@duplicate10
obs://bucket/path2/file2.jsonl@duplicate5
obs://bucket/path3/file3.jsonl@duplicate20
```

#### æ•°æ®é›†æ–‡ä»¶
- **JSONæ ¼å¼**: `.json` - äººç±»å¯è¯»ï¼Œä¾¿äºè°ƒè¯•
- **Parquetæ ¼å¼**: `.parquet` - é«˜æ€§èƒ½ï¼Œé€‚åˆå¤§æ•°æ®é›†

#### åœºæ™¯IDæ–‡ä»¶ (*.txt)
```
scene_id_001
scene_id_002
scene_id_003
```

### è¾“å‡ºæ ¼å¼

#### JSONæ•°æ®é›†æ–‡ä»¶ç¤ºä¾‹
```json
{
  "name": "training_dataset_v1",
  "description": "è®­ç»ƒæ•°æ®é›†ç‰ˆæœ¬1",
  "created_at": "2025-01-18T10:30:00",
  "subdatasets": [
    {
      "name": "example",
      "obs_path": "obs://bucket/path/file.jsonl",
      "duplication_factor": 10,
      "scene_count": 100,
      "scene_ids": ["scene_001", "scene_002", "..."]
    }
  ],
  "total_scenes": 1000,
  "total_unique_scenes": 100
}
```

## ğŸ”„ å¸¸ç”¨å·¥ä½œæµç¨‹

### 1. å¿«é€Ÿå¼€å§‹ï¼ˆå°å‹æ•°æ®é›†ï¼‰
```bash
# ä¸€é”®å®Œæˆæ‰€æœ‰æ“ä½œ
python -m spdatalab.cli build-dataset-with-bbox \
  --index-file data/index.txt \
  --dataset-name "my_dataset" \
  --output output/my_dataset.json \
  --work-dir ./logs/quick_start
```

### 2. å¤§è§„æ¨¡æ•°æ®å¤„ç†ï¼ˆæ¨èï¼‰
```bash
# æ­¥éª¤1ï¼šæ„å»ºParquetæ ¼å¼æ•°æ®é›†
python -m spdatalab.cli build-dataset \
  --index-file data/large_index.txt \
  --dataset-name "large_dataset_v1" \
  --description "å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†" \
  --output output/large_dataset.parquet \
  --format parquet

# æ­¥éª¤2ï¼šæŸ¥çœ‹æ•°æ®é›†ç»Ÿè®¡
python -m spdatalab.cli dataset-info \
  --dataset-file output/large_dataset.parquet

# æ­¥éª¤3ï¼šå¤„ç†è¾¹ç•Œæ¡†ï¼ˆå¸¦è¿›åº¦è·Ÿè¸ªï¼‰
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --batch 1000 \
  --insert-batch 1000 \
  --work-dir ./logs/large_import_$(date +%Y%m%d)

# æ­¥éª¤4ï¼šå¦‚æœä¸­æ–­ï¼Œé‡æ–°è¿è¡Œç»§ç»­å¤„ç†
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --batch 1000 \
  --work-dir ./logs/large_import_20231201

# æ­¥éª¤5ï¼šæŸ¥çœ‹å¤„ç†ç»“æœ
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --show-stats \
  --work-dir ./logs/large_import_20231201
```

### 3. ç”Ÿäº§ç¯å¢ƒå¤„ç†æµç¨‹
```bash
# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p ./logs/production/$(date +%Y%m%d)
WORK_DIR="./logs/production/$(date +%Y%m%d)"

# ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿå¤„ç†ä¸»è¦æ•°æ®
python -m spdatalab.cli process-bbox \
  --input output/production_dataset.parquet \
  --batch 1500 \
  --insert-batch 1500 \
  --work-dir "$WORK_DIR"

# ç¬¬äºŒé˜¶æ®µï¼šæ£€æŸ¥å¤„ç†ç»“æœ
python -m spdatalab.cli process-bbox \
  --input output/production_dataset.parquet \
  --show-stats \
  --work-dir "$WORK_DIR"

# ç¬¬ä¸‰é˜¶æ®µï¼šé‡è¯•å¤±è´¥æ•°æ®
python -m spdatalab.cli process-bbox \
  --input output/production_dataset.parquet \
  --retry-failed \
  --batch 500 \
  --work-dir "$WORK_DIR"

# ç¬¬å››é˜¶æ®µï¼šæœ€ç»ˆéªŒè¯
python -m spdatalab.cli process-bbox \
  --input output/production_dataset.parquet \
  --show-stats \
  --work-dir "$WORK_DIR"
```

### 4. åˆ†æ­¥æ“ä½œï¼ˆè°ƒè¯•å’Œå¼€å‘ï¼‰
```bash
# æ­¥éª¤1ï¼šæ„å»ºæ•°æ®é›†
python -m spdatalab.cli build-dataset \
  --index-file data/debug_index.txt \
  --dataset-name "debug_dataset" \
  --output output/debug_dataset.json

# æ­¥éª¤2ï¼šæŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯
python -m spdatalab.cli dataset-info \
  --dataset-file output/debug_dataset.json

# æ­¥éª¤3ï¼šå¯¼å‡ºåœºæ™¯IDè¿›è¡ŒéªŒè¯
python -m spdatalab.cli list-scenes \
  --dataset-file output/debug_dataset.json \
  --output output/debug_scene_ids.txt

# æ­¥éª¤4ï¼šå°æ‰¹é‡æµ‹è¯•è¾¹ç•Œæ¡†å¤„ç†
python -m spdatalab.cli process-bbox \
  --input output/debug_dataset.json \
  --batch 100 \
  --insert-batch 50 \
  --work-dir ./logs/debug

# æ­¥éª¤5ï¼šæŸ¥çœ‹æµ‹è¯•ç»“æœ
python -m spdatalab.cli process-bbox \
  --input output/debug_dataset.json \
  --show-stats \
  --work-dir ./logs/debug
```

### 5. å¤šæ•°æ®é›†å¹¶è¡Œå¤„ç†
```bash
# å¹¶è¡Œå¤„ç†å¤šä¸ªæ•°æ®é›†
datasets=("train" "val" "test")

for dataset in "${datasets[@]}"; do
    echo "å¤„ç† ${dataset} æ•°æ®é›†..."
    
    # æ¯ä¸ªæ•°æ®é›†ä½¿ç”¨ç‹¬ç«‹çš„å·¥ä½œç›®å½•
    python -m spdatalab.cli process-bbox \
        --input "output/${dataset}_dataset.parquet" \
        --batch 1000 \
        --work-dir "./logs/${dataset}_import" &
done

# ç­‰å¾…æ‰€æœ‰åå°ä»»åŠ¡å®Œæˆ
wait

# æ£€æŸ¥æ‰€æœ‰æ•°æ®é›†çš„å¤„ç†ç»“æœ
for dataset in "${datasets[@]}"; do
    echo "=== ${dataset} æ•°æ®é›†å¤„ç†ç»“æœ ==="
    python -m spdatalab.cli process-bbox \
        --input "output/${dataset}_dataset.parquet" \
        --show-stats \
        --work-dir "./logs/${dataset}_import"
done
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

#### 1. æ•°æ®åº“è¿æ¥å¤±è´¥
```
é”™è¯¯: è¿æ¥æ•°æ®åº“å¤±è´¥
ERROR: could not connect to server
```
**è§£å†³æ–¹æ¡ˆ:**
```bash
# æ£€æŸ¥æ•°æ®åº“æœåŠ¡çŠ¶æ€
pg_isready -h localhost -p 5432

# æ£€æŸ¥è¿æ¥å‚æ•°
export LOCAL_DSN="postgresql+psycopg://username:password@host:port/database"

# æµ‹è¯•è¿æ¥
python -c "from sqlalchemy import create_engine; create_engine('$LOCAL_DSN').connect()"
```

#### 2. è¿›åº¦è·Ÿè¸ªæ–‡ä»¶é—®é¢˜
```
é”™è¯¯: åŠ è½½æˆåŠŸè®°å½•å¤±è´¥: FileNotFoundError
è­¦å‘Š: æœªå®‰è£…pyarrowï¼Œå°†ä½¿ç”¨é™çº§çš„æ–‡æœ¬æ–‡ä»¶æ¨¡å¼
```
**è§£å†³æ–¹æ¡ˆ:**
```bash
# å®‰è£…ä¾èµ–
pip install pandas pyarrow

# æ£€æŸ¥å·¥ä½œç›®å½•æƒé™
ls -la ./logs/bbox_import/
chmod 755 ./logs/bbox_import/

# æ‰‹åŠ¨æ¸…ç†æŸåçš„çŠ¶æ€æ–‡ä»¶
rm -rf ./logs/bbox_import/*.parquet
```

#### 3. å†…å­˜å’Œæ€§èƒ½é—®é¢˜
```
é”™è¯¯: MemoryError
é”™è¯¯: å¤„ç†é€Ÿåº¦è¿‡æ…¢
```
**è§£å†³æ–¹æ¡ˆ:**
```bash
# å‡å°æ‰¹æ¬¡å¤§å°
python -m spdatalab.cli process-bbox \
  --input dataset.parquet \
  --batch 500 \
  --insert-batch 200 \
  --work-dir ./logs/low_memory

# ä½¿ç”¨ç³»ç»Ÿç›‘æ§
htop  # ç›‘æ§å†…å­˜ä½¿ç”¨
iotop # ç›‘æ§ç£ç›˜I/O
```

#### 4. é‡å¤æ•°æ®å’Œçº¦æŸå†²çª
```
é”™è¯¯: UNIQUE constraint failed
é”™è¯¯: duplicate key value violates unique constraint
```
**è§£å†³æ–¹æ¡ˆ:**
```bash
# ç¨‹åºä¼šè‡ªåŠ¨å¤„ç†é‡å¤æ•°æ®ï¼Œå¦‚æœä»æœ‰é—®é¢˜ï¼š
# 1. æ¸…ç†è¿›åº¦æ–‡ä»¶é‡æ–°å¼€å§‹
rm -rf ./logs/import/*

# 2. æˆ–è€…ç»§ç»­å¤„ç†ï¼ˆç¨‹åºä¼šè·³è¿‡å·²å­˜åœ¨çš„æ•°æ®ï¼‰
python -m spdatalab.cli process-bbox \
  --input dataset.parquet \
  --work-dir ./logs/import
```

#### 5. ç´¢å¼•æ–‡ä»¶æ ¼å¼é”™è¯¯
```
é”™è¯¯: è§£æç´¢å¼•è¡Œå¤±è´¥: invalid format
ValueError: not enough values to unpack
```
**è§£å†³æ–¹æ¡ˆ:**
```bash
# æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ ¼å¼
head -5 data/index.txt
# åº”è¯¥æ˜¯: obs://path/file.jsonl@duplicate10

# ä¿®å¤æ ¼å¼
sed 's/@duplicate/@duplicate/g' data/index.txt > data/index_fixed.txt
```

### è¿›åº¦è·Ÿè¸ªé—®é¢˜æ’æŸ¥

#### æŸ¥çœ‹è¯¦ç»†çŠ¶æ€ä¿¡æ¯
```bash
# æ£€æŸ¥å·¥ä½œç›®å½•ç»“æ„
ls -la ./logs/import/
# åº”è¯¥åŒ…å«: successful_tokens.parquet, failed_tokens.parquet, progress.json

# æŸ¥çœ‹æˆåŠŸè®°å½•ç»Ÿè®¡
python -c "
import pandas as pd
try:
    df = pd.read_parquet('./logs/import/successful_tokens.parquet')
    print(f'æˆåŠŸå¤„ç†: {len(df)} æ¡è®°å½•')
    print(f'æœ€æ–°å¤„ç†æ—¶é—´: {df["processed_at"].max()}')
except Exception as e:
    print(f'æ— æ³•è¯»å–æˆåŠŸè®°å½•: {e}')
"

# æŸ¥çœ‹å¤±è´¥è®°å½•åˆ†æ
python -c "
import pandas as pd
try:
    df = pd.read_parquet('./logs/import/failed_tokens.parquet')
    print(f'å¤±è´¥è®°å½•: {len(df)} æ¡')
    print('å¤±è´¥æ­¥éª¤ç»Ÿè®¡:')
    print(df['step'].value_counts())
    print('\næœ€è¿‘å¤±è´¥è®°å½•:')
    print(df.tail(5)[['scene_token', 'step', 'error_msg']])
except Exception as e:
    print(f'æ— æ³•è¯»å–å¤±è´¥è®°å½•: {e}')
"
```

#### æ‰‹åŠ¨æ¢å¤å’Œæ¸…ç†
```bash
# å¤‡ä»½å½“å‰è¿›åº¦
cp -r ./logs/import ./logs/import_backup_$(date +%Y%m%d_%H%M%S)

# æ¸…ç†ç‰¹å®šæ­¥éª¤çš„å¤±è´¥è®°å½•
python -c "
import pandas as pd
df = pd.read_parquet('./logs/import/failed_tokens.parquet')
# ç§»é™¤ç‰¹å®šç±»å‹çš„å¤±è´¥è®°å½•ï¼ˆå¦‚ä¸´æ—¶ç½‘ç»œé—®é¢˜ï¼‰
cleaned = df[df['step'] != 'database_insert']
cleaned.to_parquet('./logs/import/failed_tokens.parquet', index=False)
print(f'æ¸…ç†åå‰©ä½™å¤±è´¥è®°å½•: {len(cleaned)}')
"

# é‡ç½®è¿›åº¦ï¼ˆæ…ç”¨ï¼‰
# rm -rf ./logs/import/*
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### ç³»ç»Ÿçº§ä¼˜åŒ–
```bash
# è°ƒæ•´PostgreSQLé…ç½®ï¼ˆéœ€è¦DBAæƒé™ï¼‰
# shared_buffers = 256MB
# effective_cache_size = 1GB
# work_mem = 4MB

# ç›‘æ§æ•°æ®åº“æ€§èƒ½
# SELECT * FROM pg_stat_activity WHERE state = 'active';

# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h /var/lib/postgresql/
df -h ./logs/
```

#### åº”ç”¨çº§ä¼˜åŒ–
```bash
# é’ˆå¯¹ä¸åŒåœºæ™¯çš„ä¼˜åŒ–é…ç½®

# é«˜ååé‡å¤„ç†ï¼ˆSSD + é«˜å†…å­˜ï¼‰
python -m spdatalab.cli process-bbox \
  --input dataset.parquet \
  --batch 2000 \
  --insert-batch 2000 \
  --work-dir ./logs/high_throughput

# ç¨³å®šæ€§ä¼˜å…ˆï¼ˆæœºæ¢°ç¡¬ç›˜ + ä½å†…å­˜ï¼‰
python -m spdatalab.cli process-bbox \
  --input dataset.parquet \
  --batch 200 \
  --insert-batch 100 \
  --work-dir ./logs/stable

# å¹³è¡¡é…ç½®ï¼ˆæ ‡å‡†æœåŠ¡å™¨ï¼‰
python -m spdatalab.cli process-bbox \
  --input dataset.parquet \
  --batch 1000 \
  --insert-batch 500 \
  --work-dir ./logs/balanced
```

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"
python -c "import logging; logging.basicConfig(level=logging.DEBUG)"

# ä½¿ç”¨è°ƒè¯•å‚æ•°
python -m spdatalab.cli process-bbox \
  --input dataset.json \
  --batch 10 \
  --insert-batch 5 \
  --work-dir ./logs/debug \
  2>&1 | tee debug.log
```

## ğŸ“Š æ€§èƒ½åŸºå‡†å’Œå»ºè®®

### æ‰¹æ¬¡å¤§å°è°ƒä¼˜

| ç³»ç»Ÿé…ç½® | æ¨èbatch | æ¨èinsert_batch | é€‚ç”¨åœºæ™¯ |
|----------|-----------|------------------|----------|
| 32GB+ RAM, SSD | 2000 | 2000 | ç”Ÿäº§ç¯å¢ƒï¼Œé«˜æ€§èƒ½ |
| 16GB RAM, SSD | 1000 | 1000 | æ ‡å‡†é…ç½® |
| 8GB RAM, HDD | 500 | 500 | å¼€å‘ç¯å¢ƒ |
| 4GB RAM, HDD | 200 | 200 | èµ„æºå—é™ |

### æ•°æ®é›†å¤§å°å»ºè®®

| åœºæ™¯è§„æ¨¡ | æ¨èæ ¼å¼ | æ¨èå·¥ä½œæµç¨‹ | é¢„è®¡å¤„ç†æ—¶é—´ |
|----------|----------|--------------|-------------|
| <10ä¸‡åœºæ™¯ | JSON | åˆ†æ­¥æ“ä½œ | <1å°æ—¶ |
| 10-100ä¸‡åœºæ™¯ | Parquet | å¤§è§„æ¨¡å¤„ç† | 1-5å°æ—¶ |
| 100-500ä¸‡åœºæ™¯ | Parquet | ç”Ÿäº§ç¯å¢ƒæµç¨‹ | 5-20å°æ—¶ |
| >500ä¸‡åœºæ™¯ | Parquet | åˆ†ç‰‡å¹¶è¡Œå¤„ç† | >20å°æ—¶ |

## ğŸ’¡ æœ€ä½³å®è·µ

1. **é¦–æ¬¡ä½¿ç”¨**ï¼šä½¿ç”¨ `build-dataset-with-bbox` å‘½ä»¤è¿›è¡Œå¿«é€Ÿä¸Šæ‰‹
2. **å¤§æ•°æ®é›†**ï¼šä¼˜å…ˆä½¿ç”¨Parquetæ ¼å¼å’Œè¾ƒå¤§çš„æ‰¹æ¬¡å¤§å°
3. **è°ƒè¯•**ï¼šå…ˆç”¨å°æ•°æ®é›†æµ‹è¯•ï¼Œç¡®è®¤é…ç½®æ— è¯¯åå†å¤„ç†å®Œæ•´æ•°æ®
4. **ç›‘æ§**ï¼šè§‚å¯Ÿå†…å­˜ä½¿ç”¨å’Œå¤„ç†é€Ÿåº¦ï¼Œé€‚å½“è°ƒæ•´æ‰¹æ¬¡å¤§å°
5. **å¤‡ä»½**ï¼šå¤„ç†é‡è¦æ•°æ®å‰å…ˆå¤‡ä»½åŸå§‹ç´¢å¼•æ–‡ä»¶

## ğŸ”— ç›¸å…³èµ„æº

- [BBoxé›†æˆæŒ‡å—](bbox_integration_guide.md)
- [æ•°æ®é›†ç®¡ç†å™¨æ–‡æ¡£](dataset_manager_guide.md)
- [APIå‚è€ƒæ–‡æ¡£](api_reference.md) 