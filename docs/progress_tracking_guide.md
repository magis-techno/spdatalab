# è¿›åº¦è·Ÿè¸ªå’Œå¤±è´¥æ¢å¤æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»SPDataLabçš„è¿›åº¦è·Ÿè¸ªç³»ç»Ÿï¼ŒåŒ…æ‹¬å¦‚ä½•ä½¿ç”¨æ™ºèƒ½è¿›åº¦è·Ÿè¸ªã€å¤±è´¥æ¢å¤æœºåˆ¶ä»¥åŠå¤§è§„æ¨¡æ•°æ®å¤„ç†çš„æœ€ä½³å®è·µã€‚

## ğŸ¯ æ¦‚è¿°

SPDataLabçš„è¿›åº¦è·Ÿè¸ªç³»ç»Ÿä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†è®¾è®¡ï¼Œç‰¹åˆ«é’ˆå¯¹400ä¸‡+åœºæ™¯çš„å¤„ç†åœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚ä¸»è¦ç‰¹æ€§åŒ…æ‹¬ï¼š

- **æ™ºèƒ½æ–­ç‚¹ç»­ä¼ **ï¼šç¨‹åºä¸­æ–­åè‡ªåŠ¨ä»ä¸Šæ¬¡åœæ­¢çš„åœ°æ–¹ç»§ç»­
- **å¤±è´¥è®°å½•å’Œé‡è¯•**ï¼šè¯¦ç»†è®°å½•å¤±è´¥åŸå› å¹¶æ”¯æŒé€‰æ‹©æ€§é‡è¯•
- **é«˜æ•ˆçŠ¶æ€å­˜å‚¨**ï¼šä½¿ç”¨Parquetæ ¼å¼å­˜å‚¨çŠ¶æ€ï¼Œæ¯”ä¼ ç»Ÿæ–‡æœ¬æ–‡ä»¶å¿«100å€
- **å†…å­˜å‹å¥½**ï¼šç¼“å†²æœºåˆ¶å’Œæ‡’åŠ è½½ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®å¤„ç†
- **é›¶æ•°æ®ä¸¢å¤±**ï¼šå³ä½¿ç¨‹åºå´©æºƒä¹Ÿä¸ä¼šä¸¢å¤±å·²å¤„ç†çš„æ•°æ®

## ğŸ“ æ–‡ä»¶ç»“æ„

è¿›åº¦è·Ÿè¸ªç³»ç»Ÿä¼šåœ¨æŒ‡å®šçš„å·¥ä½œç›®å½•ä¸‹åˆ›å»ºä»¥ä¸‹æ–‡ä»¶ï¼š

```
work_dir/
â”œâ”€â”€ successful_tokens.parquet  # æˆåŠŸå¤„ç†çš„åœºæ™¯è®°å½•
â”œâ”€â”€ failed_tokens.parquet      # å¤±è´¥è®°å½•è¯¦æƒ…
â””â”€â”€ progress.json              # æ€»ä½“è¿›åº¦æ¦‚è§ˆï¼ˆäººç±»å¯è¯»ï¼‰
```

### æ–‡ä»¶è¯´æ˜

#### successful_tokens.parquet
è®°å½•æˆåŠŸå¤„ç†çš„åœºæ™¯ä¿¡æ¯ï¼š
```python
{
    'scene_token': str,     # åœºæ™¯ID
    'processed_at': datetime,  # å¤„ç†æ—¶é—´
    'batch_num': int        # æ‰¹æ¬¡å·
}
```

#### failed_tokens.parquet
è®°å½•å¤±è´¥çš„è¯¦ç»†ä¿¡æ¯ï¼š
```python
{
    'scene_token': str,     # å¤±è´¥çš„åœºæ™¯ID
    'error_msg': str,       # é”™è¯¯ä¿¡æ¯
    'batch_num': int,       # æ‰¹æ¬¡å·
    'step': str,           # å¤±è´¥æ­¥éª¤
    'failed_at': datetime   # å¤±è´¥æ—¶é—´
}
```

#### progress.json
æ€»ä½“è¿›åº¦æ¦‚è§ˆï¼š
```json
{
  "total_scenes": 4000000,
  "processed_scenes": 3850000,
  "inserted_records": 3835000,
  "current_batch": 3850,
  "timestamp": "2023-12-01T15:30:45",
  "successful_count": 3835000,
  "failed_count": 15000
}
```

## ğŸš€ åŸºæœ¬ä½¿ç”¨

### å¼€å§‹æ–°çš„å¤„ç†ä»»åŠ¡

```bash
# åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å·¥ä½œç›®å½•
WORK_DIR="./logs/import_$(date +%Y%m%d_%H%M%S)"

# å¼€å§‹å¤„ç†
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --batch 1000 \
  --insert-batch 1000 \
  --work-dir "$WORK_DIR"
```

### æ–­ç‚¹ç»­ä¼ 

å¦‚æœç¨‹åºä¸­æ–­ï¼Œåªéœ€é‡æ–°è¿è¡Œç›¸åŒçš„å‘½ä»¤ï¼š

```bash
# ç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å·²å¤„ç†çš„æ•°æ®å¹¶è·³è¿‡
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --batch 1000 \
  --insert-batch 1000 \
  --work-dir "./logs/import_20231201_143025"
```

### æŸ¥çœ‹å¤„ç†ç»Ÿè®¡

```bash
# æŸ¥çœ‹è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --show-stats \
  --work-dir "./logs/import_20231201_143025"
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
=== å¤„ç†ç»Ÿè®¡ä¿¡æ¯ ===
æˆåŠŸå¤„ç†: 3,850,000 ä¸ªåœºæ™¯
å¤±è´¥åœºæ™¯: 15,000 ä¸ª

æŒ‰æ­¥éª¤åˆ†ç±»çš„å¤±è´¥ç»Ÿè®¡:
  fetch_bbox: 8,000 ä¸ª
  database_insert: 4,200 ä¸ª
  fetch_meta: 2,800 ä¸ª
```

### é‡è¯•å¤±è´¥æ•°æ®

```bash
# åªé‡è¯•å¤±è´¥çš„æ•°æ®ï¼Œè·³è¿‡å·²æˆåŠŸçš„æ•°æ®
python -m spdatalab.cli process-bbox \
  --input output/large_dataset.parquet \
  --retry-failed \
  --batch 500 \
  --work-dir "./logs/import_20231201_143025"
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### å·¥ä½œç›®å½•ç®¡ç†

#### æŒ‰ä»»åŠ¡åˆ†ç±»
```bash
# ä¸ºä¸åŒç±»å‹çš„æ•°æ®é›†åˆ›å»ºç‹¬ç«‹ç›®å½•
mkdir -p logs/imports/{training,validation,test}

# è®­ç»ƒæ•°æ®
python -m spdatalab.cli process-bbox \
  --input train_dataset.parquet \
  --work-dir ./logs/imports/training

# éªŒè¯æ•°æ®
python -m spdatalab.cli process-bbox \
  --input val_dataset.parquet \
  --work-dir ./logs/imports/validation
```

#### ç‰ˆæœ¬æ§åˆ¶
```bash
# ä½¿ç”¨ç‰ˆæœ¬å·ç®¡ç†ä¸åŒçš„å¤„ç†ä»»åŠ¡
python -m spdatalab.cli process-bbox \
  --input dataset_v2.parquet \
  --work-dir "./logs/dataset_v2_$(date +%Y%m%d)"
```

### æ‰¹é‡çŠ¶æ€æŸ¥è¯¢

#### æ£€æŸ¥æ‰€æœ‰å·¥ä½œç›®å½•çš„çŠ¶æ€
```bash
#!/bin/bash
# check_all_progress.sh

for work_dir in ./logs/imports/*/; do
    echo "=== $(basename "$work_dir") ==="
    python -m spdatalab.cli process-bbox \
        --input "output/$(basename "$work_dir")_dataset.parquet" \
        --show-stats \
        --work-dir "$work_dir"
    echo
done
```

#### å¹¶è¡Œå¤„ç†ç›‘æ§
```bash
# å¯åŠ¨å¤šä¸ªå¤„ç†ä»»åŠ¡
datasets=("train" "val" "test")

for dataset in "${datasets[@]}"; do
    python -m spdatalab.cli process-bbox \
        --input "${dataset}_dataset.parquet" \
        --work-dir "./logs/${dataset}_import" &
done

# ç›‘æ§è„šæœ¬
while true; do
    clear
    for dataset in "${datasets[@]}"; do
        echo "=== $dataset å¤„ç†è¿›åº¦ ==="
        if [ -f "./logs/${dataset}_import/progress.json" ]; then
            cat "./logs/${dataset}_import/progress.json" | \
                jq -r '"å¤„ç†: \(.processed_scenes)/\(.total_scenes) (\((.processed_scenes/.total_scenes*100)|round)%)"'
        else
            echo "å°šæœªå¼€å§‹"
        fi
        echo
    done
    sleep 30
done
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### ç¼“å†²åŒºå¤§å°è°ƒä¼˜

è¿›åº¦è·Ÿè¸ªç³»ç»Ÿä½¿ç”¨å†…å­˜ç¼“å†²åŒºæ¥æ‰¹é‡å†™å…¥çŠ¶æ€æ–‡ä»¶ï¼Œé»˜è®¤ç¼“å†²åŒºå¤§å°ä¸º1000æ¡è®°å½•ã€‚

#### é«˜é¢‘å†™å…¥ä¼˜åŒ–
```python
# å¯¹äºéœ€è¦å®æ—¶çŠ¶æ€æ›´æ–°çš„åœºæ™¯ï¼Œå¯ä»¥å‡å°ç¼“å†²åŒº
# ä¿®æ”¹ bbox.py ä¸­çš„ _buffer_size å‚æ•°
tracker = LightweightProgressTracker(work_dir)
tracker._buffer_size = 100  # æ›´é¢‘ç¹çš„çŠ¶æ€ä¿å­˜
```

#### é«˜ååé‡ä¼˜åŒ–
```python
# å¯¹äºé«˜ååé‡åœºæ™¯ï¼Œå¯ä»¥å¢å¤§ç¼“å†²åŒº
tracker._buffer_size = 5000  # å‡å°‘I/Oæ¬¡æ•°
```

### æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–

#### SSDä¼˜åŒ–
```bash
# SSDä¸Šå¯ä»¥ä½¿ç”¨æ›´å¤§çš„æ‰¹æ¬¡
python -m spdatalab.cli process-bbox \
  --input dataset.parquet \
  --batch 2000 \
  --work-dir /fast_ssd/logs/import
```

#### ç½‘ç»œå­˜å‚¨ä¼˜åŒ–
```bash
# ç½‘ç»œå­˜å‚¨éœ€è¦å‡å°‘I/O
python -m spdatalab.cli process-bbox \
  --input dataset.parquet \
  --batch 500 \
  --work-dir /network_storage/logs/import
```

## ğŸ” æ•…éšœè¯Šæ–­

### çŠ¶æ€æ–‡ä»¶åˆ†æ

#### æŸ¥çœ‹æˆåŠŸè®°å½•ç»Ÿè®¡
```python
import pandas as pd
from datetime import datetime, timedelta

# åŠ è½½æˆåŠŸè®°å½•
df = pd.read_parquet('./logs/import/successful_tokens.parquet')

print(f"æ€»æˆåŠŸè®°å½•: {len(df)}")
print(f"æœ€æ—©å¤„ç†æ—¶é—´: {df['processed_at'].min()}")
print(f"æœ€æ–°å¤„ç†æ—¶é—´: {df['processed_at'].max()}")

# æŒ‰å°æ—¶ç»Ÿè®¡å¤„ç†é€Ÿåº¦
df['hour'] = df['processed_at'].dt.floor('H')
hourly_stats = df.groupby('hour').size()
print("\næ¯å°æ—¶å¤„ç†é‡:")
print(hourly_stats.tail(24))  # æœ€è¿‘24å°æ—¶

# æŒ‰æ‰¹æ¬¡ç»Ÿè®¡
batch_stats = df.groupby('batch_num').size()
print(f"\nå¹³å‡æ¯æ‰¹æ¬¡å¤„ç†: {batch_stats.mean():.0f} ä¸ªåœºæ™¯")
```

#### åˆ†æå¤±è´¥æ¨¡å¼
```python
import pandas as pd

# åŠ è½½å¤±è´¥è®°å½•
df = pd.read_parquet('./logs/import/failed_tokens.parquet')

print(f"æ€»å¤±è´¥è®°å½•: {len(df)}")

# æŒ‰æ­¥éª¤åˆ†æå¤±è´¥
step_stats = df['step'].value_counts()
print("\nå¤±è´¥æ­¥éª¤ç»Ÿè®¡:")
print(step_stats)

# æŒ‰é”™è¯¯ç±»å‹åˆ†æ
error_patterns = df['error_msg'].str.extract(r'(.*?):', expand=False).value_counts()
print("\né”™è¯¯ç±»å‹ç»Ÿè®¡:")
print(error_patterns.head(10))

# æ—¶é—´åˆ†å¸ƒåˆ†æ
df['hour'] = df['failed_at'].dt.hour
hourly_failures = df.groupby('hour').size()
print("\nå¤±è´¥æ—¶é—´åˆ†å¸ƒ:")
print(hourly_failures)

# é‡è¯•å»ºè®®
print("\n=== é‡è¯•å»ºè®® ===")
retry_candidates = df[df['step'].isin(['database_insert', 'fetch_bbox'])]
print(f"å»ºè®®é‡è¯•çš„è®°å½•: {len(retry_candidates)} ä¸ª")
```

### å¸¸è§é—®é¢˜æ’æŸ¥

#### 1. çŠ¶æ€æ–‡ä»¶æŸå
```bash
# æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
python -c "
import pandas as pd
try:
    df = pd.read_parquet('./logs/import/successful_tokens.parquet')
    print(f'æˆåŠŸè®°å½•æ–‡ä»¶æ­£å¸¸: {len(df)} æ¡è®°å½•')
except Exception as e:
    print(f'æˆåŠŸè®°å½•æ–‡ä»¶æŸå: {e}')

try:
    df = pd.read_parquet('./logs/import/failed_tokens.parquet')
    print(f'å¤±è´¥è®°å½•æ–‡ä»¶æ­£å¸¸: {len(df)} æ¡è®°å½•')
except Exception as e:
    print(f'å¤±è´¥è®°å½•æ–‡ä»¶æŸå: {e}')
"

# å¦‚æœæ–‡ä»¶æŸåï¼Œåˆ é™¤é‡æ–°å¼€å§‹
# rm ./logs/import/*.parquet
```

#### 2. è¿›åº¦ä¸ä¸€è‡´
```bash
# æ£€æŸ¥æ•°æ®åº“å®é™…è®°å½•æ•°
python -c "
from sqlalchemy import create_engine, text

eng = create_engine('postgresql+psycopg://postgres:postgres@local_pg:5432/postgres')
with eng.connect() as conn:
    result = conn.execute(text('SELECT COUNT(*) FROM clips_bbox'))
    db_count = result.scalar()
    print(f'æ•°æ®åº“å®é™…è®°å½•æ•°: {db_count}')

# ä¸çŠ¶æ€æ–‡ä»¶å¯¹æ¯”
import pandas as pd
df = pd.read_parquet('./logs/import/successful_tokens.parquet')
print(f'çŠ¶æ€æ–‡ä»¶è®°å½•æ•°: {len(df)}')
print(f'å·®å¼‚: {db_count - len(df)}')
"
```

#### 3. å†…å­˜æ³„æ¼æ’æŸ¥
```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
python -c "
import psutil
import time
import subprocess

# å¯åŠ¨å¤„ç†è¿›ç¨‹
proc = subprocess.Popen([
    'python', '-m', 'spdatalab.cli', 'process-bbox',
    '--input', 'dataset.parquet',
    '--batch', '100',
    '--work-dir', './logs/debug'
])

# ç›‘æ§å†…å­˜
for i in range(60):  # ç›‘æ§1åˆ†é’Ÿ
    try:
        p = psutil.Process(proc.pid)
        memory_mb = p.memory_info().rss / 1024 / 1024
        print(f'æ—¶é—´: {i}s, å†…å­˜: {memory_mb:.1f}MB')
        time.sleep(1)
    except psutil.NoSuchProcess:
        break

proc.terminate()
"
```

## ğŸ”„ è¿ç§»å’Œå¤‡ä»½

### çŠ¶æ€æ–‡ä»¶å¤‡ä»½
```bash
# å®šæœŸå¤‡ä»½çŠ¶æ€æ–‡ä»¶
backup_progress() {
    local work_dir="$1"
    local backup_dir="./backups/$(basename "$work_dir")_$(date +%Y%m%d_%H%M%S)"
    
    mkdir -p "$backup_dir"
    cp "$work_dir"/*.parquet "$backup_dir"/ 2>/dev/null || true
    cp "$work_dir"/progress.json "$backup_dir"/ 2>/dev/null || true
    
    echo "å·²å¤‡ä»½åˆ°: $backup_dir"
}

# ä½¿ç”¨æ–¹æ³•
backup_progress "./logs/import"
```

### åˆå¹¶å¤šä¸ªçŠ¶æ€æ–‡ä»¶
```python
import pandas as pd
from pathlib import Path

def merge_progress_files(work_dirs, output_dir):
    """åˆå¹¶å¤šä¸ªå·¥ä½œç›®å½•çš„è¿›åº¦æ–‡ä»¶"""
    all_success = []
    all_failed = []
    
    for work_dir in work_dirs:
        work_path = Path(work_dir)
        
        # åˆå¹¶æˆåŠŸè®°å½•
        success_file = work_path / 'successful_tokens.parquet'
        if success_file.exists():
            df = pd.read_parquet(success_file)
            all_success.append(df)
        
        # åˆå¹¶å¤±è´¥è®°å½•
        failed_file = work_path / 'failed_tokens.parquet'
        if failed_file.exists():
            df = pd.read_parquet(failed_file)
            all_failed.append(df)
    
    # ä¿å­˜åˆå¹¶ç»“æœ
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    if all_success:
        merged_success = pd.concat(all_success, ignore_index=True)
        merged_success = merged_success.drop_duplicates(subset=['scene_token'], keep='last')
        merged_success.to_parquet(output_path / 'successful_tokens.parquet', index=False)
        print(f"åˆå¹¶æˆåŠŸè®°å½•: {len(merged_success)} æ¡")
    
    if all_failed:
        merged_failed = pd.concat(all_failed, ignore_index=True)
        merged_failed.to_parquet(output_path / 'failed_tokens.parquet', index=False)
        print(f"åˆå¹¶å¤±è´¥è®°å½•: {len(merged_failed)} æ¡")

# ä½¿ç”¨ç¤ºä¾‹
work_dirs = ['./logs/part1', './logs/part2', './logs/part3']
merge_progress_files(work_dirs, './logs/merged')
```

### çŠ¶æ€æ–‡ä»¶æ¸…ç†
```python
def cleanup_old_progress(base_dir, days=7):
    """æ¸…ç†æŒ‡å®šå¤©æ•°å‰çš„è¿›åº¦æ–‡ä»¶"""
    import os
    import time
    from pathlib import Path
    
    cutoff_time = time.time() - (days * 24 * 60 * 60)
    cleaned_count = 0
    
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith('.parquet') or file == 'progress.json':
                file_path = Path(root) / file
                if file_path.stat().st_mtime < cutoff_time:
                    file_path.unlink()
                    cleaned_count += 1
    
    print(f"æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸæ–‡ä»¶")

# æ¸…ç†7å¤©å‰çš„æ–‡ä»¶
cleanup_old_progress('./logs', days=7)
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### ä¸åŒè§„æ¨¡çš„æ€§èƒ½è¡¨ç°

| æ•°æ®è§„æ¨¡ | å¯åŠ¨æ—¶é—´ | çŠ¶æ€æ–‡ä»¶å¤§å° | æŸ¥è¯¢æ—¶é—´ | å†…å­˜å ç”¨ |
|----------|----------|-------------|----------|----------|
| 10ä¸‡åœºæ™¯ | <1ç§’ | 2MB | <100ms | 20MB |
| 100ä¸‡åœºæ™¯ | 2ç§’ | 20MB | 200ms | 50MB |
| 400ä¸‡åœºæ™¯ | 3ç§’ | 80MB | 500ms | 100MB |
| 1000ä¸‡åœºæ™¯ | 5ç§’ | 200MB | 1ç§’ | 200MB |

### ä¸ä¼ ç»Ÿæ–¹æ¡ˆå¯¹æ¯”

| æŒ‡æ ‡ | ä¼ ç»Ÿæ–‡æœ¬æ–‡ä»¶ | Parquetæ–¹æ¡ˆ | æ€§èƒ½æå‡ |
|------|-------------|-------------|----------|
| æ–‡ä»¶å¤§å° | 400MB | 80MB | 5xå‹ç¼© |
| å¯åŠ¨æ—¶é—´ | 10ç§’+ | 3ç§’ | 3xæ›´å¿« |
| æŸ¥è¯¢é€Ÿåº¦ | 5ç§’+ | 0.5ç§’ | 10xæ›´å¿« |
| å†…å­˜å ç”¨ | 400MB+ | 100MB | 4xæ›´å°‘ |

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. å·¥ä½œç›®å½•ç»„ç»‡
```
logs/
â”œâ”€â”€ production/
â”‚   â”œâ”€â”€ 20231201_training/
â”‚   â”œâ”€â”€ 20231202_validation/
â”‚   â””â”€â”€ 20231203_test/
â”œâ”€â”€ development/
â”‚   â”œâ”€â”€ debug_001/
â”‚   â””â”€â”€ experiment_002/
â””â”€â”€ archives/
    â”œâ”€â”€ 202311/
    â””â”€â”€ 202312/
```

### 2. ç›‘æ§è„šæœ¬
```bash
#!/bin/bash
# monitor_progress.sh

WORK_DIR="$1"
DATASET="$2"

if [ -z "$WORK_DIR" ] || [ -z "$DATASET" ]; then
    echo "ç”¨æ³•: $0 <work_dir> <dataset_file>"
    exit 1
fi

while true; do
    clear
    echo "=== å¤„ç†è¿›åº¦ç›‘æ§ ==="
    echo "æ—¶é—´: $(date)"
    echo "å·¥ä½œç›®å½•: $WORK_DIR"
    echo

    if [ -f "$WORK_DIR/progress.json" ]; then
        python -c "
import json
with open('$WORK_DIR/progress.json') as f:
    data = json.load(f)
processed = data.get('processed_scenes', 0)
total = data.get('total_scenes', 1)
percentage = (processed / total) * 100
print(f'è¿›åº¦: {processed:,}/{total:,} ({percentage:.1f}%)')
print(f'æˆåŠŸ: {data.get(\"successful_count\", 0):,}')
print(f'å¤±è´¥: {data.get(\"failed_count\", 0):,}')
print(f'å½“å‰æ‰¹æ¬¡: {data.get(\"current_batch\", 0)}')
print(f'æ›´æ–°æ—¶é—´: {data.get(\"timestamp\", \"unknown\")}')
"
    else
        echo "è¿›åº¦æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¤„ç†å¯èƒ½å°šæœªå¼€å§‹"
    fi

    echo
    echo "æŒ‰ Ctrl+C é€€å‡ºç›‘æ§"
    sleep 30
done
```

### 3. è‡ªåŠ¨é‡è¯•è„šæœ¬
```bash
#!/bin/bash
# auto_retry.sh

DATASET="$1"
WORK_DIR="$2"
MAX_RETRIES=3

for i in $(seq 1 $MAX_RETRIES); do
    echo "=== é‡è¯•ç¬¬ $i æ¬¡ ==="
    
    # æ£€æŸ¥å¤±è´¥æ•°é‡
    if [ -f "$WORK_DIR/failed_tokens.parquet" ]; then
        FAILED_COUNT=$(python -c "
import pandas as pd
try:
    df = pd.read_parquet('$WORK_DIR/failed_tokens.parquet')
    print(len(df))
except:
    print(0)
")
        
        if [ "$FAILED_COUNT" -eq 0 ]; then
            echo "æ²¡æœ‰å¤±è´¥è®°å½•ï¼Œé€€å‡ºé‡è¯•"
            break
        fi
        
        echo "å‘ç° $FAILED_COUNT ä¸ªå¤±è´¥è®°å½•ï¼Œå¼€å§‹é‡è¯•..."
        
        # é‡è¯•å¤±è´¥æ•°æ®
        python -m spdatalab.cli process-bbox \
            --input "$DATASET" \
            --retry-failed \
            --batch 200 \
            --work-dir "$WORK_DIR"
    else
        echo "æ²¡æœ‰å¤±è´¥è®°å½•æ–‡ä»¶ï¼Œé€€å‡ºé‡è¯•"
        break
    fi
    
    sleep 10
done

echo "é‡è¯•å®Œæˆ"
```

è¿™ä¸ªè¿›åº¦è·Ÿè¸ªç³»ç»Ÿä¸ºå¤§è§„æ¨¡æ•°æ®å¤„ç†æä¾›äº†å¼ºå¤§çš„å®¹é”™å’Œæ¢å¤èƒ½åŠ›ï¼Œç¡®ä¿å³ä½¿åœ¨å¤„ç†400ä¸‡+åœºæ™¯çš„å¤æ‚ä»»åŠ¡ä¸­ä¹Ÿèƒ½ä¿æŒæ•°æ®çš„å®Œæ•´æ€§å’Œå¤„ç†çš„è¿ç»­æ€§ã€‚ 