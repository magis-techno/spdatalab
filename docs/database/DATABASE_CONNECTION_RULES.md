# æ•°æ®åº“è¿æ¥ä½¿ç”¨è§„åˆ™

## ğŸš¨ é‡è¦è§„åˆ™ï¼šä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„èŒƒä½¿ç”¨æ•°æ®åº“è¿æ¥

### 1. Hiveæ•°æ®åº“è¿æ¥ - ä½¿ç”¨ `hive_cursor()`

#### âœ… æ­£ç¡®ä½¿ç”¨åœºæ™¯

**æŸ¥è¯¢è½¨è¿¹ç‚¹æ•°æ®** - ä½¿ç”¨ `dataset_gy1` catalogï¼š
```python
from spdatalab.common.io_hive import hive_cursor

# æŸ¥è¯¢è½¨è¿¹ç‚¹
with hive_cursor("dataset_gy1") as cur:
    cur.execute("""
        SELECT longitude, latitude, timestamp, dataset_name
        FROM ods_location_data 
        WHERE ST_Within(ST_Point(longitude, latitude), ST_GeomFromText(%s, 4326))
    """, (polygon_wkt,))
```

**æŸ¥è¯¢ä¸šåŠ¡æ•°æ®** - ä½¿ç”¨é»˜è®¤ `app_gy1` catalogï¼š
```python
# æŸ¥è¯¢scene_idæ˜ å°„ã€event_idç­‰ä¸šåŠ¡æ•°æ®
with hive_cursor() as cur:  # é»˜è®¤ä½¿ç”¨app_gy1
    cur.execute("""
        SELECT origin_name AS data_name, id AS scene_id, event_id, event_name
        FROM transform.ods_t_data_fragment_datalake 
        WHERE origin_name IN %(tok)s
    """, {"tok": tuple(data_names)})
```

### 2. æœ¬åœ°PostgreSQLè¿æ¥ - ä½¿ç”¨ `create_engine()`

#### âœ… æ­£ç¡®ä½¿ç”¨åœºæ™¯

**ä»…ç”¨äºæœ¬åœ°PostgreSQLæ“ä½œ**ï¼š
```python
from sqlalchemy import create_engine

# è¿æ¥æœ¬åœ°PostgreSQL
LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"
engine = create_engine(LOCAL_DSN, future=True)

# ä¿å­˜ç»“æœåˆ°æœ¬åœ°æ•°æ®åº“
with engine.connect() as conn:
    gdf.to_postgis('result_table', conn, if_exists='append', index=False)
```

### 3. æ•°æ®åº“åˆ†å·¥æ˜ç¡®

| æ•°æ®åº“ç±»å‹ | è¿æ¥æ–¹å¼ | Catalog | ç”¨é€” |
|------------|----------|---------|------|
| Hive | `hive_cursor("dataset_gy1")` | dataset_gy1 | è½¨è¿¹ç‚¹æ•°æ®æŸ¥è¯¢ |
| Hive | `hive_cursor()` | app_gy1 (é»˜è®¤) | ä¸šåŠ¡æ•°æ®æŸ¥è¯¢ (scene_id, event_idç­‰) |
| PostgreSQL | `create_engine(LOCAL_DSN)` | - | ç»“æœå­˜å‚¨å’Œæœ¬åœ°è¡¨æ“ä½œ |

### 4. âŒ ç¦æ­¢çš„é”™è¯¯ç”¨æ³•

```python
# âŒ é”™è¯¯ï¼šä¸è¦ç”¨create_engineè¿æ¥Hive
engine = create_engine("hive://...")  # ç¦æ­¢ï¼

# âŒ é”™è¯¯ï¼šä¸è¦ç”¨hive_cursorè¿æ¥PostgreSQL  
with hive_cursor() as cur:
    cur.execute("CREATE TABLE local_table...")  # ç¦æ­¢ï¼

# âŒ é”™è¯¯ï¼šcatalogä½¿ç”¨é”™è¯¯
with hive_cursor("app_gy1") as cur:
    cur.execute("SELECT * FROM ods_location_data")  # åº”è¯¥ç”¨dataset_gy1ï¼
```

### 5. ä»£ç ç¤ºä¾‹æ¨¡æ¿

#### æ ‡å‡†æŸ¥è¯¢æ¨¡å¼ï¼š
```python
def query_trajectories_in_polygon(polygon_wkt: str) -> pd.DataFrame:
    # 1. æŸ¥è¯¢è½¨è¿¹ç‚¹ (ä½¿ç”¨dataset_gy1)
    with hive_cursor("dataset_gy1") as cur:
        cur.execute(trajectory_sql, (polygon_wkt,))
        points_df = pd.DataFrame(cur.fetchall(), columns=[d[0] for d in cur.description])
    
    # 2. æŸ¥è¯¢businessæ•°æ® (ä½¿ç”¨é»˜è®¤app_gy1)
    data_names = points_df['dataset_name'].unique().tolist()
    with hive_cursor() as cur:
        cur.execute(business_sql, {"tok": tuple(data_names)})
        business_df = pd.DataFrame(cur.fetchall(), columns=[d[0] for d in cur.description])
    
    # 3. åˆå¹¶æ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ°PostgreSQL
    result_gdf = merge_and_process(points_df, business_df)
    
    # 4. ä¿å­˜ç»“æœ (ä½¿ç”¨local PostgreSQL)
    with engine.connect() as conn:
        result_gdf.to_postgis('result_table', conn, if_exists='append', index=False)
    
    return result_gdf
```

### 6. æ£€æŸ¥æ¸…å•

åœ¨å†™ä»£ç æ—¶ï¼Œè¯·æ£€æŸ¥ï¼š
- [ ] æŸ¥è¯¢è½¨è¿¹ç‚¹æ•°æ®æ—¶ä½¿ç”¨äº† `hive_cursor("dataset_gy1")`
- [ ] æŸ¥è¯¢ä¸šåŠ¡æ•°æ®æ—¶ä½¿ç”¨äº† `hive_cursor()` (é»˜è®¤app_gy1)
- [ ] æœ¬åœ°PostgreSQLæ“ä½œä½¿ç”¨äº† `create_engine(LOCAL_DSN)`
- [ ] æ²¡æœ‰æ··ç”¨è¿æ¥æ–¹å¼
- [ ] SQLè¯­å¥ä½¿ç”¨äº†æ­£ç¡®çš„catalogä¸‹çš„è¡¨

### 7. å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

| é”™è¯¯ç—‡çŠ¶ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|----------|----------|----------|
| è¡¨ä¸å­˜åœ¨é”™è¯¯ | catalogä½¿ç”¨é”™è¯¯ | æ£€æŸ¥æ˜¯å¦ç”¨å¯¹äº†dataset_gy1/app_gy1 |
| è¿æ¥è¶…æ—¶ | ä½¿ç”¨äº†é”™è¯¯çš„è¿æ¥æ–¹å¼ | æ”¹ä¸ºä½¿ç”¨hive_cursor |
| æ•°æ®ç±»å‹é”™è¯¯ | PostgreSQLå’ŒHiveæ··ç”¨ | æ˜ç¡®åŒºåˆ†æœ¬åœ°å’Œè¿œç¨‹æ“ä½œ |

---

**ğŸ”¥ è®°ä½ï¼šHiveç”¨hive_cursorï¼ŒPostgreSQLç”¨create_engineï¼Œç»ä¸æ··ç”¨ï¼** 