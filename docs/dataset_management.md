# æ•°æ®é›†ç®¡ç†ç³»ç»Ÿ

## æ¦‚è¿°

æ•°æ®é›†ç®¡ç†ç³»ç»Ÿæä¾›äº†ä¸€ä¸ªç»“æ„åŒ–çš„æ–¹å¼æ¥ç»„ç»‡å’Œç®¡ç†åŒ…å«å¤šä¸ªå­æ•°æ®é›†çš„å¤§å‹æ•°æ®é›†ã€‚ç³»ç»Ÿæ”¯æŒä»ç´¢å¼•æ–‡ä»¶æ„å»ºæ•°æ®é›†ï¼Œæå–åœºæ™¯IDï¼Œå¤„ç†æ•°æ®å€å¢ï¼Œä»¥åŠå„ç§æŸ¥è¯¢å’Œå¯¼å‡ºåŠŸèƒ½ã€‚

**é‡è¦ç‰¹æ€§**ï¼š
- æ”¯æŒJSONå’ŒParquetä¸¤ç§å­˜å‚¨æ ¼å¼
- Parquetæ ¼å¼é€‚åˆå¤§è§„æ¨¡æ•°æ®ï¼ˆ400ä¸‡+åœºæ™¯IDï¼‰
- é«˜æ•ˆçš„å‹ç¼©å’ŒæŸ¥è¯¢æ€§èƒ½
- è‡ªåŠ¨æ ¼å¼æ£€æµ‹å’Œè½¬æ¢
- æ”¯æŒå¤šç§æ•°æ®æºç±»å‹ï¼ˆæ ‡å‡†è®­ç»ƒæ•°æ®ã€é—®é¢˜å•æ•°æ®ï¼‰

## æ•°æ®æºç±»å‹

### 1. æ ‡å‡†è®­ç»ƒæ•°æ®æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
- **æ•°æ®æ¥æº**ï¼šOBSå­˜å‚¨çš„shrinkæ–‡ä»¶
- **ç´¢å¼•æ ¼å¼**ï¼š`obs_path@duplicateN`
- **åœºæ™¯æå–**ï¼šç›´æ¥ä»shrinkæ–‡ä»¶ä¸­æå–scene_id
- **é€‚ç”¨åœºæ™¯**ï¼šå¸¸è§„è®­ç»ƒæ•°æ®é›†æ„å»º

### 2. é—®é¢˜å•æ•°æ®æ¨¡å¼ ğŸ†•
- **æ•°æ®æ¥æº**ï¼šé—®é¢˜å•ç³»ç»Ÿçš„URLé“¾æ¥
- **ç´¢å¼•æ ¼å¼**ï¼šé—®é¢˜å•URLæˆ–URL+å±æ€§
- **åœºæ™¯æå–**ï¼šé€šè¿‡æ•°æ®åº“æŸ¥è¯¢è·å–scene_id
- **é€‚ç”¨åœºæ™¯**ï¼šé—®é¢˜å•æ•°æ®åˆ†æå’Œå¤„ç†

#### é—®é¢˜å•æ•°æ®å¤„ç†æµç¨‹
1. **URLè§£æ** â†’ æå–æ•°æ®åç§°ï¼ˆå¦‚ `10000_ddi-application-667754027299119535`ï¼‰
2. **ç¬¬ä¸€æ¬¡æŸ¥è¯¢** â†’ é€šè¿‡æ•°æ®åç§°ä» `elasticsearch_ros.ods_ddi_index002_datalake` è·å– `defect_id`
3. **ç¬¬äºŒæ¬¡æŸ¥è¯¢** â†’ é€šè¿‡ `defect_id` ä» `transform.ods_t_data_fragment_datalake` è·å– `scene_id`
4. **æ•°æ®é›†æ„å»º** â†’ ç”Ÿæˆä¸bboxå…¼å®¹çš„æ•°æ®é›†æ ¼å¼

## æ ¼å¼é€‰æ‹©æŒ‡å—

### JSONæ ¼å¼
- **é€‚ç”¨åœºæ™¯**ï¼šå°åˆ°ä¸­å‹æ•°æ®é›†ï¼ˆ< 10ä¸‡åœºæ™¯IDï¼‰
- **ä¼˜åŠ¿**ï¼šå¯è¯»æ€§å¥½ï¼Œå…¼å®¹æ€§å¼ºï¼Œè°ƒè¯•æ–¹ä¾¿
- **åŠ£åŠ¿**ï¼šæ–‡ä»¶è¾ƒå¤§ï¼Œè¯»å–é€Ÿåº¦æ…¢

### Parquetæ ¼å¼ â­ æ¨èç”¨äºå¤§æ•°æ®é›†
- **é€‚ç”¨åœºæ™¯**ï¼šå¤§å‹æ•°æ®é›†ï¼ˆ> 10ä¸‡åœºæ™¯IDï¼Œç‰¹åˆ«æ˜¯400ä¸‡+ï¼‰
- **ä¼˜åŠ¿**ï¼š
  - åˆ—å¼å­˜å‚¨ï¼Œå‹ç¼©ç‡é«˜ï¼ˆé€šå¸¸æ¯”JSONå°80-90%ï¼‰
  - è¯»å–é€Ÿåº¦å¿«
  - æ”¯æŒé«˜æ•ˆæŸ¥è¯¢å’Œè¿‡æ»¤
  - å†…å­˜ä½¿ç”¨æ›´å°‘
- **åŠ£åŠ¿**ï¼šéœ€è¦é¢å¤–ä¾èµ–ï¼ˆpandas, pyarrowï¼‰

## å®‰è£…è¦æ±‚

åŸºç¡€åŠŸèƒ½ï¼ˆJSONæ ¼å¼ï¼‰ï¼š
```bash
# åŸºç¡€å®‰è£…ï¼Œåªéœ€è¦ç°æœ‰ä¾èµ–
```

Parquetæ ¼å¼æ”¯æŒï¼š
```bash
# å®‰è£…parquetæ ¼å¼ä¾èµ–
pip install pandas pyarrow
```

é—®é¢˜å•æ•°æ®æ¨¡å¼è¦æ±‚ï¼š
```bash
# é—®é¢˜å•æ¨¡å¼éœ€è¦æ•°æ®åº“è®¿é—®æƒé™
# ç¡®ä¿å¯ä»¥è®¿é—®ä»¥ä¸‹æ•°æ®åº“è¡¨ï¼š
# - elasticsearch_ros.ods_ddi_index002_datalake
# - transform.ods_t_data_fragment_datalake
# 
# æ•°æ®åº“è¿æ¥é€šè¿‡ spdatalab.common.io_hive.hive_cursor å»ºç«‹
```

## æ•°æ®ç»“æ„è®¾è®¡

### æ ¸å¿ƒæ¦‚å¿µ

1. **æ•°æ®é›† (Dataset)**: åŒ…å«å¤šä¸ªå­æ•°æ®é›†çš„é¡¶å±‚å®¹å™¨
2. **å­æ•°æ®é›† (SubDataset)**: å­˜å‚¨åœ¨OBSä¸Šçš„å®é™…æ•°æ®æ–‡ä»¶ï¼ŒåŒ…å«å¤šä¸ªåœºæ™¯
3. **åœºæ™¯ID (Scene ID)**: æ¯ä¸ªåœºæ™¯çš„å”¯ä¸€æ ‡è¯†ç¬¦
4. **å€å¢å› å­ (Duplication Factor)**: æ¯ä¸ªå­æ•°æ®é›†çš„é‡å¤å€æ•°

### æ•°æ®ç»“æ„

```python
@dataclass
class SubDataset:
    name: str                    # å­æ•°æ®é›†åç§°
    obs_path: str               # OBSè·¯å¾„
    duplication_factor: int     # å€å¢å› å­
    scene_count: int           # åœºæ™¯æ•°é‡
    scene_ids: List[str]       # åœºæ™¯IDåˆ—è¡¨
    metadata: Dict             # é¢å¤–å…ƒæ•°æ®

@dataclass
class Dataset:
    name: str                  # æ•°æ®é›†åç§°
    description: str           # æ•°æ®é›†æè¿°
    subdatasets: List[SubDataset]  # å­æ•°æ®é›†åˆ—è¡¨
    created_at: str           # åˆ›å»ºæ—¶é—´
    total_scenes: int         # æ€»åœºæ™¯æ•°ï¼ˆå«å€å¢ï¼‰
    total_unique_scenes: int  # å”¯ä¸€åœºæ™¯æ•°ï¼ˆä¸å«å€å¢ï¼‰
    metadata: Dict           # é¢å¤–å…ƒæ•°æ®
```

## ä½¿ç”¨æ–¹æ³•

### 1. ä»ç´¢å¼•æ–‡ä»¶æ„å»ºæ•°æ®é›†ï¼ˆæ ‡å‡†è®­ç»ƒæ•°æ®ï¼‰

ç´¢å¼•æ–‡ä»¶æ ¼å¼ï¼šæ¯è¡ŒåŒ…å«ä¸€ä¸ªOBSè·¯å¾„å’Œå€å¢å› å­
```
obs://path/to/subdataset1/file.shrink@duplicate20
obs://path/to/subdataset2/file.shrink@duplicate10
obs://path/to/subdataset3/file.shrink@duplicate5
```

ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·ï¼š
```bash
# æ„å»ºæ•°æ®é›† - JSONæ ¼å¼ï¼ˆé»˜è®¤ï¼‰
python -m spdatalab.cli build-dataset \
    --index-file data/index.txt \
    --dataset-name "Dataset" \
    --description "GOD E2E training dataset" \
    --output datasets/dataset.json

# æ„å»ºæ•°æ®é›† - Parquetæ ¼å¼ï¼ˆæ¨èç”¨äºå¤§æ•°æ®é›†ï¼‰
python -m spdatalab.cli build-dataset \
    --index-file data/index.txt \
    --dataset-name "Dataset" \
    --description "GOD E2E training dataset" \
    --output datasets/dataset.parquet \
    --format parquet
```

ä½¿ç”¨Python APIï¼š
```python
from spdatalab.dataset.dataset_manager import DatasetManager

manager = DatasetManager()
dataset = manager.build_dataset_from_index(
    "data/index.txt",
    "Dataset", 
    "GOD E2E training dataset"
)

# ä¿å­˜ä¸ºJSONæ ¼å¼
manager.save_dataset(dataset, "datasets/dataset.json", format='json')

# ä¿å­˜ä¸ºParquetæ ¼å¼ï¼ˆæ¨èç”¨äºå¤§æ•°æ®é›†ï¼‰
manager.save_dataset(dataset, "datasets/dataset.parquet", format='parquet')
```

### 2. ä»é—®é¢˜å•URLæ„å»ºæ•°æ®é›† ğŸ†•

#### è¾“å…¥æ–‡ä»¶æ ¼å¼

**åŸºç¡€æ ¼å¼**ï¼ˆå½“å‰æ”¯æŒï¼‰ï¼š
```
https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119535
https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119536
https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119537
```

**æ‰©å±•æ ¼å¼**ï¼ˆæ”¯æŒé¢å¤–å±æ€§ï¼‰ï¼š
```
https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119535|priority=high|region=beijing|type=lane_change
https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119536|priority=low|region=shanghai|type=intersection
```

#### ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·

```bash
# æ„å»ºé—®é¢˜å•æ•°æ®é›† - JSONæ ¼å¼
python -m spdatalab.cli build-dataset \
    --index-file defect_urls.txt \
    --dataset-name "DefectDataset" \
    --description "é—®é¢˜å•æ•°æ®é›†" \
    --output datasets/defect_dataset.json \
    --defect-mode

# æ„å»ºé—®é¢˜å•æ•°æ®é›† - Parquetæ ¼å¼
python -m spdatalab.cli build-dataset \
    --index-file defect_urls.txt \
    --dataset-name "DefectDataset" \
    --description "é—®é¢˜å•æ•°æ®é›†" \
    --output datasets/defect_dataset.parquet \
    --format parquet \
    --defect-mode
```

#### ä½¿ç”¨Python API

```python
from spdatalab.dataset.dataset_manager import DatasetManager

# æ–¹æ³•1ï¼šåˆ›å»ºæ—¶æŒ‡å®šé—®é¢˜å•æ¨¡å¼
manager = DatasetManager(defect_mode=True)
dataset = manager.build_dataset_from_index(
    "defect_urls.txt",
    "DefectDataset",
    "é—®é¢˜å•æ•°æ®é›†"
)

# æ–¹æ³•2ï¼šè¿è¡Œæ—¶æŒ‡å®šé—®é¢˜å•æ¨¡å¼
manager = DatasetManager()
dataset = manager.build_dataset_from_index(
    "defect_urls.txt",
    "DefectDataset",
    "é—®é¢˜å•æ•°æ®é›†",
    defect_mode=True
)

# ä¿å­˜æ•°æ®é›†
manager.save_dataset(dataset, "datasets/defect_dataset.json", format='json')
```

#### é—®é¢˜å•æ•°æ®é›†ç‰¹ç‚¹

- **åˆå¹¶æ¶æ„**ï¼šä¸€ä¸ªæ–‡ä»¶ä¸­çš„æ‰€æœ‰URLç”Ÿæˆä¸€ä¸ªå­æ•°æ®é›†ï¼Œå‡å°‘è¡¨æ•°é‡
- **æ— å€å¢å› å­**ï¼šé—®é¢˜å•æ•°æ®é€šå¸¸ä¸éœ€è¦é‡å¤ï¼Œ`duplication_factor` é»˜è®¤ä¸º 1
- **åœºæ™¯çº§å±æ€§**ï¼šæ¯ä¸ªåœºæ™¯çš„å±æ€§å­˜å‚¨åœ¨`scene_attributes`ä¸­ï¼Œæ”¯æŒçµæ´»çš„è‡ªå®šä¹‰å­—æ®µ
- **æ•°æ®åº“ä¾èµ–**ï¼šéœ€è¦è®¿é—®Hiveæ•°æ®åº“æ¥æŸ¥è¯¢scene_id
- **é”™è¯¯å¤„ç†**ï¼šè¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬æŸ¥è¯¢å¤±è´¥å’Œæ— scene_idçš„æƒ…å†µ

#### è¾“å‡ºæ ¼å¼ç¤ºä¾‹

```json
{
  "name": "DefectDataset",
  "description": "é—®é¢˜å•æ•°æ®é›†",
  "metadata": {
    "data_type": "defect",
    "source_file": "defect_urls.txt"
  },
  "subdatasets": [
    {
      "name": "DefectDataset_defects",
      "obs_path": "defect_urls.txt",
      "duplication_factor": 1,
      "scene_count": 3,
      "scene_ids": [
        "632c1e86c95a42c9a3b6c83257ed3f82",
        "632c1e86c95a42c9a3b6c83257ed3f83",
        "632c1e86c95a42c9a3b6c83257ed3f84"
      ],
      "metadata": {
        "data_type": "defect",
        "source_file": "defect_urls.txt",
        "scene_attributes": {
          "632c1e86c95a42c9a3b6c83257ed3f82": {
            "original_url": "https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119535",
            "data_name": "10000_ddi-application-667754027299119535",
            "line_number": 1,
            "priority": "high",
            "region": "beijing",
            "type": "lane_change"
          },
          "632c1e86c95a42c9a3b6c83257ed3f83": {
            "original_url": "https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119536",
            "data_name": "10000_ddi-application-667754027299119536",
            "line_number": 2,
            "priority": "low",
            "region": "shanghai",
            "type": "intersection"
          },
          "632c1e86c95a42c9a3b6c83257ed3f84": {
            "original_url": "https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119537",
            "data_name": "10000_ddi-application-667754027299119537",
            "line_number": 3,
            "priority": "medium",
            "region": "guangzhou"
          }
        }
      }
    }
  ]
}
```

### 3. æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯

```bash
# æ˜¾ç¤ºæ•°æ®é›†è¯¦ç»†ä¿¡æ¯ï¼ˆè‡ªåŠ¨æ£€æµ‹æ ¼å¼ï¼‰
python -m spdatalab.cli dataset-info --dataset-file datasets/dataset.parquet

# è·å–ç»Ÿè®¡ä¿¡æ¯
python -m spdatalab.cli dataset-stats --dataset-file datasets/dataset.parquet
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
æ•°æ®é›†ä¿¡æ¯:
  åç§°: Dataset
  æè¿°: GOD E2E training dataset
  åˆ›å»ºæ—¶é—´: 2025-01-27T10:30:00
  å­æ•°æ®é›†æ•°é‡: 3
  æ€»å”¯ä¸€åœºæ™¯æ•°: 4000000
  æ€»åœºæ™¯æ•°(å«å€å¢): 60000000

å­æ•°æ®é›†è¯¦æƒ…:
  1. lane_change_1_sub_ddi_2773412e2e_2025_05_18_11_07_59
     - OBSè·¯å¾„: obs://yw-ads-training-gy1/data/god/.../file.shrink
     - åœºæ™¯æ•°: 2000000
     - å€å¢å› å­: 20
     - å€å¢ååœºæ™¯æ•°: 40000000
  ...
```

### 4. åˆ—å‡ºåœºæ™¯ID

```bash
# åˆ—å‡ºæ‰€æœ‰åœºæ™¯IDï¼ˆä¸å«å€å¢ï¼‰
python -m spdatalab.cli list-scenes \
    --dataset-file datasets/dataset.parquet \
    --output scene_ids.txt

# åˆ—å‡ºç‰¹å®šå­æ•°æ®é›†çš„åœºæ™¯ID
python -m spdatalab.cli list-scenes \
    --dataset-file datasets/dataset.parquet \
    --subdataset "lane_change_1_sub_ddi_2773412e2e_2025_05_18_11_07_59" \
    --output lane_change_scenes.txt

# ç›´æ¥åœ¨æ§åˆ¶å°æ˜¾ç¤º
python -m spdatalab.cli list-scenes \
    --dataset-file datasets/dataset.parquet
```

### 5. å¯¼å‡ºåœºæ™¯IDä¸ºParquetæ ¼å¼

```bash
# å¯¼å‡ºå”¯ä¸€åœºæ™¯IDï¼ˆä¸å«å€å¢ï¼‰
python -m spdatalab.cli export-scene-ids \
    --dataset-file datasets/dataset.parquet \
    --output scene_ids_unique.parquet

# å¯¼å‡ºåŒ…å«å€å¢çš„å®Œæ•´åœºæ™¯IDåˆ—è¡¨
python -m spdatalab.cli export-scene-ids \
    --dataset-file datasets/dataset.parquet \
    --output scene_ids_full.parquet \
    --include-duplicates
```

### 6. æŸ¥è¯¢Parquetæ•°æ®é›†

```bash
# æŸ¥è¯¢æ‰€æœ‰æ•°æ®
python -m spdatalab.cli query-parquet \
    --parquet-file datasets/dataset.parquet

# æŒ‰å­æ•°æ®é›†è¿‡æ»¤
python -m spdatalab.cli query-parquet \
    --parquet-file datasets/dataset.parquet \
    --subdataset "lane_change_1_sub_ddi_2773412e2e_2025_05_18_11_07_59"

# æŒ‰å€å¢å› å­è¿‡æ»¤
python -m spdatalab.cli query-parquet \
    --parquet-file datasets/dataset.parquet \
    --duplication-factor 20

# ä¿å­˜æŸ¥è¯¢ç»“æœ
python -m spdatalab.cli query-parquet \
    --parquet-file datasets/dataset.parquet \
    --subdataset "lane_change_1_sub_ddi_2773412e2e_2025_05_18_11_07_59" \
    --output filtered_results.parquet
```

### 7. Python API ä½¿ç”¨ç¤ºä¾‹

```python
from spdatalab.dataset.dataset_manager import DatasetManager

# åˆ›å»ºç®¡ç†å™¨
manager = DatasetManager()

# åŠ è½½æ•°æ®é›†ï¼ˆè‡ªåŠ¨æ£€æµ‹æ ¼å¼ï¼‰
dataset = manager.load_dataset("datasets/dataset.parquet")

# è·å–å­æ•°æ®é›†ä¿¡æ¯
subdataset = manager.get_subdataset_info(dataset, "lane_change_1_sub_ddi_2773412e2e_2025_05_18_11_07_59")
if subdataset:
    print(f"å­æ•°æ®é›†: {subdataset.name}")
    print(f"åœºæ™¯æ•°: {subdataset.scene_count}")
    print(f"å€å¢å› å­: {subdataset.duplication_factor}")

# åˆ—å‡ºæ‰€æœ‰åœºæ™¯ID
all_scene_ids = manager.list_scene_ids(dataset)
print(f"æ€»åœºæ™¯æ•°: {len(all_scene_ids)}")

# åˆ—å‡ºç‰¹å®šå­æ•°æ®é›†çš„åœºæ™¯ID
subset_scene_ids = manager.list_scene_ids(dataset, "lane_change_1_sub_ddi_2773412e2e_2025_05_18_11_07_59")
print(f"å­æ•°æ®é›†åœºæ™¯æ•°: {len(subset_scene_ids)}")

# ç”ŸæˆåŒ…å«å€å¢çš„åœºæ™¯ID
duplicated_scenes = list(manager.generate_scene_list_with_duplication(dataset))
print(f"å€å¢åæ€»åœºæ™¯æ•°: {len(duplicated_scenes)}")

# æŸ¥è¯¢Parquetæ•°æ®ï¼ˆä»…å½“æ•°æ®é›†ä¸ºparquetæ ¼å¼æ—¶å¯ç”¨ï¼‰
df = manager.query_scenes_parquet("datasets/dataset.parquet", duplication_factor=20)
print(f"å€å¢å› å­ä¸º20çš„åœºæ™¯æ•°: {len(df)}")

# å¯¼å‡ºåœºæ™¯IDä¸ºParquetæ ¼å¼
manager.export_scene_ids_parquet(dataset, "scene_ids.parquet", include_duplicates=True)

# è·å–ç»Ÿè®¡ä¿¡æ¯
stats = manager.get_dataset_stats(dataset)
print(f"æ•°æ®é›†ç»Ÿè®¡: {stats}")

# ä¿å­˜ä¿®æ”¹åçš„æ•°æ®é›†
manager.save_dataset(dataset, "datasets/updated_dataset.parquet", format='parquet')
```

## æ•°æ®é›†æ–‡ä»¶æ ¼å¼

### JSONæ ¼å¼
```json
{
  "name": "Dataset",
  "description": "GOD E2E training dataset",
  "created_at": "2025-01-27T10:30:00.123456",
  "total_scenes": 15000,
  "total_unique_scenes": 1000,
  "metadata": {},
  "subdatasets": [
    {
      "name": "lane_change_1_sub_ddi_2773412e2e_2025_05_18_11_07_59",
      "obs_path": "obs://yw-ads-training-gy1/data/god/.../file.shrink",
      "duplication_factor": 20,
      "scene_count": 500,
      "scene_ids": ["scene_001", "scene_002", "..."],
      "metadata": {}
    }
  ]
}
```

### Parquetæ ¼å¼
Parquetæ ¼å¼å°†æ•°æ®å­˜å‚¨ä¸ºè¡¨æ ¼ç»“æ„ï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªåœºæ™¯IDï¼š

| dataset_name | dataset_description | subdataset_name | obs_path | duplication_factor | scene_id | metadata |
|--------------|---------------------|-----------------|----------|-------------------|----------|----------|
| Dataset | GOD E2E training... | golden... | obs://... | 20 | scene_001 | {...} |
| Dataset | GOD E2E training... | golden... | obs://... | 20 | scene_002 | {...} |

åŒæ—¶ä¼šç”Ÿæˆä¸€ä¸ª `.meta.json` æ–‡ä»¶ä¿å­˜æ•°æ®é›†å…ƒä¿¡æ¯ã€‚

## æ€§èƒ½å¯¹æ¯”

ä»¥ä¸‹æ˜¯400ä¸‡åœºæ™¯IDçš„æ•°æ®é›†çš„æ ¼å¼å¯¹æ¯”ï¼š

| æ ¼å¼ | æ–‡ä»¶å¤§å° | åŠ è½½æ—¶é—´ | å†…å­˜ä½¿ç”¨ | æŸ¥è¯¢æ€§èƒ½ |
|------|----------|----------|----------|----------|
| JSON | ~800MB | ~30s | ~2GB | æ…¢ |
| Parquet | ~120MB | ~3s | ~400MB | å¿« |

**ç»“è®º**ï¼šå¯¹äºå¤§å‹æ•°æ®é›†ï¼ŒParquetæ ¼å¼åœ¨æ‰€æœ‰æ–¹é¢éƒ½æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

## æœ€ä½³å®è·µ

### 1. æ ¼å¼é€‰æ‹©ç­–ç•¥
```python
# æ ¹æ®æ•°æ®é›†å¤§å°é€‰æ‹©æ ¼å¼
def choose_format(scene_count):
    if scene_count < 100000:
        return 'json'  # å°æ•°æ®é›†ï¼Œä½¿ç”¨JSONä¾¿äºè°ƒè¯•
    else:
        return 'parquet'  # å¤§æ•°æ®é›†ï¼Œä½¿ç”¨Parquetæå‡æ€§èƒ½
```

### 2. å¤§æ•°æ®é›†å¤„ç†
- ä¼˜å…ˆä½¿ç”¨Parquetæ ¼å¼
- åˆ©ç”¨æŸ¥è¯¢åŠŸèƒ½è¿›è¡Œæ•°æ®è¿‡æ»¤
- åˆ†æ‰¹å¤„ç†è¶…å¤§æ•°æ®é›†
- ä½¿ç”¨åˆé€‚çš„å€å¢å› å­é¿å…è¿‡åº¦é‡å¤

### 3. æ•°æ®é›†å‘½å
- ä½¿ç”¨æè¿°æ€§çš„æ•°æ®é›†åç§°
- åŒ…å«ç‰ˆæœ¬ä¿¡æ¯æˆ–æ—¥æœŸ
- æ·»åŠ æœ‰æ„ä¹‰çš„æè¿°
- é—®é¢˜å•æ•°æ®é›†å»ºè®®ä½¿ç”¨ `defect_` å‰ç¼€

### 4. é—®é¢˜å•æ•°æ®å¤„ç†
- ç¡®ä¿æ•°æ®åº“è¿æ¥æ­£å¸¸
- ç›‘æ§æŸ¥è¯¢å¤±è´¥ç‡ï¼ŒåŠæ—¶å¤„ç†å¼‚å¸¸URL
- ä½¿ç”¨æ‰©å±•å±æ€§æ ¼å¼è®°å½•é—®é¢˜å•çš„é‡è¦ä¿¡æ¯
- å®šæœŸæ¸…ç†å’Œæ›´æ–°é—®é¢˜å•æ•°æ®é›†
- **æ¨èåˆå¹¶æ¶æ„**ï¼šä¸€ä¸ªæ–‡ä»¶ç”Ÿæˆä¸€ä¸ªå­æ•°æ®é›†ï¼Œå‡å°‘bboxè¡¨æ•°é‡
- **åœºæ™¯å±æ€§ç®¡ç†**ï¼šåˆ©ç”¨`scene_attributes`å­˜å‚¨åœºæ™¯çº§åˆ«çš„è‡ªå®šä¹‰ä¿¡æ¯

### 5. æ€§èƒ½ä¼˜åŒ–
```python
# å¯¹äºéå¸¸å¤§çš„æ•°æ®é›†ï¼Œå¯ä»¥åˆ†æ‰¹æ„å»º
def build_large_dataset_in_batches(index_files, dataset_name):
    manager = DatasetManager()
    datasets = []
    
    for i, index_file in enumerate(index_files):
        batch_dataset = manager.build_dataset_from_index(
            index_file, 
            f"{dataset_name}_batch_{i}"
        )
        datasets.append(batch_dataset)
    
    # åˆå¹¶æ•°æ®é›†
    merged_dataset = merge_datasets(datasets, dataset_name)
    return merged_dataset
```

### 6. æŸ¥è¯¢ä¼˜åŒ–
```python
# ä½¿ç”¨pandasè¿›è¡Œå¤æ‚æŸ¥è¯¢
import pandas as pd

def complex_query_example(parquet_file):
    # ç›´æ¥ä½¿ç”¨pandasè¯»å–å’ŒæŸ¥è¯¢
    df = pd.read_parquet(parquet_file)
    
    # å¤æ‚è¿‡æ»¤æ¡ä»¶
    filtered = df[
        (df['duplication_factor'] >= 10) & 
        (df['subdataset_name'].str.contains('lane_change'))
    ]
    
    # ç»Ÿè®¡åˆ†æ
    stats = filtered.groupby('subdataset_name')['scene_id'].count()
    
    return filtered, stats
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **Parquetä¾èµ–ç¼ºå¤±**
   ```bash
   # å®‰è£…å¿…è¦ä¾èµ–
   pip install pandas pyarrow
   ```

2. **å†…å­˜ä¸è¶³ï¼ˆå¤„ç†è¶…å¤§æ•°æ®é›†ï¼‰**
   ```python
   # ä½¿ç”¨chunkedå¤„ç†
   def process_large_dataset_chunked(parquet_file, chunk_size=100000):
       for chunk in pd.read_parquet(parquet_file, chunksize=chunk_size):
           # å¤„ç†æ¯ä¸ªchunk
           process_chunk(chunk)
   ```

3. **æ ¼å¼è½¬æ¢**
   ```bash
   # JSONè½¬Parquet
   python -c "
   from spdatalab.dataset.dataset_manager import DatasetManager
   manager = DatasetManager()
   dataset = manager.load_dataset('dataset.json')
   manager.save_dataset(dataset, 'dataset.parquet', format='parquet')
   "
   ```

4. **é—®é¢˜å•æ•°æ®åº“è¿æ¥å¤±è´¥**
   ```python
   # æ£€æŸ¥æ•°æ®åº“è¿æ¥
   from spdatalab.common.io_hive import hive_cursor
   
   try:
       with hive_cursor() as cur:
           cur.execute("SELECT 1")
           print("æ•°æ®åº“è¿æ¥æ­£å¸¸")
   except Exception as e:
       print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
   ```

5. **é—®é¢˜å•URLè§£æå¤±è´¥**
   ```python
   # æ£€æŸ¥URLæ ¼å¼
   url = "https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119535"
   
   import re
   pattern = r'dataName=([^&]+)'
   match = re.search(pattern, url)
   if match:
       data_name = match.group(1)
       print(f"æå–çš„æ•°æ®åç§°: {data_name}")
   else:
       print("URLæ ¼å¼ä¸æ­£ç¡®")
   ```

6. **é—®é¢˜å•æŸ¥è¯¢æ— ç»“æœ**
   ```python
   # æ‰‹åŠ¨æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
   from spdatalab.common.io_hive import hive_cursor
   
   data_name = "10000_ddi-application-667754027299119535"
   
   with hive_cursor() as cur:
       # æ£€æŸ¥ç¬¬ä¸€æ­¥æŸ¥è¯¢
       cur.execute(
           "SELECT defect_id FROM elasticsearch_ros.ods_ddi_index002_datalake WHERE id = %s",
           (data_name,)
       )
       result = cur.fetchone()
       if result:
           defect_id = result[0]
           print(f"æ‰¾åˆ°defect_id: {defect_id}")
           
           # æ£€æŸ¥ç¬¬äºŒæ­¥æŸ¥è¯¢
           cur.execute(
               "SELECT id FROM transform.ods_t_data_fragment_datalake WHERE origin_source_id = %s",
               (defect_id,)
           )
           result = cur.fetchone()
           if result:
               scene_id = result[0]
               print(f"æ‰¾åˆ°scene_id: {scene_id}")
           else:
               print(f"æœªæ‰¾åˆ°scene_idï¼Œdefect_id: {defect_id}")
       else:
           print(f"æœªæ‰¾åˆ°defect_idï¼Œdata_name: {data_name}")
   ```

### æ—¥å¿—é…ç½®

ç³»ç»Ÿä½¿ç”¨Python loggingæ¨¡å—ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®æ—¥å¿—çº§åˆ«è·å–è¯¦ç»†ä¿¡æ¯ï¼š

```python
import logging
logging.basicConfig(level=logging.INFO)
```

æˆ–åœ¨å‘½ä»¤è¡Œä¸­æŸ¥çœ‹è¯¦ç»†è¾“å‡ºã€‚

## æ‰©å±•åŠŸèƒ½

### 1. è‡ªå®šä¹‰æŸ¥è¯¢
```python
def custom_parquet_query(parquet_file, custom_filter):
    """è‡ªå®šä¹‰ParquetæŸ¥è¯¢å‡½æ•°ã€‚"""
    df = pd.read_parquet(parquet_file)
    
    # åº”ç”¨è‡ªå®šä¹‰è¿‡æ»¤å™¨
    filtered_df = df[custom_filter(df)]
    
    return filtered_df

# ä½¿ç”¨ç¤ºä¾‹
result = custom_parquet_query(
    "dataset.parquet",
    lambda df: (df['scene_id'].str.startswith('scene_abc')) & 
               (df['duplication_factor'] > 5)
)
```

### 2. æ‰¹é‡æ“ä½œ
```python
def batch_export_by_subdataset(dataset, output_dir):
    """æŒ‰å­æ•°æ®é›†æ‰¹é‡å¯¼å‡ºã€‚"""
    manager = DatasetManager()
    
    for subdataset in dataset.subdatasets:
        output_file = Path(output_dir) / f"{subdataset.name}.parquet"
        
        # åˆ›å»ºåªåŒ…å«å½“å‰å­æ•°æ®é›†çš„ä¸´æ—¶æ•°æ®é›†
        temp_dataset = Dataset(
            name=f"temp_{subdataset.name}",
            subdatasets=[subdataset]
        )
        
        manager.export_scene_ids_parquet(temp_dataset, str(output_file))
```

### 3. æ•°æ®éªŒè¯
```python
def validate_dataset_integrity(parquet_file):
    """éªŒè¯æ•°æ®é›†å®Œæ•´æ€§ã€‚"""
    df = pd.read_parquet(parquet_file)
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    required_columns = ['dataset_name', 'subdataset_name', 'scene_id', 'duplication_factor']
    missing_columns = set(required_columns) - set(df.columns)
    
    if missing_columns:
        raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_columns}")
    
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    null_counts = df.isnull().sum()
    if null_counts.any():
        print(f"å‘ç°ç©ºå€¼: {null_counts[null_counts > 0]}")
    
    # æ£€æŸ¥é‡å¤scene_idï¼ˆåœ¨åŒä¸€å­æ•°æ®é›†å†…ï¼‰
    duplicates = df.groupby(['subdataset_name', 'scene_id']).size()
    duplicates = duplicates[duplicates > 1]
    
    if len(duplicates) > 0:
        print(f"å‘ç°é‡å¤scene_id: {len(duplicates)}ä¸ª")
    
    return {
        'total_records': len(df),
        'unique_scene_ids': df['scene_id'].nunique(),
        'unique_subdatasets': df['subdataset_name'].nunique(),
        'has_nulls': null_counts.any(),
        'has_duplicates': len(duplicates) > 0
    }
``` 

### 4. é—®é¢˜å•æ•°æ®å¤„ç†
```python
def process_defect_urls_with_retry(url_file, max_retries=3):
    """å¤„ç†é—®é¢˜å•URLï¼Œæ”¯æŒé‡è¯•æœºåˆ¶ã€‚"""
    from spdatalab.dataset.dataset_manager import DatasetManager
    
    manager = DatasetManager(defect_mode=True)
    failed_urls = []
    
    for retry in range(max_retries):
        try:
            dataset = manager.build_dataset_from_index(
                url_file,
                f"DefectDataset_retry_{retry}",
                "é—®é¢˜å•æ•°æ®é›†"
            )
            
            # æ£€æŸ¥å¤±è´¥ç‡
            stats = manager.stats
            fail_rate = stats['failed_files'] / stats['total_files'] if stats['total_files'] > 0 else 0
            
            if fail_rate > 0.1:  # å¤±è´¥ç‡è¶…è¿‡10%
                print(f"è­¦å‘Š: å¤±è´¥ç‡è¿‡é«˜ ({fail_rate:.2%})ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®åº“è¿æ¥")
            
            return dataset
            
        except Exception as e:
            print(f"ç¬¬ {retry + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
            if retry == max_retries - 1:
                raise
    
    return None

def analyze_defect_dataset(dataset_file):
    """åˆ†æé—®é¢˜å•æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    from spdatalab.dataset.dataset_manager import DatasetManager
    
    manager = DatasetManager()
    dataset = manager.load_dataset(dataset_file)
    
    # ç»Ÿè®¡é—®é¢˜å•å±æ€§
    attributes_stats = {}
    
    for subdataset in dataset.subdatasets:
        metadata = subdataset.metadata
        
        # ç»Ÿè®¡å„ç§å±æ€§
        for key, value in metadata.items():
            if key.startswith('data_') or key in ['original_url', 'line_number']:
                continue  # è·³è¿‡ç³»ç»Ÿå­—æ®µ
                
            if key not in attributes_stats:
                attributes_stats[key] = {}
            
            if value not in attributes_stats[key]:
                attributes_stats[key][value] = 0
            attributes_stats[key][value] += 1
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print("é—®é¢˜å•æ•°æ®é›†åˆ†æ:")
    print(f"æ€»è®¡é—®é¢˜å•æ•°é‡: {len(dataset.subdatasets)}")
    print(f"æ€»è®¡åœºæ™¯æ•°: {dataset.total_scenes}")
    
    for attr, values in attributes_stats.items():
        print(f"\n{attr} åˆ†å¸ƒ:")
        for value, count in sorted(values.items(), key=lambda x: x[1], reverse=True):
            print(f"  {value}: {count}")
    
    return attributes_stats
```

## é—®é¢˜å•æ•°æ®å¤„ç†å®Œæ•´ç¤ºä¾‹

### 1. å‡†å¤‡é—®é¢˜å•URLæ–‡ä»¶

åˆ›å»º `defect_urls.txt` æ–‡ä»¶ï¼š
```
https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119535|priority=high|region=beijing|type=lane_change
https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119536|priority=medium|region=shanghai|type=intersection
https://pre-prod.adscloud.huawei.com/ddi-app/#/layout/ddi-system-evaluation/event-list-detail?dataName=10000_ddi-application-667754027299119537|priority=low|region=guangzhou|type=merging
```

### 2. æ„å»ºé—®é¢˜å•æ•°æ®é›†

```python
from spdatalab.dataset.dataset_manager import DatasetManager
import logging

# å¯ç”¨è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.INFO)

# åˆ›å»ºæ•°æ®é›†ç®¡ç†å™¨
manager = DatasetManager(defect_mode=True)

# æ„å»ºæ•°æ®é›†
dataset = manager.build_dataset_from_index(
    "defect_urls.txt",
    "DefectAnalysisDataset",
    "é—®é¢˜å•åˆ†ææ•°æ®é›† v1.0"
)

# ä¿å­˜æ•°æ®é›†
manager.save_dataset(dataset, "defect_dataset.json", format='json')
manager.save_dataset(dataset, "defect_dataset.parquet", format='parquet')

print(f"æ•°æ®é›†æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(dataset.subdatasets)} ä¸ªé—®é¢˜å•")
```

### 3. æ•°æ®é›†åˆ†æ

```python
# åŠ è½½æ•°æ®é›†
dataset = manager.load_dataset("defect_dataset.json")

# è·å–ç»Ÿè®¡ä¿¡æ¯
stats = manager.get_dataset_stats(dataset)
print("æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
for key, value in stats.items():
    print(f"  {key}: {value}")

# åˆ†æé—®é¢˜å•å±æ€§
analyze_defect_dataset("defect_dataset.parquet")

# è¾“å‡ºæ‰€æœ‰åœºæ™¯ID
scene_ids = manager.list_scene_ids(dataset)
print(f"\næå–çš„åœºæ™¯ID:")
for i, scene_id in enumerate(scene_ids, 1):
    print(f"  {i}. {scene_id}")
```

### 4. ä¸bboxé›†æˆä½¿ç”¨

```python
# å¯¼å‡ºä¸ºbboxå…¼å®¹æ ¼å¼
with open("bbox_scene_ids.txt", "w") as f:
    for scene_id in scene_ids:
        f.write(f"{scene_id}\n")

print("åœºæ™¯IDå·²å¯¼å‡ºä¸ºbboxå…¼å®¹æ ¼å¼: bbox_scene_ids.txt")
```

### 5. å¤„ç†å¤±è´¥çš„é—®é¢˜å•

```python
# æ£€æŸ¥å¤„ç†ç»Ÿè®¡
print(f"å¤„ç†ç»Ÿè®¡:")
print(f"  æ€»è®¡å¤„ç†: {manager.stats['total_files']} ä¸ªURL")
print(f"  æˆåŠŸå¤„ç†: {manager.stats['processed_files']} ä¸ª")
print(f"  å¤±è´¥å¤„ç†: {manager.stats['failed_files']} ä¸ª")
print(f"  æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {manager.stats['defect_query_failed']} ä¸ª")
print(f"  æ— scene_id: {manager.stats['defect_no_scene']} ä¸ª")

# å¦‚æœå¤±è´¥ç‡è¿‡é«˜ï¼Œè¿›è¡Œæ•…éšœæ’é™¤
if manager.stats['failed_files'] > 0:
    print(f"\nå¤±è´¥ç‡: {manager.stats['failed_files']/manager.stats['total_files']:.2%}")
    print("å»ºè®®æ£€æŸ¥:")
    print("  1. æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸")
    print("  2. URLæ ¼å¼æ˜¯å¦æ­£ç¡®")
    print("  3. æ•°æ®æ˜¯å¦å­˜åœ¨äºæ•°æ®åº“ä¸­")
```

### 6. å‘½ä»¤è¡Œä½¿ç”¨

```bash
# æ„å»ºé—®é¢˜å•æ•°æ®é›†
python -m spdatalab.cli build-dataset \
    --index-file defect_urls.txt \
    --dataset-name "DefectAnalysisDataset" \
    --description "é—®é¢˜å•åˆ†ææ•°æ®é›† v1.0" \
    --output defect_dataset.parquet \
    --format parquet \
    --defect-mode

# æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯
python -m spdatalab.cli dataset-info \
    --dataset-file defect_dataset.parquet

# å¯¼å‡ºåœºæ™¯ID
python -m spdatalab.cli list-scenes \
    --dataset-file defect_dataset.parquet \
    --output bbox_scene_ids.txt
```

è¿™æ ·å°±å®Œæˆäº†ä»é—®é¢˜å•URLåˆ°å¯ç”¨äºbboxåˆ†æçš„scene_idçš„å®Œæ•´æµç¨‹ã€‚ 