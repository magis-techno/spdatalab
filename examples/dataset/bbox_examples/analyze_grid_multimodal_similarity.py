#!/usr/bin/env python3
"""
Gridå¤šæ¨¡æ€ç›¸ä¼¼æ€§åˆ†æ
===================

åŠŸèƒ½ï¼š
1. ä»å†—ä½™åˆ†æç»“æœä¸­é€‰æ‹©é«˜å†—ä½™grid
2. æå–gridå†…çš„dataset_nameåˆ—è¡¨
3. è°ƒç”¨å¤šæ¨¡æ€APIæ£€ç´¢ç›¸å…³å›¾ç‰‡
4. åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ

ä½¿ç”¨æ–¹æ³•ï¼š
    # åŸºç¡€ä½¿ç”¨ï¼šåˆ†æA72åŸå¸‚çš„top1å†—ä½™grid
    python analyze_grid_multimodal_similarity.py --city A72
    
    # æŒ‡å®šgridæ’åå’ŒæŸ¥è¯¢æ–‡æœ¬
    python analyze_grid_multimodal_similarity.py --city A72 --grid-rank 2 --query-text "å¤œæ™š"
    
    # æŒ‡å®šåˆ†ææ—¥æœŸå’Œè¿”å›ç»“æœæ•°
    python analyze_grid_multimodal_similarity.py \
        --city A72 \
        --analysis-date 2025-10-09 \
        --max-results 200
    
    # æŒ‡å®šcollectionï¼ˆç›¸æœºï¼‰
    python analyze_grid_multimodal_similarity.py \
        --city A72 \
        --collection ddi_collection_camera_encoded_2

ä¾èµ–ï¼š
    - city_grid_density è¡¨ï¼ˆéœ€è¦å…ˆè¿è¡Œ batch_grid_analysis.pyï¼‰
    - clips_bbox_unified è§†å›¾
    - å¤šæ¨¡æ€APIé…ç½®ï¼ˆ.envæ–‡ä»¶ï¼‰
"""

import sys
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from collections import Counter
import statistics

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from spdatalab.dataset.bbox import LOCAL_DSN
    from spdatalab.dataset.multimodal_data_retriever import APIConfig, MultimodalRetriever
except ImportError:
    sys.path.insert(0, str(project_root / "src"))
    from spdatalab.dataset.bbox import LOCAL_DSN
    from spdatalab.dataset.multimodal_data_retriever import APIConfig, MultimodalRetriever

from sqlalchemy import create_engine, text


def select_target_grid(conn, city_id: str, grid_rank: int = 1, 
                      analysis_date: Optional[str] = None) -> Optional[Dict]:
    """ä»city_grid_densityé€‰æ‹©ç›®æ ‡grid
    
    Args:
        conn: æ•°æ®åº“è¿æ¥
        city_id: åŸå¸‚ID
        grid_rank: Gridæ’åï¼ˆ1è¡¨ç¤ºæœ€é«˜å†—ä½™ï¼‰
        analysis_date: åˆ†ææ—¥æœŸï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æœ€æ–°æ—¥æœŸ
        
    Returns:
        Gridä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«grid_x, grid_y, bbox_countç­‰
    """
    print(f"\nğŸ¯ é€‰æ‹©ç›®æ ‡Grid")
    print("=" * 60)
    
    # æ„å»ºæŸ¥è¯¢SQL
    if analysis_date:
        sql = text("""
            SELECT 
                grid_x,
                grid_y,
                bbox_count,
                subdataset_count,
                scene_count,
                involved_subdatasets,
                involved_scenes,
                ST_AsText(geometry) as grid_geom,
                analysis_date
            FROM city_grid_density
            WHERE city_id = :city_id 
              AND analysis_date = :analysis_date
            ORDER BY bbox_count DESC
            LIMIT 1 OFFSET :offset
        """)
        result = conn.execute(sql, {
            'city_id': city_id,
            'analysis_date': analysis_date,
            'offset': grid_rank - 1
        }).fetchone()
    else:
        sql = text("""
            SELECT 
                grid_x,
                grid_y,
                bbox_count,
                subdataset_count,
                scene_count,
                involved_subdatasets,
                involved_scenes,
                ST_AsText(geometry) as grid_geom,
                analysis_date
            FROM city_grid_density
            WHERE city_id = :city_id 
              AND analysis_date = (SELECT MAX(analysis_date) FROM city_grid_density WHERE city_id = :city_id)
            ORDER BY bbox_count DESC
            LIMIT 1 OFFSET :offset
        """)
        result = conn.execute(sql, {
            'city_id': city_id,
            'offset': grid_rank - 1
        }).fetchone()
    
    if not result:
        print(f"âŒ æœªæ‰¾åˆ°åŸå¸‚ {city_id} çš„Gridæ•°æ®")
        print(f"ğŸ’¡ æç¤ºï¼š")
        print(f"   1. ç¡®è®¤åŸå¸‚IDæ˜¯å¦æ­£ç¡®")
        print(f"   2. ç¡®è®¤å·²è¿è¡Œè¿‡ batch_grid_analysis.py")
        print(f"   3. å°è¯•é™ä½ --grid-rank å‚æ•°")
        return None
    
    grid_info = {
        'city_id': city_id,
        'grid_x': result.grid_x,
        'grid_y': result.grid_y,
        'bbox_count': result.bbox_count,
        'subdataset_count': result.subdataset_count,
        'scene_count': result.scene_count,
        'involved_subdatasets': result.involved_subdatasets or [],
        'involved_scenes': result.involved_scenes or [],
        'grid_geom': result.grid_geom,
        'analysis_date': str(result.analysis_date)
    }
    
    print(f"ğŸ“ é€‰æ‹©Grid: {city_id} åŸå¸‚, Rank #{grid_rank}")
    print(f"   åˆ†ææ—¥æœŸ: {grid_info['analysis_date']}")
    print(f"   Gridåæ ‡: ({grid_info['grid_x']}, {grid_info['grid_y']})")
    print(f"   BBoxæ•°é‡: {grid_info['bbox_count']}")
    print(f"   Sceneæ•°é‡: {grid_info['scene_count']}")
    print(f"   Datasetæ•°é‡: {grid_info['subdataset_count']}")
    
    return grid_info


def extract_grid_datasets(conn, grid_info: Dict) -> Tuple[List[str], Dict]:
    """æå–gridå†…çš„datasetåˆ—è¡¨å’Œç»Ÿè®¡ä¿¡æ¯
    
    Args:
        conn: æ•°æ®åº“è¿æ¥
        grid_info: Gridä¿¡æ¯
        
    Returns:
        (dataset_nameåˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯å­—å…¸)
    """
    print(f"\nğŸ“¦ æå–Gridå†…çš„æ•°æ®")
    print("=" * 60)
    
    # é€šè¿‡ç©ºé—´è¿æ¥è·å–gridå†…çš„æ‰€æœ‰bbox
    sql = text("""
        SELECT DISTINCT 
            b.dataset_name,
            b.scene_token,
            b.subdataset_name
        FROM city_grid_density g
        JOIN clips_bbox_unified b ON ST_Intersects(g.geometry, b.geometry)
        WHERE g.city_id = :city_id 
          AND g.grid_x = :grid_x 
          AND g.grid_y = :grid_y
          AND g.analysis_date = :analysis_date
          AND b.all_good = true
        ORDER BY b.dataset_name, b.scene_token
    """)
    
    results = conn.execute(sql, {
        'city_id': grid_info['city_id'],
        'grid_x': grid_info['grid_x'],
        'grid_y': grid_info['grid_y'],
        'analysis_date': grid_info['analysis_date']
    }).fetchall()
    
    if not results:
        print(f"âš ï¸ Gridå†…æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„bboxæ•°æ®")
        return [], {}
    
    # æå–å”¯ä¸€çš„dataset_name
    dataset_names = list(set(row.dataset_name for row in results if row.dataset_name))
    
    # ç»Ÿè®¡ä¿¡æ¯
    scene_tokens = list(set(row.scene_token for row in results if row.scene_token))
    subdataset_names = list(set(row.subdataset_name for row in results if row.subdataset_name))
    
    # æŒ‰datasetç»Ÿè®¡sceneæ•°é‡
    dataset_scene_count = Counter()
    for row in results:
        if row.dataset_name and row.scene_token:
            dataset_scene_count[row.dataset_name] += 1
    
    stats = {
        'total_datasets': len(dataset_names),
        'total_scenes': len(scene_tokens),
        'total_subdatasets': len(subdataset_names),
        'total_records': len(results),
        'dataset_scene_count': dataset_scene_count
    }
    
    print(f"âœ… æå–å®Œæˆ:")
    print(f"   Datasetæ•°é‡: {stats['total_datasets']}")
    print(f"   Sceneæ•°é‡: {stats['total_scenes']}")
    print(f"   å­æ•°æ®é›†æ•°é‡: {stats['total_subdatasets']}")
    print(f"   æ€»è®°å½•æ•°: {stats['total_records']}")
    
    # æ˜¾ç¤ºå‰10ä¸ªdataset
    if dataset_names:
        print(f"\nğŸ“‹ Gridå†…çš„æ•°æ®é›† (å‰10ä¸ª):")
        for i, ds_name in enumerate(dataset_names[:10], 1):
            scene_cnt = dataset_scene_count.get(ds_name, 0)
            # æˆªæ–­è¿‡é•¿çš„dataset_name
            display_name = ds_name if len(ds_name) <= 60 else ds_name[:57] + "..."
            print(f"   {i}. {display_name} ({scene_cnt} scenes)")
        
        if len(dataset_names) > 10:
            print(f"   ... è¿˜æœ‰ {len(dataset_names) - 10} ä¸ªæ•°æ®é›†")
    
    return dataset_names, stats


def call_multimodal_api(retriever: MultimodalRetriever, query_text: str, 
                       collection: str, city_id: str, dataset_names: List[str],
                       max_results: int = 100) -> List[Dict]:
    """è°ƒç”¨å¤šæ¨¡æ€APIæ£€ç´¢
    
    Args:
        retriever: MultimodalRetrieverå®ä¾‹
        query_text: æŸ¥è¯¢æ–‡æœ¬
        collection: Collectionåç§°
        city_id: åŸå¸‚ID
        dataset_names: Datasetåç§°åˆ—è¡¨
        max_results: æœ€å¤§è¿”å›ç»“æœæ•°
        
    Returns:
        æ£€ç´¢ç»“æœåˆ—è¡¨
    """
    print(f"\nğŸ” è°ƒç”¨å¤šæ¨¡æ€API")
    print("=" * 60)
    
    # æ„å»ºåŸå¸‚è¿‡æ»¤æ¡ä»¶
    filter_dict = {
        "conditions": [[{
            "field": "ddi_basic.city_code",
            "func": "$eq",
            "value": city_id,
            "format": "string"
        }]],
        "logic": ["$and"],
        "cursorKey": None
    }
    
    print(f"ğŸ“ æŸ¥è¯¢å‚æ•°:")
    print(f"   æŸ¥è¯¢æ–‡æœ¬: '{query_text}'")
    print(f"   Collection: {collection}")
    print(f"   åŸå¸‚è¿‡æ»¤: {city_id}")
    print(f"   Datasetè¿‡æ»¤: {len(dataset_names)} ä¸ª")
    print(f"   æœ€å¤§ç»“æœæ•°: {max_results}")
    
    try:
        # è°ƒç”¨API
        results = retriever.retrieve_by_text(
            text=query_text,
            collection=collection,
            count=max_results,
            dataset_name=dataset_names,
            filter_dict=filter_dict
        )
        
        print(f"âœ… APIè°ƒç”¨æˆåŠŸ: è¿”å› {len(results)} æ¡ç»“æœ")
        
        return results
        
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


def analyze_similarity(results: List[Dict], top_n: int = 10) -> None:
    """åˆ†æç›¸ä¼¼åº¦åˆ†å¸ƒ
    
    Args:
        results: æ£€ç´¢ç»“æœåˆ—è¡¨
        top_n: æ˜¾ç¤ºtop Nä¸ªç»“æœ
    """
    print(f"\nğŸ“Š ç›¸ä¼¼åº¦åˆ†æ")
    print("=" * 60)
    
    if not results:
        print("âš ï¸ æ²¡æœ‰ç»“æœå¯åˆ†æ")
        return
    
    # æå–ç›¸ä¼¼åº¦å€¼
    similarities = [r.get('similarity', 0.0) for r in results]
    similarities = [s for s in similarities if s > 0]  # è¿‡æ»¤æ— æ•ˆå€¼
    
    if not similarities:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„ç›¸ä¼¼åº¦æ•°æ®")
        return
    
    # åŸºç¡€ç»Ÿè®¡
    min_sim = min(similarities)
    max_sim = max(similarities)
    avg_sim = statistics.mean(similarities)
    median_sim = statistics.median(similarities)
    
    print(f"ğŸ“ˆ ç›¸ä¼¼åº¦ç»Ÿè®¡:")
    print(f"   èŒƒå›´: {min_sim:.3f} ~ {max_sim:.3f}")
    print(f"   å¹³å‡: {avg_sim:.3f}")
    print(f"   ä¸­ä½æ•°: {median_sim:.3f}")
    print(f"   æ ·æœ¬æ•°: {len(similarities)}")
    
    # ç›¸ä¼¼åº¦åˆ†å¸ƒç›´æ–¹å›¾
    print(f"\nğŸ“Š ç›¸ä¼¼åº¦åˆ†å¸ƒç›´æ–¹å›¾:")
    bins = [(i/10, (i+1)/10) for i in range(10)]  # 0.0-0.1, 0.1-0.2, ..., 0.9-1.0
    
    for low, high in bins:
        count = sum(1 for s in similarities if low <= s < high)
        if count > 0 or (low <= avg_sim < high):  # æ˜¾ç¤ºæœ‰æ•°æ®çš„åŒºé—´æˆ–åŒ…å«å¹³å‡å€¼çš„åŒºé—´
            bar_length = int(count / len(similarities) * 50)  # æœ€å¤š50ä¸ªå­—ç¬¦
            bar = "â–ˆ" * bar_length
            pct = count / len(similarities) * 100
            print(f"   {low:.1f}-{high:.1f}: {bar} ({count}, {pct:.1f}%)")
    
    # æŒ‰datasetåˆ†ç»„ç»Ÿè®¡
    print(f"\nğŸ“¦ æŒ‰Datasetåˆ†ç»„:")
    dataset_sims = {}
    for r in results:
        ds_name = r.get('dataset_name', 'unknown')
        sim = r.get('similarity', 0.0)
        if ds_name not in dataset_sims:
            dataset_sims[ds_name] = []
        dataset_sims[ds_name].append(sim)
    
    # æŒ‰å¹³å‡ç›¸ä¼¼åº¦æ’åº
    dataset_stats = []
    for ds_name, sims in dataset_sims.items():
        dataset_stats.append({
            'dataset': ds_name,
            'count': len(sims),
            'avg_similarity': statistics.mean(sims),
            'max_similarity': max(sims)
        })
    
    dataset_stats.sort(key=lambda x: x['avg_similarity'], reverse=True)
    
    # æ˜¾ç¤ºå‰5ä¸ªdataset
    print(f"   Top 5 Dataset (æŒ‰å¹³å‡ç›¸ä¼¼åº¦):")
    for i, ds in enumerate(dataset_stats[:5], 1):
        ds_display = ds['dataset'] if len(ds['dataset']) <= 50 else ds['dataset'][:47] + "..."
        print(f"   {i}. {ds_display}")
        print(f"      ç»“æœæ•°: {ds['count']}, å¹³å‡ç›¸ä¼¼åº¦: {ds['avg_similarity']:.3f}, æœ€é«˜: {ds['max_similarity']:.3f}")
    
    # Top N æœ€ç›¸ä¼¼ç»“æœ
    print(f"\nğŸ” Top {top_n} æœ€ç›¸ä¼¼ç»“æœ:")
    sorted_results = sorted(results, key=lambda x: x.get('similarity', 0.0), reverse=True)
    
    for i, r in enumerate(sorted_results[:top_n], 1):
        sim = r.get('similarity', 0.0)
        ds_name = r.get('dataset_name', 'unknown')
        timestamp = r.get('timestamp', 0)
        img_path = r.get('metadata', {}).get('img_path', 'N/A')
        
        # æˆªæ–­é•¿è·¯å¾„
        if len(ds_name) > 50:
            ds_name_display = ds_name[:47] + "..."
        else:
            ds_name_display = ds_name
        
        if len(img_path) > 80:
            img_path_display = img_path[:77] + "..."
        else:
            img_path_display = img_path
        
        print(f"\n   {i}. ç›¸ä¼¼åº¦: {sim:.3f}")
        print(f"      Dataset: {ds_name_display}")
        print(f"      Timestamp: {timestamp}")
        print(f"      å›¾ç‰‡è·¯å¾„: {img_path_display}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Gridå¤šæ¨¡æ€ç›¸ä¼¼æ€§åˆ†æ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºç¡€ä½¿ç”¨
  python %(prog)s --city A72
  
  # å®Œæ•´å‚æ•°
  python %(prog)s --city A72 --grid-rank 2 --query-text "å¤œæ™š" --max-results 200
        """
    )
    
    parser.add_argument('--city', required=True, 
                       help='åŸå¸‚IDï¼ˆå¦‚ A72, A263ï¼‰')
    parser.add_argument('--grid-rank', type=int, default=1,
                       help='Gridæ’åï¼Œ1è¡¨ç¤ºæœ€é«˜å†—ä½™ï¼ˆé»˜è®¤: 1ï¼‰')
    parser.add_argument('--query-text', default='ç™½å¤©',
                       help='æŸ¥è¯¢æ–‡æœ¬ï¼ˆé»˜è®¤: "ç™½å¤©"ï¼‰')
    parser.add_argument('--collection', default='ddi_collection_camera_encoded_1',
                       help='Collectionåç§°ï¼ˆé»˜è®¤: ddi_collection_camera_encoded_1ï¼‰')
    parser.add_argument('--max-results', type=int, default=100,
                       help='æœ€å¤§è¿”å›ç»“æœæ•°ï¼ˆé»˜è®¤: 100ï¼‰')
    parser.add_argument('--analysis-date', type=str,
                       help='åˆ†ææ—¥æœŸï¼ˆæ ¼å¼: YYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°æ—¥æœŸ')
    parser.add_argument('--top-n', type=int, default=10,
                       help='æ˜¾ç¤ºtop Nä¸ªæœ€ç›¸ä¼¼ç»“æœï¼ˆé»˜è®¤: 10ï¼‰')
    
    args = parser.parse_args()
    
    print("ğŸš€ Gridå¤šæ¨¡æ€ç›¸ä¼¼æ€§åˆ†æ")
    print("=" * 60)
    
    try:
        # 1. è¿æ¥æ•°æ®åº“
        print("\nğŸ”Œ è¿æ¥æ•°æ®åº“...")
        engine = create_engine(LOCAL_DSN, future=True)
        
        with engine.connect() as conn:
            # æµ‹è¯•è¿æ¥
            conn.execute(text("SELECT 1"))
            print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
            
            # 2. é€‰æ‹©ç›®æ ‡Grid
            grid_info = select_target_grid(
                conn, 
                args.city, 
                args.grid_rank,
                args.analysis_date
            )
            
            if not grid_info:
                return 1
            
            # 3. æå–Gridå†…çš„æ•°æ®
            dataset_names, stats = extract_grid_datasets(conn, grid_info)
            
            if not dataset_names:
                print("\nâŒ Gridå†…æ²¡æœ‰æœ‰æ•ˆçš„datasetï¼Œæ— æ³•ç»§ç»­")
                return 1
            
            print(f"\nğŸ’¡ æç¤º: å°†ä½¿ç”¨ {len(dataset_names)} ä¸ªdatasetè¿›è¡Œè¿‡æ»¤")
            if len(dataset_names) > 50:
                print(f"âš ï¸ Datasetæ•°é‡è¾ƒå¤šï¼ŒAPIè°ƒç”¨å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
        
        # 4. åˆå§‹åŒ–å¤šæ¨¡æ€API
        print(f"\nğŸ”§ åˆå§‹åŒ–å¤šæ¨¡æ€API...")
        api_config = APIConfig.from_env()
        retriever = MultimodalRetriever(api_config)
        print(f"âœ… APIé…ç½®åŠ è½½æˆåŠŸ")
        
        # 5. è°ƒç”¨å¤šæ¨¡æ€API
        results = call_multimodal_api(
            retriever,
            args.query_text,
            args.collection,
            args.city,
            dataset_names,
            args.max_results
        )
        
        if not results:
            print("\nâš ï¸ APIæœªè¿”å›ç»“æœ")
            print("ğŸ’¡ å¯èƒ½åŸå› :")
            print("   - æŸ¥è¯¢æ–‡æœ¬ä¸gridå†…çš„æ•°æ®ä¸åŒ¹é…")
            print("   - Datasetè¿‡æ»¤è¿‡äºä¸¥æ ¼")
            print("   - APIé…ç½®æˆ–ç½‘ç»œé—®é¢˜")
            return 1
        
        # 6. åˆ†æç›¸ä¼¼åº¦
        analyze_similarity(results, args.top_n)
        
        # 7. æ€»ç»“
        print(f"\n" + "=" * 60)
        print(f"âœ… åˆ†æå®Œæˆ")
        print(f"=" * 60)
        print(f"ğŸ“ Grid: {args.city} ({grid_info['grid_x']}, {grid_info['grid_y']})")
        print(f"ğŸ“¦ Datasetæ•°é‡: {len(dataset_names)}")
        print(f"ğŸ” æ£€ç´¢ç»“æœ: {len(results)}")
        print(f"ğŸ“Š ç›¸ä¼¼åº¦èŒƒå›´: {min(r.get('similarity', 0) for r in results):.3f} ~ "
              f"{max(r.get('similarity', 0) for r in results):.3f}")
        
        print(f"\nğŸ’¡ åç»­å¯ä»¥:")
        print(f"   1. å°è¯•ä¸åŒçš„æŸ¥è¯¢æ–‡æœ¬ï¼ˆ--query-textï¼‰")
        print(f"   2. åˆ†æå…¶ä»–æ’åçš„gridï¼ˆ--grid-rankï¼‰")
        print(f"   3. ä¸‹è½½å›¾ç‰‡è¿›è¡Œè§†è§‰ç›¸ä¼¼æ€§åˆ†æï¼ˆå¾…å®ç°ï¼‰")
        print("=" * 60)
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())

