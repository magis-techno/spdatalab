#!/usr/bin/env python3
"""
è¯Šæ–­Gridèšç±»é—®é¢˜çš„ä¸´æ—¶è„šæœ¬

å¿«é€Ÿæ£€æŸ¥ï¼š
1. è½¨è¿¹ç‚¹æ•°æ®æ˜¯å¦æ­£å¸¸
2. åˆ‡åˆ†æ˜¯å¦æˆåŠŸ
3. è´¨é‡è¿‡æ»¤çš„å…·ä½“åŸå› 
"""

import sys
from pathlib import Path
import numpy as np
import pandas as pd
from shapely import wkt
from sqlalchemy import create_engine, text

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

from spdatalab.dataset.grid_trajectory_clustering import (
    GridTrajectoryClusterer, 
    ClusterConfig
)

def diagnose_grid_clustering(city_id='A72', grid_rank=1):
    """è¯Šæ–­gridèšç±»é—®é¢˜"""
    
    print("="*70)
    print("ğŸ”¬ Gridèšç±»é—®é¢˜è¯Šæ–­")
    print("="*70)
    
    # åˆ›å»ºèšç±»å™¨
    config = ClusterConfig(
        query_limit=50000,
        min_distance=50.0,
        max_duration=15.0,
        min_points=5
    )
    clusterer = GridTrajectoryClusterer(config)
    
    # 1. åŠ è½½grid
    print(f"\n1ï¸âƒ£ åŠ è½½Grid...")
    grids_df = clusterer.load_hotspot_grids(city_id=city_id, limit=1)
    
    if grids_df.empty:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°grid")
        return
    
    grid = grids_df.iloc[0]
    grid_id = grid['grid_id']
    geometry = grid['geometry']
    
    print(f"âœ… Grid #{grid_id}")
    
    # 2. æŸ¥è¯¢è½¨è¿¹ç‚¹
    print(f"\n2ï¸âƒ£ æŸ¥è¯¢è½¨è¿¹ç‚¹...")
    points_df = clusterer.query_trajectory_points(geometry, grid_id)
    
    if points_df.empty:
        print("âŒ æ²¡æœ‰è½¨è¿¹ç‚¹")
        return
    
    print(f"âœ… æŸ¥è¯¢åˆ° {len(points_df)} ä¸ªç‚¹ï¼Œ{points_df['dataset_name'].nunique()} æ¡è½¨è¿¹")
    
    # æ£€æŸ¥æ•°æ®
    print(f"\nğŸ“Š æ•°æ®æ£€æŸ¥:")
    print(f"  timestampèŒƒå›´: {points_df['timestamp'].min()} - {points_df['timestamp'].max()}")
    
    # æ£€æµ‹timestampå•ä½
    ts_min = points_df['timestamp'].min()
    ts_max = points_df['timestamp'].max()
    if ts_min < 1e10 and ts_max < 1e10:
        print(f"  timestampå•ä½: ç§’çº§")
    elif ts_min < 1e13 and ts_max < 1e13:
        print(f"  timestampå•ä½: æ¯«ç§’çº§")
    elif ts_min < 1e16 and ts_max < 1e16:
        print(f"  timestampå•ä½: å¾®ç§’çº§")
    else:
        print(f"  timestampå•ä½: æ··åˆ/çº³ç§’çº§ âš ï¸")
    
    print(f"  lonèŒƒå›´: [{points_df['lon'].min():.6f}, {points_df['lon'].max():.6f}]")
    print(f"  latèŒƒå›´: [{points_df['lat'].min():.6f}, {points_df['lat'].max():.6f}]")
    print(f"  twist_linearèŒƒå›´: [{points_df['twist_linear'].min():.2f}, {points_df['twist_linear'].max():.2f}] m/s")
    
    # æŠ½æ ·æ£€æŸ¥å‡ æ¡è½¨è¿¹
    print(f"\n3ï¸âƒ£ æŠ½æ ·æ£€æŸ¥å‰3æ¡è½¨è¿¹:")
    for i, (dataset_name, group) in enumerate(points_df.groupby('dataset_name')):
        if i >= 3:
            break
        
        group = group.sort_values('timestamp')
        
        # è‡ªåŠ¨æ£€æµ‹timestampå•ä½
        sample_ts = group['timestamp'].median()
        if sample_ts < 1e10:
            scale = 1  # ç§’
        elif sample_ts < 1e13:
            scale = 1e3  # æ¯«ç§’
        elif sample_ts < 1e16:
            scale = 1e6  # å¾®ç§’
        else:
            scale = 1e9  # çº³ç§’
        
        # ç»Ÿä¸€ä¸ºç§’
        timestamps_sec = group['timestamp'] / scale
        time_span = timestamps_sec.max() - timestamps_sec.min()
        
        # è®¡ç®—æ€»è·ç¦»
        total_dist = 0
        for j in range(1, len(group)):
            lat1, lon1 = group.iloc[j-1]['lat'], group.iloc[j-1]['lon']
            lat2, lon2 = group.iloc[j]['lat'], group.iloc[j]['lon']
            dist = haversine_distance(lat1, lon1, lat2, lon2)
            total_dist += dist
        
        print(f"\n  è½¨è¿¹ {i+1}: {dataset_name}")
        print(f"    ç‚¹æ•°: {len(group)}")
        print(f"    æ—¶é—´è·¨åº¦: {time_span:.2f}ç§’")
        print(f"    æ€»è·ç¦»: {total_dist:.2f}ç±³")
        print(f"    å¹³å‡é€Ÿåº¦: {group['twist_linear'].mean():.2f} m/s")
        
        # åˆ¤æ–­æ˜¯å¦èƒ½åˆ‡åˆ†
        if len(group) < config.min_points:
            print(f"    âš ï¸ ç‚¹æ•°ä¸è¶³ï¼ˆ< {config.min_points}ï¼‰")
        elif time_span < config.max_duration:
            print(f"    âš ï¸ æ—¶é—´è·¨åº¦å¤ªçŸ­ï¼ˆ< {config.max_duration}ç§’ï¼‰")
        elif total_dist < config.min_distance:
            print(f"    âš ï¸ è·ç¦»å¤ªçŸ­ï¼ˆ< {config.min_distance}ç±³ï¼‰")
        else:
            print(f"    âœ… åº”è¯¥èƒ½åˆ‡åˆ†")
    
    # 4. å°è¯•åˆ‡åˆ†
    print(f"\n4ï¸âƒ£ å°è¯•åˆ‡åˆ†è½¨è¿¹...")
    segments = clusterer.segment_trajectories(points_df)
    print(f"âœ… åˆ‡åˆ†å¾—åˆ° {len(segments)} ä¸ªè½¨è¿¹æ®µ")
    
    if not segments:
        print("âŒ åˆ‡åˆ†å¤±è´¥ï¼æ²¡æœ‰ç”Ÿæˆä»»ä½•è½¨è¿¹æ®µ")
        print("\nğŸ” å¯èƒ½åŸå› :")
        print("  1. æ¯æ¡è½¨è¿¹ç‚¹æ•°éƒ½ < min_points (5)")
        print("  2. è½¨è¿¹æ—¶é—´è·¨åº¦éƒ½å¤ªçŸ­")
        print("  3. è½¨è¿¹è·ç¦»éƒ½å¤ªçŸ­")
        return
    
    # 5. è´¨é‡è¿‡æ»¤
    print(f"\n5ï¸âƒ£ è´¨é‡è¿‡æ»¤æ£€æŸ¥:")
    quality_stats = {}
    
    for segment in segments[:10]:  # åªæ£€æŸ¥å‰10ä¸ª
        is_valid, reason = clusterer.filter_segment_quality(segment)
        quality_stats[reason] = quality_stats.get(reason, 0) + 1
    
    print(f"  å‰10ä¸ªè½¨è¿¹æ®µçš„è´¨é‡åˆ†å¸ƒ:")
    for reason, count in sorted(quality_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"    {reason}: {count}")
    
    # 6. å®Œæ•´ç»Ÿè®¡
    print(f"\n6ï¸âƒ£ å®Œæ•´è´¨é‡ç»Ÿè®¡:")
    all_quality_stats = {}
    valid_count = 0
    
    for segment in segments:
        is_valid, reason = clusterer.filter_segment_quality(segment)
        all_quality_stats[reason] = all_quality_stats.get(reason, 0) + 1
        if is_valid:
            valid_count += 1
    
    print(f"  æ€»è½¨è¿¹æ®µæ•°: {len(segments)}")
    print(f"  æœ‰æ•ˆè½¨è¿¹æ®µ: {valid_count} ({valid_count/len(segments)*100:.1f}%)")
    print(f"\n  è´¨é‡åˆ†å¸ƒ:")
    for reason, count in sorted(all_quality_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"    {reason:20s}: {count:5d} ({count/len(segments)*100:.1f}%)")
    
    # 7. å»ºè®®
    print(f"\nğŸ’¡ å»ºè®®:")
    if valid_count == 0:
        if 'stationary' in all_quality_stats and all_quality_stats['stationary'] > len(segments) * 0.5:
            print("  âš ï¸ å¤§éƒ¨åˆ†è½¨è¿¹æ®µè¢«åˆ¤å®šä¸ºåŸåœ°ä¸åŠ¨")
            print("     å»ºè®®ï¼šé™ä½ --min-movement å‚æ•°ï¼ˆå½“å‰10ç±³ï¼‰")
        
        if 'gps_jump' in all_quality_stats and all_quality_stats['gps_jump'] > len(segments) * 0.3:
            print("  âš ï¸ å¤§é‡GPSè·³ç‚¹")
            print("     å»ºè®®ï¼šå¢åŠ  --max-jump å‚æ•°ï¼ˆå½“å‰100ç±³ï¼‰")
        
        if 'insufficient_points' in all_quality_stats:
            print("  âš ï¸ è½¨è¿¹æ®µç‚¹æ•°ä¸è¶³")
            print("     å»ºè®®ï¼šé™ä½ --min-points å‚æ•°ï¼ˆå½“å‰5ï¼‰")
    else:
        print("  âœ… æœ‰æœ‰æ•ˆè½¨è¿¹æ®µï¼Œå¯ä»¥ç»§ç»­èšç±»")

def haversine_distance(lat1, lon1, lat2, lon2):
    """è®¡ç®—ä¸¤ç‚¹é—´è·ç¦»ï¼ˆç±³ï¼‰"""
    R = 6371000
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    return R * c

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='è¯Šæ–­Gridèšç±»é—®é¢˜')
    parser.add_argument('--city', default='A72', help='åŸå¸‚ID')
    parser.add_argument('--grid-rank', type=int, default=1, help='Gridæ’å')
    args = parser.parse_args()
    
    diagnose_grid_clustering(args.city, args.grid_rank)

