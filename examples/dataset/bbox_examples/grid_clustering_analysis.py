#!/usr/bin/env python3
"""
Gridè½¨è¿¹èšç±»åˆ†æžç¤ºä¾‹è„šæœ¬
===============================

åŸºäºŽcity_hotspotsè¡¨çš„çƒ­ç‚¹gridï¼Œå¯¹æ¯ä¸ª200mÃ—200måŒºåŸŸå†…çš„é«˜è´¨é‡è½¨è¿¹è¿›è¡Œèšç±»åˆ†æžã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»Žcity_hotspotsè¡¨åŠ è½½çƒ­ç‚¹grid
2. æŸ¥è¯¢gridå†…çš„é«˜è´¨é‡è½¨è¿¹ç‚¹ï¼ˆworkstage=2ï¼‰
3. æŒ‰è·ç¦»ä¼˜å…ˆ+æ—¶é•¿ä¸Šé™ç­–ç•¥åˆ‡åˆ†è½¨è¿¹æ®µï¼ˆ50ç±³/15ç§’ï¼‰
4. æå–10ç»´ç‰¹å¾å‘é‡ï¼ˆé€Ÿåº¦ã€åŠ é€Ÿåº¦ã€èˆªå‘ã€å½¢æ€ï¼‰
5. DBSCANèšç±»åˆ†æž
6. ä¿å­˜ç»“æžœåˆ°æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•ï¼š
    # 1. å¯¹æŸä¸ªåŸŽå¸‚çš„hotspot gridsè¿›è¡Œèšç±»
    python examples/dataset/bbox_examples/grid_clustering_analysis.py \\
        --city A72 \\
        --min-distance 50 \\
        --max-duration 15
    
    # 2. åªå¤„ç†å‰Nä¸ªçƒ­ç‚¹grids
    python examples/dataset/bbox_examples/grid_clustering_analysis.py \\
        --top-n 5 \\
        --eps 0.4 \\
        --min-samples 3
    
    # 3. æŒ‡å®šç‰¹å®šgrid IDs
    python examples/dataset/bbox_examples/grid_clustering_analysis.py \\
        --grid-ids 123 456 789
    
    # 4. å¯¼å‡ºèšç±»ç»“æžœåˆ°GeoJSON
    python examples/dataset/bbox_examples/grid_clustering_analysis.py \\
        --city A72 \\
        --top-n 1 \\
        --export-geojson results.geojson

è¾“å‡ºç»“æžœï¼š
    1. æ•°æ®åº“è¡¨ï¼šgrid_trajectory_segments, grid_clustering_summary
    2. ç»ˆç«¯ç»Ÿè®¡æŠ¥å‘Š
    3. GeoJSONæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
"""

import sys
from pathlib import Path
import argparse
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from spdatalab.dataset.grid_trajectory_clustering import GridTrajectoryClusterer, ClusterConfig
except ImportError:
    sys.path.insert(0, str(project_root / "src"))
    from spdatalab.dataset.grid_trajectory_clustering import GridTrajectoryClusterer, ClusterConfig

import pandas as pd
import geopandas as gpd
from shapely import wkt


def export_to_geojson(clusterer, city_id: str, analysis_id: str, output_file: str):
    """å¯¼å‡ºèšç±»ç»“æžœåˆ°GeoJSONæ–‡ä»¶"""
    print(f"\nðŸ“¤ å¯¼å‡ºèšç±»ç»“æžœåˆ°GeoJSON...")
    
    # æŸ¥è¯¢ç»“æžœ
    sql = f"""
        SELECT 
            id,
            grid_id,
            city_id,
            dataset_name,
            segment_index,
            cluster_label,
            quality_flag,
            avg_speed,
            trajectory_length_m,
            duration,
            ST_AsText(geometry) as geometry_wkt
        FROM grid_trajectory_segments
        WHERE city_id = '{city_id}'
        AND quality_flag = 'valid'
        ORDER BY grid_id, cluster_label;
    """
    
    with clusterer.engine.connect() as conn:
        df = pd.read_sql(sql, conn)
    
    if df.empty:
        print("âš ï¸ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        return
    
    # è½¬æ¢ä¸ºGeoDataFrame
    df['geometry'] = df['geometry_wkt'].apply(wkt.loads)
    gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')
    
    # åˆ é™¤WKTåˆ—
    gdf = gdf.drop(columns=['geometry_wkt'])
    
    # å¯¼å‡º
    gdf.to_file(output_file, driver='GeoJSON')
    
    print(f"âœ… æˆåŠŸå¯¼å‡º {len(gdf)} ä¸ªè½¨è¿¹æ®µåˆ°: {output_file}")
    print(f"   èšç±»æ•°é‡: {gdf['cluster_label'].nunique()}")
    print(f"   Gridæ•°é‡: {gdf['grid_id'].nunique()}")


def print_summary_stats(clusterer, city_id: str = None):
    """æ‰“å°æ±‡æ€»ç»Ÿè®¡"""
    print(f"\n" + "="*70)
    print(f"ðŸ“Š èšç±»ç»“æžœæ±‡æ€»ç»Ÿè®¡")
    print(f"="*70)
    
    # æŸ¥è¯¢è½¨è¿¹æ®µç»Ÿè®¡
    where_clause = f"WHERE city_id = '{city_id}'" if city_id else ""
    
    segments_sql = f"""
        SELECT 
            COUNT(*) as total_segments,
            COUNT(*) FILTER (WHERE quality_flag = 'valid') as valid_segments,
            COUNT(DISTINCT grid_id) as grid_count,
            COUNT(DISTINCT cluster_label) as cluster_count,
            COUNT(*) FILTER (WHERE cluster_label = -1) as noise_count
        FROM grid_trajectory_segments
        {where_clause};
    """
    
    with clusterer.engine.connect() as conn:
        stats = pd.read_sql(segments_sql, conn).iloc[0]
    
    print(f"\nè½¨è¿¹æ®µç»Ÿè®¡:")
    print(f"  æ€»è½¨è¿¹æ®µæ•°: {stats['total_segments']}")
    print(f"  æœ‰æ•ˆè½¨è¿¹æ®µ: {stats['valid_segments']} ({stats['valid_segments']/max(stats['total_segments'],1)*100:.1f}%)")
    print(f"  Gridæ•°é‡: {stats['grid_count']}")
    print(f"  èšç±»æ•°é‡: {stats['cluster_count']}")
    print(f"  å™ªå£°ç‚¹: {stats['noise_count']} ({stats['noise_count']/max(stats['valid_segments'],1)*100:.1f}%)")
    
    # æŸ¥è¯¢èšç±»è¡Œä¸ºç»Ÿè®¡
    behavior_sql = f"""
        SELECT 
            behavior_label,
            COUNT(*) as count,
            AVG(segment_count) as avg_segments_per_cluster,
            AVG(centroid_avg_speed) as avg_speed
        FROM grid_clustering_summary
        {where_clause.replace('city_id', 's.city_id') if where_clause else ''}
        GROUP BY behavior_label
        ORDER BY count DESC;
    """
    
    with clusterer.engine.connect() as conn:
        behavior_stats = pd.read_sql(behavior_sql, conn)
    
    if not behavior_stats.empty:
        print(f"\nè¡Œä¸ºç±»åž‹åˆ†å¸ƒ:")
        for _, row in behavior_stats.iterrows():
            print(f"  {row['behavior_label']:15s}: {row['count']:3.0f}ä¸ªèšç±» "
                  f"(å¹³å‡ {row['avg_segments_per_cluster']:.1f}æ®µ/èšç±», "
                  f"å¹³å‡é€Ÿåº¦ {row['avg_speed']:.1f}m/s)")
    
    # æŸ¥è¯¢è´¨é‡è¿‡æ»¤ç»Ÿè®¡
    quality_sql = f"""
        SELECT 
            quality_flag,
            COUNT(*) as count
        FROM grid_trajectory_segments
        {where_clause}
        GROUP BY quality_flag
        ORDER BY count DESC;
    """
    
    with clusterer.engine.connect() as conn:
        quality_stats = pd.read_sql(quality_sql, conn)
    
    if not quality_stats.empty:
        print(f"\nè´¨é‡è¿‡æ»¤ç»Ÿè®¡:")
        for _, row in quality_stats.iterrows():
            print(f"  {row['quality_flag']:20s}: {row['count']:5.0f} ({row['count']/quality_stats['count'].sum()*100:.1f}%)")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Gridè½¨è¿¹èšç±»åˆ†æž',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # å¯¹æŸä¸ªåŸŽå¸‚çš„å‰5ä¸ªçƒ­ç‚¹gridè¿›è¡Œèšç±»
  python grid_clustering_analysis.py --city A72 --top-n 5
  
  # æŒ‡å®šç‰¹å®šçš„grid IDs
  python grid_clustering_analysis.py --grid-ids 123 456 789
  
  # è‡ªå®šä¹‰åˆ‡åˆ†å’Œèšç±»å‚æ•°
  python grid_clustering_analysis.py --city A72 --min-distance 30 --max-duration 20 --eps 0.3
  
  # å¯¼å‡ºç»“æžœåˆ°GeoJSON
  python grid_clustering_analysis.py --city A72 --top-n 1 --export-geojson results.geojson
        """
    )
    
    # Gridé€‰æ‹©å‚æ•°
    grid_group = parser.add_argument_group('Gridé€‰æ‹©')
    grid_group.add_argument('--city', help='åŸŽå¸‚IDè¿‡æ»¤ï¼ˆå¦‚ï¼šA72ï¼‰')
    grid_group.add_argument('--top-n', type=int, help='åªå¤„ç†å‰Nä¸ªçƒ­ç‚¹grid')
    grid_group.add_argument('--grid-ids', type=int, nargs='+', help='æŒ‡å®šç‰¹å®šgridçš„IDåˆ—è¡¨')
    
    # åˆ‡åˆ†å‚æ•°
    segment_group = parser.add_argument_group('è½¨è¿¹åˆ‡åˆ†å‚æ•°')
    segment_group.add_argument('--min-distance', type=float, default=50.0, 
                              help='ä¸»åˆ‡åˆ†æ¡ä»¶ï¼šç´¯è®¡è·ç¦»ï¼ˆç±³ï¼‰ï¼Œé»˜è®¤50')
    segment_group.add_argument('--max-duration', type=float, default=15.0,
                              help='å¼ºåˆ¶åˆ‡åˆ†ï¼šæ—¶é•¿ä¸Šé™ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤15')
    segment_group.add_argument('--min-points', type=int, default=5,
                              help='æœ€å°‘ç‚¹æ•°è¦æ±‚ï¼Œé»˜è®¤5')
    segment_group.add_argument('--time-gap', type=float, default=3.0,
                              help='æ—¶é—´é—´éš”æ–­å¼€é˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3')
    
    # è´¨é‡è¿‡æ»¤å‚æ•°
    quality_group = parser.add_argument_group('è´¨é‡è¿‡æ»¤å‚æ•°')
    quality_group.add_argument('--min-movement', type=float, default=10.0,
                              help='æœ€å°ç§»åŠ¨è·ç¦»ï¼ˆç±³ï¼‰ï¼Œé»˜è®¤10')
    quality_group.add_argument('--max-jump', type=float, default=100.0,
                              help='æœ€å¤§ç‚¹é—´è·ï¼ˆç±³ï¼‰ï¼Œé»˜è®¤100')
    quality_group.add_argument('--max-speed', type=float, default=30.0,
                              help='æœ€å¤§åˆç†é€Ÿåº¦ï¼ˆm/sï¼‰ï¼Œé»˜è®¤30')
    
    # èšç±»å‚æ•°
    cluster_group = parser.add_argument_group('èšç±»å‚æ•°')
    cluster_group.add_argument('--eps', type=float, default=0.4,
                              help='DBSCANè·ç¦»é˜ˆå€¼ï¼Œé»˜è®¤0.4')
    cluster_group.add_argument('--min-samples', type=int, default=3,
                              help='DBSCANæœ€å°æ ·æœ¬æ•°ï¼Œé»˜è®¤3')
    
    # è¾“å‡ºå‚æ•°
    output_group = parser.add_argument_group('è¾“å‡ºå‚æ•°')
    output_group.add_argument('--export-geojson', metavar='FILE',
                             help='å¯¼å‡ºç»“æžœåˆ°GeoJSONæ–‡ä»¶')
    output_group.add_argument('--show-summary', action='store_true', default=True,
                             help='æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡ï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    
    args = parser.parse_args()
    
    # æ‰“å°é…ç½®
    print("ðŸš€ Gridè½¨è¿¹èšç±»åˆ†æž")
    print("=" * 70)
    print(f"\nðŸ“‹ é…ç½®å‚æ•°:")
    print(f"   Gridé€‰æ‹©: ", end="")
    if args.city:
        print(f"åŸŽå¸‚={args.city}", end="")
    if args.top_n:
        print(f", å‰{args.top_n}ä¸ª", end="")
    if args.grid_ids:
        print(f", IDs={args.grid_ids}", end="")
    if not (args.city or args.top_n or args.grid_ids):
        print("æ‰€æœ‰grid", end="")
    print()
    
    print(f"   åˆ‡åˆ†ç­–ç•¥: {args.min_distance}ç±³ / {args.max_duration}ç§’")
    print(f"   è´¨é‡è¿‡æ»¤: ç§»åŠ¨>{args.min_movement}m, è·³ç‚¹<{args.max_jump}m, é€Ÿåº¦<{args.max_speed}m/s")
    print(f"   èšç±»å‚æ•°: eps={args.eps}, min_samples={args.min_samples}")
    
    # åˆ›å»ºé…ç½®
    config = ClusterConfig(
        min_distance=args.min_distance,
        max_duration=args.max_duration,
        min_points=args.min_points,
        time_gap_threshold=args.time_gap,
        min_movement=args.min_movement,
        max_jump=args.max_jump,
        max_speed=args.max_speed,
        eps=args.eps,
        min_samples=args.min_samples
    )
    
    # åˆ›å»ºèšç±»å™¨
    clusterer = GridTrajectoryClusterer(config)
    
    # æ‰§è¡Œèšç±»
    try:
        stats_df = clusterer.process_all_grids(
            city_id=args.city,
            max_grids=args.top_n,
            grid_ids=args.grid_ids
        )
        
        if stats_df.empty:
            print("\nâŒ æ²¡æœ‰å¤„ç†ä»»ä½•grid")
            return 1
        
        # ä¿å­˜ç»Ÿè®¡åˆ°CSV
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        stats_file = f"grid_clustering_stats_{timestamp}.csv"
        stats_df.to_csv(stats_file, index=False)
        print(f"\nðŸ’¾ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        
        # æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡
        if args.show_summary:
            print_summary_stats(clusterer, args.city)
        
        # å¯¼å‡ºGeoJSON
        if args.export_geojson:
            if args.city:
                # ç”Ÿæˆanalysis_idï¼ˆä½¿ç”¨æœ€æ–°çš„ï¼‰
                analysis_id = f"clustering_{timestamp}"
                export_to_geojson(clusterer, args.city, analysis_id, args.export_geojson)
            else:
                print("\nâš ï¸ å¯¼å‡ºGeoJSONéœ€è¦æŒ‡å®š--cityå‚æ•°")
        
        print(f"\nâœ… åˆ†æžå®Œæˆï¼")
        print(f"\nðŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print(f"   1. åœ¨QGISä¸­åŠ è½½ grid_trajectory_segments è¡¨")
        print(f"   2. æŒ‰ cluster_label å­—æ®µåˆ†ç±»ç€è‰²")
        print(f"   3. æŸ¥çœ‹ grid_clustering_summary è¡¨äº†è§£èšç±»ç»Ÿè®¡")
        if args.export_geojson:
            print(f"   4. åœ¨QGIS/Kepler.glä¸­åŠ è½½ {args.export_geojson}")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ åˆ†æžå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())







