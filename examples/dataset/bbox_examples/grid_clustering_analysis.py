#!/usr/bin/env python3
"""
Gridè½¨è¿¹èšç±»åˆ†æžç¤ºä¾‹è„šæœ¬
===============================

åŸºäºŽcity_hotspotsè¡¨çš„çƒ­ç‚¹gridï¼Œå¯¹æ¯ä¸ª200mÃ—200måŒºåŸŸå†…çš„é«˜è´¨é‡è½¨è¿¹è¿›è¡Œèšç±»åˆ†æžã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»Žcity_hotspotsè¡¨åŠ è½½çƒ­ç‚¹grid
2. æŸ¥è¯¢gridå†…çš„é«˜è´¨é‡è½¨è¿¹ç‚¹ï¼ˆworkstage=2ï¼‰
3. æŒ‰è·ç¦»ä¼˜å…ˆ+æ—¶é•¿ä¸Šé™ç­–ç•¥åˆ‡åˆ†è½¨è¿¹æ®µï¼ˆ50ç±³/15ç§’ï¼‰
4. æå–10ç»´ç‰¹å¾å‘é‡ï¼ˆé€Ÿåº¦ã€åŠ é€Ÿåº¦ã€èˆªå‘ã€å½¢æ€ï¼‰
5. DBSCANèšç±»åˆ†æž
6. ä¿å­˜ç»“æžœåˆ°æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•ï¼š
    # 1. å¯¹æŸä¸ªåŸŽå¸‚çš„hotspot gridsè¿›è¡Œèšç±»
    python examples/dataset/bbox_examples/grid_clustering_analysis.py \\
        --city A72 \\
        --min-distance 50 \\
        --max-duration 15
    
    # 2. åªå¤„ç†å‰Nä¸ªçƒ­ç‚¹grids
    python examples/dataset/bbox_examples/grid_clustering_analysis.py \\
        --top-n 5 \\
        --eps 0.4 \\
        --min-samples 3
    
    # 3. æŒ‡å®šç‰¹å®šgrid IDs
    python examples/dataset/bbox_examples/grid_clustering_analysis.py \\
        --grid-ids 123 456 789
    
    # 4. å¯¼å‡ºèšç±»ç»“æžœåˆ°GeoJSON
    python examples/dataset/bbox_examples/grid_clustering_analysis.py \\
        --city A72 \\
        --top-n 1 \\
        --export-geojson results.geojson

è¾“å‡ºç»“æžœï¼š
    1. æ•°æ®åº“è¡¨ï¼šgrid_trajectory_segments, grid_clustering_summary
    2. ç»ˆç«¯ç»Ÿè®¡æŠ¥å‘Š
    3. GeoJSONæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
"""

import sys
from pathlib import Path
import argparse
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from spdatalab.dataset.grid_trajectory_clustering import GridTrajectoryClusterer, ClusterConfig
except ImportError:
    sys.path.insert(0, str(project_root / "src"))
    from spdatalab.dataset.grid_trajectory_clustering import GridTrajectoryClusterer, ClusterConfig

import pandas as pd
import geopandas as gpd
from shapely import wkt


def export_to_geojson(clusterer, city_id: str, analysis_id: str, output_file: str):
    """å¯¼å‡ºèšç±»ç»“æžœåˆ°GeoJSONæ–‡ä»¶"""
    print(f"\nðŸ“¤ å¯¼å‡ºèšç±»ç»“æžœåˆ°GeoJSON...")
    
    # æŸ¥è¯¢ç»“æžœ
    sql = f"""
        SELECT 
            id,
            grid_id,
            city_id,
            dataset_name,
            segment_index,
            cluster_label,
            quality_flag,
            avg_speed,
            trajectory_length_m,
            duration,
            ST_AsText(geometry) as geometry_wkt
        FROM grid_trajectory_segments
        WHERE city_id = '{city_id}'
        AND quality_flag = 'valid'
        ORDER BY grid_id, cluster_label;
    """
    
    with clusterer.engine.connect() as conn:
        df = pd.read_sql(sql, conn)
    
    if df.empty:
        print("âš ï¸ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
        return
    
    # è½¬æ¢ä¸ºGeoDataFrame
    df['geometry'] = df['geometry_wkt'].apply(wkt.loads)
    gdf = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')
    
    # åˆ é™¤WKTåˆ—
    gdf = gdf.drop(columns=['geometry_wkt'])
    
    # å¯¼å‡º
    gdf.to_file(output_file, driver='GeoJSON')
    
    print(f"âœ… æˆåŠŸå¯¼å‡º {len(gdf)} ä¸ªè½¨è¿¹æ®µåˆ°: {output_file}")
    print(f"   èšç±»æ•°é‡: {gdf['cluster_label'].nunique()}")
    print(f"   Gridæ•°é‡: {gdf['grid_id'].nunique()}")


def print_summary_stats_from_memory(stats_df: pd.DataFrame):
    """ä»Žå†…å­˜ç»Ÿè®¡ç»“æžœæ‰“å°æ±‡æ€»ï¼ˆä¸ä¾èµ–æ•°æ®åº“è¡¨ï¼‰"""
    print(f"\n" + "="*70)
    print(f"ðŸ“Š èšç±»ç»“æžœæ±‡æ€»ç»Ÿè®¡ (åŸºäºŽè¿è¡Œç»“æžœ)")
    print(f"="*70)
    
    if stats_df.empty:
        print("\nâš ï¸ æ²¡æœ‰ç»Ÿè®¡æ•°æ®")
        return
    
    # ç­›é€‰æˆåŠŸçš„grid
    successful = stats_df[stats_df['success'] == True]
    
    if successful.empty:
        print("\nâš ï¸ æ²¡æœ‰æˆåŠŸå¤„ç†çš„grid")
        return
    
    # è½¨è¿¹æ®µç»Ÿè®¡ï¼ˆä»Žstats_dfæ±‡æ€»ï¼‰
    total_points = successful['total_points'].sum()
    total_trajectories = successful['trajectory_count'].sum()
    total_segments = successful['total_segments'].sum()
    valid_segments = successful['valid_segments'].sum()
    grid_count = len(successful)
    
    print(f"\nðŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"  å¤„ç†Gridæ•°: {grid_count}")
    print(f"  æ€»è½¨è¿¹ç‚¹æ•°: {total_points:,}")
    print(f"  æ€»è½¨è¿¹æ•°: {total_trajectories:,}")
    print(f"  æ€»è½¨è¿¹æ®µæ•°: {total_segments:,}")
    print(f"  æœ‰æ•ˆè½¨è¿¹æ®µ: {valid_segments:,} ({valid_segments/max(total_segments,1)*100:.1f}%)")
    
    # èšç±»ç»Ÿè®¡ï¼ˆä»Žcluster_infoæ±‡æ€»ï¼‰
    all_clusters = []
    for _, row in successful.iterrows():
        if 'cluster_info' in row and isinstance(row['cluster_info'], dict):
            for label, info in row['cluster_info'].items():
                all_clusters.append({
                    'grid_id': row['grid_id'],
                    'cluster_label': label,
                    'behavior_label': info.get('behavior_label', 'unknown'),
                    'segment_count': info.get('segment_count', 0),
                    'speed_range': info.get('speed_range', ''),
                    'avg_speed': info.get('centroid_avg_speed', 0)
                })
    
    if all_clusters:
        clusters_df = pd.DataFrame(all_clusters)
        
        # æŒ‰è¡Œä¸ºç±»åž‹ç»Ÿè®¡
        behavior_stats = clusters_df.groupby('behavior_label').agg({
            'cluster_label': 'count',
            'segment_count': 'sum',
            'avg_speed': 'mean'
        }).reset_index()
        behavior_stats.columns = ['behavior_label', 'cluster_count', 'total_segments', 'avg_speed']
        behavior_stats = behavior_stats.sort_values('total_segments', ascending=False)
        
        print(f"\nðŸŽ¯ è¡Œä¸ºç±»åž‹åˆ†å¸ƒ:")
        for _, row in behavior_stats.iterrows():
            print(f"  {row['behavior_label']:15s}: {row['cluster_count']:3.0f}ä¸ªèšç±», "
                  f"{row['total_segments']:5.0f}æ®µ "
                  f"(å¹³å‡é€Ÿåº¦ {row['avg_speed']:.1f}m/s)")
        
        # å™ªå£°ç»Ÿè®¡
        noise_clusters = clusters_df[clusters_df['cluster_label'] == -1]
        if not noise_clusters.empty:
            noise_segments = noise_clusters['segment_count'].sum()
            print(f"\nâš ï¸ å™ªå£°ç‚¹: {len(noise_clusters)}ä¸ªå™ªå£°ç°‡, {noise_segments}æ®µ "
                  f"({noise_segments/valid_segments*100:.1f}%)")
    
    # è´¨é‡è¿‡æ»¤ç»Ÿè®¡ï¼ˆä»Žquality_statsæ±‡æ€»ï¼‰
    all_quality = {}
    for _, row in successful.iterrows():
        if 'quality_stats' in row and isinstance(row['quality_stats'], dict):
            for flag, count in row['quality_stats'].items():
                all_quality[flag] = all_quality.get(flag, 0) + count
    
    if all_quality:
        print(f"\nðŸ” è´¨é‡è¿‡æ»¤ç»Ÿè®¡:")
        quality_total = sum(all_quality.values())
        for flag, count in sorted(all_quality.items(), key=lambda x: x[1], reverse=True):
            print(f"  {flag:20s}: {count:5.0f} ({count/quality_total*100:.1f}%)")


def print_summary_stats_from_db(clusterer, city_id: str = None):
    """ä»Žæ•°æ®åº“æŸ¥è¯¢ç»Ÿè®¡ï¼ˆéœ€è¦è¡¨å·²å­˜åœ¨ï¼‰"""
    print(f"\n" + "="*70)
    print(f"ðŸ“Š èšç±»ç»“æžœæ±‡æ€»ç»Ÿè®¡ (ä»Žæ•°æ®åº“)")
    print(f"="*70)
    
    try:
        # æŸ¥è¯¢è½¨è¿¹æ®µç»Ÿè®¡
        where_clause = f"WHERE city_id = '{city_id}'" if city_id else ""
        
        segments_sql = f"""
            SELECT 
                COUNT(*) as total_segments,
                COUNT(*) FILTER (WHERE quality_flag = 'valid') as valid_segments,
                COUNT(DISTINCT grid_id) as grid_count,
                COUNT(DISTINCT cluster_label) as cluster_count,
                COUNT(*) FILTER (WHERE cluster_label = -1) as noise_count
            FROM grid_trajectory_segments
            {where_clause};
        """
        
        with clusterer.engine.connect() as conn:
            stats = pd.read_sql(segments_sql, conn).iloc[0]
        
        print(f"\nè½¨è¿¹æ®µç»Ÿè®¡:")
        print(f"  æ€»è½¨è¿¹æ®µæ•°: {stats['total_segments']}")
        print(f"  æœ‰æ•ˆè½¨è¿¹æ®µ: {stats['valid_segments']} ({stats['valid_segments']/max(stats['total_segments'],1)*100:.1f}%)")
        print(f"  Gridæ•°é‡: {stats['grid_count']}")
        print(f"  èšç±»æ•°é‡: {stats['cluster_count']}")
        print(f"  å™ªå£°ç‚¹: {stats['noise_count']} ({stats['noise_count']/max(stats['valid_segments'],1)*100:.1f}%)")
        
        # æŸ¥è¯¢èšç±»è¡Œä¸ºç»Ÿè®¡
        behavior_sql = f"""
            SELECT 
                behavior_label,
                COUNT(*) as count,
                AVG(segment_count) as avg_segments_per_cluster,
                AVG(centroid_avg_speed) as avg_speed
            FROM grid_clustering_summary
            {where_clause.replace('city_id', 's.city_id') if where_clause else ''}
            GROUP BY behavior_label
            ORDER BY count DESC;
        """
        
        with clusterer.engine.connect() as conn:
            behavior_stats = pd.read_sql(behavior_sql, conn)
        
        if not behavior_stats.empty:
            print(f"\nè¡Œä¸ºç±»åž‹åˆ†å¸ƒ:")
            for _, row in behavior_stats.iterrows():
                print(f"  {row['behavior_label']:15s}: {row['count']:3.0f}ä¸ªèšç±» "
                      f"(å¹³å‡ {row['avg_segments_per_cluster']:.1f}æ®µ/èšç±», "
                      f"å¹³å‡é€Ÿåº¦ {row['avg_speed']:.1f}m/s)")
        
        # æŸ¥è¯¢è´¨é‡è¿‡æ»¤ç»Ÿè®¡
        quality_sql = f"""
            SELECT 
                quality_flag,
                COUNT(*) as count
            FROM grid_trajectory_segments
            {where_clause}
            GROUP BY quality_flag
            ORDER BY count DESC;
        """
        
        with clusterer.engine.connect() as conn:
            quality_stats = pd.read_sql(quality_sql, conn)
        
        if not quality_stats.empty:
            print(f"\nè´¨é‡è¿‡æ»¤ç»Ÿè®¡:")
            for _, row in quality_stats.iterrows():
                print(f"  {row['quality_flag']:20s}: {row['count']:5.0f} ({row['count']/quality_stats['count'].sum()*100:.1f}%)")
    
    except Exception as e:
        print(f"\nâš ï¸ æ— æ³•ä»Žæ•°æ®åº“æŸ¥è¯¢ç»Ÿè®¡: {e}")
        print(f"   æç¤ºï¼šå¯èƒ½éœ€è¦å…ˆåˆ›å»ºæ•°æ®åº“è¡¨ï¼Œæˆ–ä½¿ç”¨å†…å­˜ç»Ÿè®¡")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='Gridè½¨è¿¹èšç±»åˆ†æž',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # å¯¹æŸä¸ªåŸŽå¸‚çš„å‰5ä¸ªçƒ­ç‚¹gridè¿›è¡Œèšç±»
  python grid_clustering_analysis.py --city A72 --top-n 5
  
  # æŒ‡å®šç‰¹å®šçš„grid IDs
  python grid_clustering_analysis.py --grid-ids 123 456 789
  
  # è‡ªå®šä¹‰åˆ‡åˆ†å’Œèšç±»å‚æ•°
  python grid_clustering_analysis.py --city A72 --min-distance 30 --max-duration 20 --eps 0.3
  
  # å¯¼å‡ºç»“æžœåˆ°GeoJSON
  python grid_clustering_analysis.py --city A72 --top-n 1 --export-geojson results.geojson
        """
    )
    
    # Gridé€‰æ‹©å‚æ•°
    grid_group = parser.add_argument_group('Gridé€‰æ‹©')
    grid_group.add_argument('--city', help='åŸŽå¸‚IDè¿‡æ»¤ï¼ˆå¦‚ï¼šA72ï¼‰')
    grid_group.add_argument('--top-n', type=int, help='åªå¤„ç†å‰Nä¸ªçƒ­ç‚¹grid')
    grid_group.add_argument('--grid-ids', type=int, nargs='+', help='æŒ‡å®šç‰¹å®šgridçš„IDåˆ—è¡¨')
    
    # æŸ¥è¯¢å‚æ•°
    query_group = parser.add_argument_group('è½¨è¿¹æŸ¥è¯¢å‚æ•°')
    query_group.add_argument('--query-limit', type=int, default=50000,
                            help='æ¯ä¸ªgridçš„è½¨è¿¹ç‚¹æŸ¥è¯¢é™åˆ¶ï¼Œé»˜è®¤50000')
    
    # åˆ‡åˆ†å‚æ•°
    segment_group = parser.add_argument_group('è½¨è¿¹åˆ‡åˆ†å‚æ•°')
    segment_group.add_argument('--min-distance', type=float, default=50.0, 
                              help='ä¸»åˆ‡åˆ†æ¡ä»¶ï¼šç´¯è®¡è·ç¦»ï¼ˆç±³ï¼‰ï¼Œé»˜è®¤50')
    segment_group.add_argument('--max-duration', type=float, default=15.0,
                              help='å¼ºåˆ¶åˆ‡åˆ†ï¼šæ—¶é•¿ä¸Šé™ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤15')
    segment_group.add_argument('--min-points', type=int, default=5,
                              help='æœ€å°‘ç‚¹æ•°è¦æ±‚ï¼Œé»˜è®¤5')
    segment_group.add_argument('--time-gap', type=float, default=3.0,
                              help='æ—¶é—´é—´éš”æ–­å¼€é˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3')
    
    # è´¨é‡è¿‡æ»¤å‚æ•°
    quality_group = parser.add_argument_group('è´¨é‡è¿‡æ»¤å‚æ•°')
    quality_group.add_argument('--min-movement', type=float, default=10.0,
                              help='æœ€å°ç§»åŠ¨è·ç¦»ï¼ˆç±³ï¼‰ï¼Œé»˜è®¤10')
    quality_group.add_argument('--max-jump', type=float, default=100.0,
                              help='æœ€å¤§ç‚¹é—´è·ï¼ˆç±³ï¼‰ï¼Œé»˜è®¤100')
    quality_group.add_argument('--max-speed', type=float, default=30.0,
                              help='æœ€å¤§åˆç†é€Ÿåº¦ï¼ˆm/sï¼‰ï¼Œé»˜è®¤30')
    
    # èšç±»å‚æ•°
    cluster_group = parser.add_argument_group('èšç±»å‚æ•°')
    cluster_group.add_argument('--eps', type=float, default=0.8,
                              help='DBSCANè·ç¦»é˜ˆå€¼ï¼Œé»˜è®¤0.8ï¼ˆå‡å°‘å™ªå£°ï¼‰')
    cluster_group.add_argument('--min-samples', type=int, default=5,
                              help='DBSCANæœ€å°æ ·æœ¬æ•°ï¼Œé»˜è®¤5ï¼ˆæ›´ç¨³å®šçš„ç°‡ï¼‰')
    
    # è¾“å‡ºå‚æ•°
    output_group = parser.add_argument_group('è¾“å‡ºå‚æ•°')
    output_group.add_argument('--save-to-database', action='store_true',
                             help='ä¿å­˜ç»“æžœåˆ°æ•°æ®åº“è¡¨ï¼ˆéœ€è¦å…ˆåˆ›å»ºè¡¨ï¼‰')
    output_group.add_argument('--export-geojson', metavar='FILE',
                             help='å¯¼å‡ºç»“æžœåˆ°GeoJSONæ–‡ä»¶')
    output_group.add_argument('--show-summary', action='store_true', default=True,
                             help='æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡ï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    
    args = parser.parse_args()
    
    # æ‰“å°é…ç½®
    print("ðŸš€ Gridè½¨è¿¹èšç±»åˆ†æž")
    print("=" * 70)
    print(f"\nðŸ“‹ é…ç½®å‚æ•°:")
    print(f"   Gridé€‰æ‹©: ", end="")
    if args.city:
        print(f"åŸŽå¸‚={args.city}", end="")
    if args.top_n:
        print(f", å‰{args.top_n}ä¸ª", end="")
    if args.grid_ids:
        print(f", IDs={args.grid_ids}", end="")
    if not (args.city or args.top_n or args.grid_ids):
        print("æ‰€æœ‰grid", end="")
    print()
    
    print(f"   æŸ¥è¯¢é™åˆ¶: {args.query_limit}ç‚¹/grid")
    print(f"   åˆ‡åˆ†ç­–ç•¥: {args.min_distance}ç±³ / {args.max_duration}ç§’")
    print(f"   è´¨é‡è¿‡æ»¤: ç§»åŠ¨>{args.min_movement}m, è·³ç‚¹<{args.max_jump}m, é€Ÿåº¦<{args.max_speed}m/s")
    print(f"   èšç±»å‚æ•°: eps={args.eps}, min_samples={args.min_samples}")
    print(f"   æ•°æ®åº“ä¿å­˜: {'âœ… å¼€å¯' if args.save_to_database else 'âŒ å…³é—­ï¼ˆä»…å†…å­˜ç»Ÿè®¡ï¼‰'}")
    
    # åˆ›å»ºé…ç½®
    config = ClusterConfig(
        query_limit=args.query_limit,
        min_distance=args.min_distance,
        max_duration=args.max_duration,
        min_points=args.min_points,
        time_gap_threshold=args.time_gap,
        min_movement=args.min_movement,
        max_jump=args.max_jump,
        max_speed=args.max_speed,
        eps=args.eps,
        min_samples=args.min_samples,
        save_to_database=args.save_to_database
    )
    
    # åˆ›å»ºèšç±»å™¨
    clusterer = GridTrajectoryClusterer(config)
    
    # æ‰§è¡Œèšç±»
    try:
        stats_df = clusterer.process_all_grids(
            city_id=args.city,
            max_grids=args.top_n,
            grid_ids=args.grid_ids
        )
        
        if stats_df.empty:
            print("\nâŒ æ²¡æœ‰å¤„ç†ä»»ä½•grid")
            return 1
        
        # ä¿å­˜ç»Ÿè®¡åˆ°CSV
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        stats_file = f"grid_clustering_stats_{timestamp}.csv"
        stats_df.to_csv(stats_file, index=False)
        print(f"\nðŸ’¾ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        
        # æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡ï¼ˆä»Žå†…å­˜ï¼Œä¸ä¾èµ–æ•°æ®åº“è¡¨ï¼‰
        if args.show_summary:
            print_summary_stats_from_memory(stats_df)
        
        # å¯¼å‡ºGeoJSONï¼ˆéœ€è¦æ•°æ®åº“è¡¨ï¼‰
        if args.export_geojson:
            if args.save_to_database:
                export_to_geojson(clusterer, args.city, analysis_id, args.export_geojson)
            else:
                print("\nâš ï¸ GeoJSONå¯¼å‡ºéœ€è¦å…ˆä¿å­˜åˆ°æ•°æ®åº“")
                print("   è¯·é‡æ–°è¿è¡Œå¹¶æ·»åŠ  --save-to-database å‚æ•°")
                print("   æˆ–ä½¿ç”¨å†…å­˜ç»Ÿè®¡ç»“æžœ")
        
        print(f"\nâœ… åˆ†æžå®Œæˆï¼")
        
        # æ ¹æ®æ˜¯å¦ä¿å­˜åˆ°æ•°æ®åº“ï¼Œç»™å‡ºä¸åŒçš„ä¸‹ä¸€æ­¥å»ºè®®
        if args.save_to_database:
            print(f"\nðŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
            print(f"   1. åœ¨QGISä¸­åŠ è½½ grid_trajectory_segments è¡¨")
            print(f"   2. æŒ‰ cluster_label å­—æ®µåˆ†ç±»ç€è‰²")
            print(f"   3. æŸ¥çœ‹ grid_clustering_summary è¡¨äº†è§£èšç±»ç»Ÿè®¡")
            if args.export_geojson:
                print(f"   4. åœ¨QGIS/Kepler.glä¸­åŠ è½½ {args.export_geojson}")
        else:
            print(f"\nðŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
            print(f"   â€¢ æŸ¥çœ‹ä¸Šæ–¹ç»Ÿè®¡ç»“æžœï¼Œè°ƒæ•´å‚æ•°ï¼ˆ--eps, --min-samplesï¼‰")
            print(f"   â€¢ è‹¥è¦ä¿å­˜åˆ°æ•°æ®åº“æŸ¥çœ‹è¯¦ç»†ç»“æžœ:")
            print(f"     1. åˆ›å»ºæ•°æ®åº“è¡¨: psql -f sql/grid_clustering_tables.sql")
            print(f"     2. é‡æ–°è¿è¡Œå¹¶æ·»åŠ  --save-to-database å‚æ•°")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ åˆ†æžå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())







