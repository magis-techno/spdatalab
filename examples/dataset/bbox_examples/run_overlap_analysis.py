#!/usr/bin/env python3
"""
Dockerå…¼å®¹çš„bboxå ç½®åˆ†æå¯åŠ¨è„šæœ¬

è¿™ä¸ªè„šæœ¬ä¸“é—¨ä¸ºDockerç¯å¢ƒè®¾è®¡ï¼Œè‡ªåŠ¨å¤„ç†è·¯å¾„å’Œä¾èµ–å¯¼å…¥é—®é¢˜ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    # åœ¨Dockerå®¹å™¨ä¸­è¿è¡Œ
    python examples/dataset/bbox_examples/run_overlap_analysis.py

    # å¸¦å‚æ•°è¿è¡Œ
    python examples/dataset/bbox_examples/run_overlap_analysis.py \
        --city beijing --refresh-view --top-n 15
"""

import sys
import os
import signal
import atexit
from pathlib import Path

def setup_environment():
    """è®¾ç½®è¿è¡Œç¯å¢ƒï¼Œå¤„ç†è·¯å¾„å’Œå¯¼å…¥é—®é¢˜"""
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    script_path = Path(__file__).resolve()
    project_root = script_path.parent.parent.parent
    
    print(f"ğŸ”§ è„šæœ¬ä½ç½®: {script_path}")
    print(f"ğŸ”§ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    
    # æ·»åŠ å¯èƒ½çš„è·¯å¾„
    paths_to_add = [
        str(project_root),
        str(project_root / "src"),
        "/workspace",
        "/workspace/src"
    ]
    
    for path in paths_to_add:
        if path not in sys.path:
            sys.path.insert(0, path)
    
    print(f"ğŸ”§ Pythonè·¯å¾„:")
    for i, path in enumerate(sys.path[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"   {i}: {path}")
    
    # å°è¯•å¯¼å…¥æµ‹è¯•
    try:
        # å°è¯•æ–¹å¼1ï¼šç›´æ¥å¯¼å…¥
        from spdatalab.dataset.bbox import LOCAL_DSN
        print("âœ… å¯¼å…¥æ–¹å¼: ç›´æ¥å¯¼å…¥ spdatalab")
        return True
    except ImportError as e1:
        try:
            # å°è¯•æ–¹å¼2ï¼šä»srcå¯¼å…¥
            from src.spdatalab.dataset.bbox import LOCAL_DSN
            print("âœ… å¯¼å…¥æ–¹å¼: ä»srcå¯¼å…¥ spdatalab")
            return True
        except ImportError as e2:
            print(f"âŒ å¯¼å…¥å¤±è´¥:")
            print(f"   æ–¹å¼1é”™è¯¯: {e1}")
            print(f"   æ–¹å¼2é”™è¯¯: {e2}")
            
            # æ˜¾ç¤ºå½“å‰ç›®å½•ç»“æ„ä»¥ä¾›è°ƒè¯•
            print(f"\nğŸ” å½“å‰ç›®å½•ç»“æ„:")
            cwd = Path.cwd()
            print(f"   å½“å‰å·¥ä½œç›®å½•: {cwd}")
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨spdatalabæ¨¡å—
            possible_paths = [
                cwd / "spdatalab",
                cwd / "src" / "spdatalab", 
                project_root / "spdatalab",
                project_root / "src" / "spdatalab"
            ]
            
            for path in possible_paths:
                exists = path.exists()
                print(f"   {path}: {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}")
                
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Dockerå…¼å®¹çš„BBoxå ç½®åˆ†æ")
    print("=" * 50)
    
    # è®¾ç½®ä¼˜é›…é€€å‡ºå¤„ç†
    shutdown_requested = False
    current_connection = None
    analysis_start_time = None
    
    def signal_handler(signum, frame):
        nonlocal shutdown_requested, current_connection, analysis_start_time
        print(f"\n\nğŸ›‘ æ”¶åˆ°é€€å‡ºä¿¡å· ({signal.Signals(signum).name})")
        shutdown_requested = True
        print(f"ğŸ”„ æ­£åœ¨å®‰å…¨é€€å‡º...")
        
        if analysis_start_time:
            elapsed = datetime.now() - analysis_start_time
            print(f"â±ï¸ å·²è¿è¡Œæ—¶é—´: {elapsed}")
        
        if current_connection:
            try:
                current_connection.close()
                print(f"âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
            except Exception as e:
                print(f"âš ï¸ å…³é—­è¿æ¥æ—¶å‡ºé”™: {e}")
        
        print(f"âœ… ä¼˜é›…é€€å‡ºå®Œæˆ")
        sys.exit(0)
    
    def check_shutdown():
        if shutdown_requested:
            print(f"ğŸ›‘ æ£€æµ‹åˆ°é€€å‡ºè¯·æ±‚ï¼Œåœæ­¢æ‰§è¡Œ")
            sys.exit(0)
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    try:
        signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
        signal.signal(signal.SIGTERM, signal_handler)  # ç»ˆæ­¢ä¿¡å·
        if hasattr(signal, 'SIGBREAK'):  # Windows
            signal.signal(signal.SIGBREAK, signal_handler)
    except ValueError:
        print("âš ï¸ æ— æ³•æ³¨å†Œä¿¡å·å¤„ç†å™¨")
    
    # è®¾ç½®ç¯å¢ƒ
    if not setup_environment():
        print("\nâŒ ç¯å¢ƒè®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        sys.exit(1)
    
    print("\nğŸš€ ç¯å¢ƒè®¾ç½®æˆåŠŸï¼Œå¼€å§‹å¯¼å…¥æ¨¡å—...")
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        import argparse
        from datetime import datetime
        
        # å°è¯•å¯¼å…¥åˆ†æå™¨ï¼ˆä½¿ç”¨ç»Ÿä¸€è§†å›¾ï¼‰
        try:
            from spdatalab.dataset.bbox import (
                create_unified_view,
                list_bbox_tables,
                LOCAL_DSN
            )
        except ImportError:
            from src.spdatalab.dataset.bbox import (
                create_unified_view,
                list_bbox_tables,
                LOCAL_DSN
            )
        
        from sqlalchemy import create_engine, text
        import pandas as pd
        
        print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        parser = argparse.ArgumentParser(description='Dockerå…¼å®¹çš„BBoxå ç½®åˆ†æï¼ˆä¼˜åŒ–ç‰ˆï¼‰')
        parser.add_argument('--city', required=False, help='åŸå¸‚è¿‡æ»¤ï¼ˆå¼ºçƒˆå»ºè®®æŒ‡å®šä»¥é¿å…æ€§èƒ½é—®é¢˜ï¼‰')
        parser.add_argument('--subdatasets', nargs='+', help='å­æ•°æ®é›†è¿‡æ»¤')
        parser.add_argument('--min-overlap-area', type=float, default=0.0, help='æœ€å°é‡å é¢ç§¯é˜ˆå€¼ï¼ˆåªåœ¨--calculate-areaæ—¶ç”Ÿæ•ˆï¼‰')
        parser.add_argument('--top-n', type=int, default=15, help='è¿”å›çš„çƒ­ç‚¹æ•°é‡')
        parser.add_argument('--analysis-id', help='è‡ªå®šä¹‰åˆ†æID')
        parser.add_argument('--refresh-view', action='store_true', help='å¼ºåˆ¶åˆ·æ–°ç»Ÿä¸€è§†å›¾')
        parser.add_argument('--test-only', action='store_true', help='åªè¿è¡Œæµ‹è¯•ï¼Œä¸æ‰§è¡Œåˆ†æ')
        parser.add_argument('--suggest-city', action='store_true', help='æ˜¾ç¤ºåŸå¸‚åˆ†æå»ºè®®å¹¶é€€å‡º')
        parser.add_argument('--estimate-time', action='store_true', help='ä¼°ç®—åˆ†ææ—¶é—´å¹¶é€€å‡º')
        # ğŸ”¥ ç½‘æ ¼åŒ–åˆ†æå‚æ•°ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        parser.add_argument('--grid-size', type=float, default=0.002, help='ç½‘æ ¼å¤§å°ï¼ˆåº¦ï¼‰ï¼Œé»˜è®¤0.002åº¦çº¦200ç±³')
        parser.add_argument('--percentile', type=float, default=90, help='å¯†åº¦é˜ˆå€¼åˆ†ä½æ•°ï¼ˆ0-100ï¼‰ï¼Œé»˜è®¤90åˆ†ä½æ•°')
        parser.add_argument('--top-percent', type=float, default=5, help='è¿”å›æœ€å¯†é›†çš„å‰X%ç½‘æ ¼ï¼Œé»˜è®¤5%')
        parser.add_argument('--max-results', type=int, default=50, help='æœ€å¤§è¿”å›ç½‘æ ¼æ•°é‡é™åˆ¶ï¼Œé»˜è®¤50')
        parser.add_argument('--calculate-area', action='store_true', help='è®¡ç®—é‡å é¢ç§¯å¹¶åº”ç”¨min-overlap-areaé˜ˆå€¼ï¼ˆé»˜è®¤åªæ£€æŸ¥ç›¸äº¤ï¼‰')
        # å…¼å®¹æ—§å‚æ•°
        parser.add_argument('--density-threshold', type=int, help='å›ºå®šå¯†åº¦é˜ˆå€¼ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼Œå»ºè®®ä½¿ç”¨--percentileï¼‰')
        # ğŸ§¹ æ¸…ç†å’Œè¯Šæ–­åŠŸèƒ½
        parser.add_argument('--diagnose', action='store_true', help='è¯Šæ–­bboxæ•°æ®çŠ¶æ€å¹¶é€€å‡º')
        parser.add_argument('--cleanup-views', action='store_true', help='æ¸…ç†æ—§çš„bboxè§†å›¾')
        
        args = parser.parse_args()
        
        # å‚æ•°å…¼å®¹æ€§å¤„ç†
        if args.density_threshold:
            print(f"â„¹ï¸ ä½¿ç”¨å…¼å®¹æ¨¡å¼: å›ºå®šå¯†åº¦é˜ˆå€¼ {args.density_threshold}")
        
        print(f"\nğŸ“‹ åˆ†æå‚æ•°:")
        print(f"   åŸå¸‚è¿‡æ»¤: {args.city}")
        print(f"   å¼ºåˆ¶åˆ·æ–°è§†å›¾: {args.refresh_view}")
        print(f"   ğŸ”¥ æ™ºèƒ½ç½‘æ ¼åŒ–åˆ†æ: å·²å¯ç”¨")
        print(f"   ğŸ“ ç½‘æ ¼å¤§å°: {args.grid_size}Â° Ã— {args.grid_size}Â° (çº¦200mÃ—200m)")
        
        if args.density_threshold:
            print(f"   ğŸ“Š å›ºå®šå¯†åº¦é˜ˆå€¼: {args.density_threshold} bbox/ç½‘æ ¼")
        else:
            print(f"   ğŸ“Š åŠ¨æ€å¯†åº¦é˜ˆå€¼: {args.percentile}åˆ†ä½æ•°")
        
        print(f"   ğŸ“Š è¿”å›ç­–ç•¥: å‰{args.top_percent}%æœ€å¯†é›†ç½‘æ ¼")
        print(f"   ğŸ”¢ æœ€å¤§è¿”å›æ•°é‡: {args.max_results}ä¸ªç½‘æ ¼")
        print(f"   ğŸ¯ åˆ†ææ¨¡å¼: {'é¢ç§¯è®¡ç®—æ¨¡å¼' if args.calculate_area else 'å¿«é€Ÿç›¸äº¤æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰'}")
        if args.calculate_area and args.min_overlap_area > 0:
            print(f"   ğŸ“ æœ€å°é‡å é¢ç§¯: {args.min_overlap_area}")
        elif args.calculate_area:
            print(f"   ğŸ“ è®¡ç®—é¢ç§¯ä½†ä¸è¿‡æ»¤")
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        print(f"\nğŸ”Œ è¿æ¥æ•°æ®åº“...")
        engine = create_engine(LOCAL_DSN, future=True)
        
        with engine.connect() as conn:
            result = conn.execute(text("SELECT 1 as test;"))
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        # æ£€æŸ¥bboxè¡¨
        print(f"\nğŸ“Š æ£€æŸ¥bboxåˆ†è¡¨...")
        tables = list_bbox_tables(engine)
        bbox_tables = [t for t in tables if t.startswith('clips_bbox_') and t != 'clips_bbox']
        print(f"âœ… å‘ç° {len(bbox_tables)} ä¸ªbboxåˆ†è¡¨")
        
        if len(bbox_tables) == 0:
            print("âŒ æ²¡æœ‰å‘ç°bboxåˆ†è¡¨ï¼Œæ— æ³•æ‰§è¡Œåˆ†æ")
            return
        
        # æ£€æŸ¥ç»Ÿä¸€è§†å›¾ï¼ˆä½¿ç”¨æ ‡å‡†è§†å›¾ï¼‰
        print(f"\nğŸ” æ£€æŸ¥ç»Ÿä¸€è§†å›¾...")
        view_name = "clips_bbox_unified"
        
        check_view_sql = text(f"""
            SELECT EXISTS (
                SELECT FROM information_schema.views 
                WHERE table_schema = 'public' 
                AND table_name = '{view_name}'
            );
        """)
        
        # åˆ›å»ºæŒä¹…è¿æ¥ç”¨äºæ•´ä¸ªåˆ†æè¿‡ç¨‹
        conn = engine.connect()
        
        try:
            result = conn.execute(check_view_sql)
            view_exists = result.scalar()
            
            if not view_exists or args.refresh_view:
                if args.refresh_view:
                    print(f"ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œé‡æ–°åˆ›å»ºè§†å›¾...")
                else:
                    print(f"ğŸ“Œ è§†å›¾ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è§†å›¾...")
                
                success = create_unified_view(engine, view_name)
                if not success:
                    print("âŒ ç»Ÿä¸€è§†å›¾åˆ›å»ºå¤±è´¥")
                    return
                print(f"âœ… ç»Ÿä¸€è§†å›¾åˆ›å»ºæˆåŠŸ")
            else:
                print(f"âœ… ç»Ÿä¸€è§†å›¾å·²å­˜åœ¨")
            
            # æ£€æŸ¥æ•°æ®é‡
            count_sql = text(f"SELECT COUNT(*) FROM {view_name};")
            count_result = conn.execute(count_sql)
            row_count = count_result.scalar()
            print(f"ğŸ“Š ç»Ÿä¸€è§†å›¾åŒ…å« {row_count:,} æ¡bboxè®°å½•")
            
            if row_count == 0:
                print("âš ï¸ ç»Ÿä¸€è§†å›¾ä¸ºç©ºï¼Œå¯èƒ½åˆ†è¡¨ä¸­æ²¡æœ‰æ•°æ®")
                return
            
            # å¦‚æœåªæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œåˆ°è¿™é‡Œå°±ç»“æŸ
            if args.test_only:
                print(f"\nâœ… æµ‹è¯•æ¨¡å¼å®Œæˆï¼Œæ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼")
                print(f"ğŸ’¡ ç§»é™¤ --test-only å‚æ•°å¯ä»¥æ‰§è¡Œå®Œæ•´åˆ†æ")
                return
            
            # å¦‚æœç”¨æˆ·æƒ³æŸ¥çœ‹åŸå¸‚å»ºè®®
            if args.suggest_city:
                print(f"\nğŸ™ï¸ åŸå¸‚åˆ†æå»ºè®®")
                print("-" * 40)
                
                city_stats_sql = f"""
                SELECT 
                    city_id,
                    COUNT(*) as total_count,
                    COUNT(*) FILTER (WHERE all_good = true) as good_count,
                    ROUND(100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*), 1) as good_percent,
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 50000 THEN '> 10åˆ†é’Ÿ'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 10000 THEN '2-10åˆ†é’Ÿ'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 1000 THEN '< 2åˆ†é’Ÿ'
                        ELSE '< 30ç§’'
                    END as estimated_time,
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) BETWEEN 1000 AND 20000 AND 
                             100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*) > 90 THEN 'â­â­â­ æ¨è'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) BETWEEN 500 AND 50000 AND 
                             100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*) > 85 THEN 'â­â­ è¾ƒå¥½'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 100 THEN 'â­ å¯ç”¨'
                        ELSE 'âŒ ä¸å»ºè®®'
                    END as recommendation
                FROM {view_name}
                WHERE city_id IS NOT NULL
                GROUP BY city_id
                HAVING COUNT(*) FILTER (WHERE all_good = true) > 0
                ORDER BY 
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) BETWEEN 1000 AND 20000 AND 
                             100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*) > 90 THEN 1
                        WHEN COUNT(*) FILTER (WHERE all_good = true) BETWEEN 500 AND 50000 AND 
                             100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*) > 85 THEN 2
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 100 THEN 3
                        ELSE 4
                    END,
                    COUNT(*) FILTER (WHERE all_good = true) DESC;
                """
                
                city_df = pd.read_sql(city_stats_sql, engine)
                if not city_df.empty:
                    print("ğŸ“Š åŸå¸‚åˆ†æå»ºè®®è¡¨:")
                    print(city_df.to_string(index=False))
                    
                    recommended = city_df[city_df['recommendation'].str.contains('â­â­â­')]
                    if not recommended.empty:
                        best_city = recommended.iloc[0]['city_id']
                        print(f"\nğŸ’¡ æ¨èåŸå¸‚: {best_city}")
                        print(f"   - å»ºè®®å‘½ä»¤: --city {best_city}")
                else:
                    print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„åŸå¸‚æ•°æ®")
                return
            
            # å¦‚æœç”¨æˆ·æƒ³è¯Šæ–­æ•°æ®çŠ¶æ€
            if args.diagnose:
                print(f"\nğŸ” æ•°æ®çŠ¶æ€è¯Šæ–­")
                print("-" * 40)
                
                # æ£€æŸ¥åˆ†è¡¨æ•°é‡å’Œæ•°æ®é‡
                print(f"ğŸ“‹ åˆ†è¡¨çŠ¶æ€:")
                print(f"   å‘ç° {len(bbox_tables)} ä¸ªbboxåˆ†è¡¨")
                print(f"   è§†å›¾è®°å½•æ•°: {row_count:,}")
                
                # æ£€æŸ¥æ•°æ®è´¨é‡åˆ†å¸ƒ
                quality_sql = f"""
                SELECT 
                    COUNT(*) as total,
                    COUNT(*) FILTER (WHERE all_good = true) as good,
                    COUNT(DISTINCT city_id) as cities,
                    COUNT(DISTINCT subdataset_name) as subdatasets
                FROM {view_name};
                """
                quality_result = conn.execute(text(quality_sql)).fetchone()
                good_percent = (quality_result.good / quality_result.total * 100) if quality_result.total > 0 else 0
                
                print(f"ğŸ“Š æ•°æ®è´¨é‡:")
                print(f"   æ€»è®°å½•æ•°: {quality_result.total:,}")
                print(f"   ä¼˜è´¨è®°å½•: {quality_result.good:,} ({good_percent:.1f}%)")
                print(f"   åŸå¸‚æ•°é‡: {quality_result.cities}")
                print(f"   æ•°æ®é›†æ•°é‡: {quality_result.subdatasets}")
                
                # å»ºè®®
                if quality_result.good < 1000:
                    print(f"\nğŸ’¡ å»ºè®®: æ•°æ®é‡è¾ƒå°ï¼Œä»»ä½•åŸå¸‚éƒ½å¯å¿«é€Ÿåˆ†æ")
                elif quality_result.good > 100000:
                    print(f"\nğŸ’¡ å»ºè®®: æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨ --suggest-city é€‰æ‹©åˆé€‚åŸå¸‚")
                else:
                    print(f"\nğŸ’¡ å»ºè®®: æ•°æ®çŠ¶æ€è‰¯å¥½ï¼Œå¯ä»¥è¿›è¡Œoverlapåˆ†æ")
                
                print(f"\nâœ… è¯Šæ–­å®Œæˆ")
                return
            
            # å¦‚æœç”¨æˆ·æƒ³æ¸…ç†è§†å›¾
            if args.cleanup_views:
                print(f"\nğŸ§¹ æ¸…ç†bboxç›¸å…³è§†å›¾")
                print("-" * 40)
                
                views_to_check = [
                    'clips_bbox_unified_qgis',
                    'clips_bbox_unified_mat', 
                    'qgis_bbox_overlap_hotspots'
                ]
                
                check_views_sql = text("""
                    SELECT table_name 
                    FROM information_schema.views 
                    WHERE table_schema = 'public' 
                    AND table_name = ANY(:view_names);
                """)
                
                existing_views = conn.execute(check_views_sql, {'view_names': views_to_check}).fetchall()
                existing_view_names = [row[0] for row in existing_views]
                
                if not existing_view_names:
                    print("âœ… æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„è§†å›¾")
                else:
                    print(f"ğŸ“‹ æ‰¾åˆ°ä»¥ä¸‹è§†å›¾ï¼Œå°†åˆ é™¤:")
                    for view_name_to_delete in existing_view_names:
                        print(f"   - {view_name_to_delete}")
                    
                    for view_name_to_delete in existing_view_names:
                        try:
                            drop_sql = text(f"DROP VIEW IF EXISTS {view_name_to_delete} CASCADE;")
                            conn.execute(drop_sql)
                            print(f"âœ… åˆ é™¤è§†å›¾: {view_name_to_delete}")
                        except Exception as e:
                            print(f"âŒ åˆ é™¤å¤±è´¥ {view_name_to_delete}: {str(e)}")
                    
                    conn.commit()
                    print(f"âœ… è§†å›¾æ¸…ç†å®Œæˆ")
                
                return
            
            # å¦‚æœç”¨æˆ·æƒ³ä¼°ç®—æ—¶é—´
            if args.estimate_time:
                print(f"\nâ±ï¸ åˆ†ææ—¶é—´ä¼°ç®—")
                print("-" * 40)
                
                where_condition = f"WHERE city_id = '{args.city}'" if args.city else "WHERE city_id IS NOT NULL"
                time_estimate_sql = f"""
                SELECT 
                    COUNT(*) FILTER (WHERE all_good = true) as analyzable_count,
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 100000 THEN 'âš ï¸ å¾ˆé•¿ (>30åˆ†é’Ÿ)'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 50000 THEN 'â³ è¾ƒé•¿ (10-30åˆ†é’Ÿ)'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 10000 THEN 'â° ä¸­ç­‰ (2-10åˆ†é’Ÿ)'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 1000 THEN 'âš¡ è¾ƒå¿« (<2åˆ†é’Ÿ)'
                        ELSE 'ğŸš€ å¾ˆå¿« (<30ç§’)'
                    END as time_estimate,
                    '{args.city if args.city else "å…¨éƒ¨åŸå¸‚"}' as scope
                FROM {view_name}
                {where_condition};
                """
                
                estimate_result = conn.execute(text(time_estimate_sql)).fetchone()
                print(f"ğŸ“Š åˆ†æèŒƒå›´: {estimate_result.scope}")
                print(f"ğŸ“ˆ å¯åˆ†ææ•°æ®: {estimate_result.analyzable_count:,} ä¸ªbbox")
                print(f"â±ï¸ é¢„ä¼°æ—¶é—´: {estimate_result.time_estimate}")
                
                if estimate_result.analyzable_count > 50000:
                    print(f"ğŸ’¡ å»ºè®®: æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®æŒ‡å®šå…·ä½“åŸå¸‚è¿›è¡Œåˆ†æ")
                return
        
        finally:
            # ç¡®ä¿è¿æ¥æ€»æ˜¯ä¼šè¢«å…³é—­
            if conn:
                conn.close()
        
        # åˆ›å»ºåˆ†æç»“æœè¡¨
        print(f"\nğŸ› ï¸ å‡†å¤‡åˆ†æè¡¨...")
        
        analysis_table = "bbox_overlap_analysis_results"
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        check_table_sql = text(f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = '{analysis_table}'
            );
        """)
        
        with engine.connect() as conn:
            result = conn.execute(check_table_sql)
            table_exists = result.scalar()
            
            if not table_exists:
                print(f"ğŸ“Œ åˆ†æè¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è¡¨...")
                
                # ç›´æ¥ä½¿ç”¨å†…ç½®SQLåˆ›å»ºè¡¨
                create_sql = f"""
                CREATE TABLE {analysis_table} (
                    id SERIAL PRIMARY KEY,
                    analysis_id VARCHAR(100) NOT NULL,
                    analysis_type VARCHAR(50) DEFAULT 'bbox_overlap',
                    analysis_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    hotspot_rank INTEGER,
                    overlap_count INTEGER,
                    total_overlap_area NUMERIC,
                    subdataset_count INTEGER,
                    scene_count INTEGER,
                    involved_subdatasets TEXT[],
                    involved_scenes TEXT[],
                    analysis_params TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                -- æ·»åŠ å‡ ä½•åˆ—
                SELECT AddGeometryColumn('public', '{analysis_table}', 'geometry', 4326, 'GEOMETRY', 2);
                
                -- åˆ›å»ºç´¢å¼•
                CREATE INDEX idx_{analysis_table}_analysis_id ON {analysis_table} (analysis_id);
                CREATE INDEX idx_{analysis_table}_rank ON {analysis_table} (hotspot_rank);
                CREATE INDEX idx_{analysis_table}_geom ON {analysis_table} USING GIST (geometry);
                """
                
                conn.execute(text(create_sql))
                conn.commit()
                print(f"âœ… åˆ†æè¡¨åˆ›å»ºæˆåŠŸ")
            else:
                print(f"âœ… åˆ†æè¡¨å·²å­˜åœ¨")
        
        # ç”Ÿæˆåˆ†æID
        if not args.analysis_id:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            analysis_id = f"overlap_docker_{timestamp}"
        else:
            analysis_id = args.analysis_id
        
        print(f"\nğŸš€ å¼€å§‹å ç½®åˆ†æ: {analysis_id}")
        
        # ğŸš¨ å¼ºåˆ¶åŸå¸‚è¿‡æ»¤æœºåˆ¶
        if not args.city:
            print("âŒ é”™è¯¯: å¿…é¡»æŒ‡å®šåŸå¸‚è¿‡æ»¤æ¡ä»¶ä»¥é¿å…æ€§èƒ½é—®é¢˜")
            print("ğŸ’¡ ä½¿ç”¨ --suggest-city æŸ¥çœ‹æ¨èçš„åŸå¸‚")
            print("ğŸ’¡ æˆ–ä½¿ç”¨ --city your_city_name æŒ‡å®šåŸå¸‚")
            print("ğŸ’¡ ç¤ºä¾‹: --city A72")
            return
        
        print(f"ğŸ™ï¸ åŸå¸‚è¿‡æ»¤: {args.city}")
        
        # æ„å»ºé¢å¤–è¿‡æ»¤æ¡ä»¶
        where_conditions = []
        if args.subdatasets:
            subdataset_list = "', '".join(args.subdatasets)
            where_conditions.append(f"a.subdataset_name IN ('{subdataset_list}') AND b.subdataset_name IN ('{subdataset_list}')")
            print(f"ğŸ“¦ å­æ•°æ®é›†è¿‡æ»¤: {len(args.subdatasets)} ä¸ª")
        
        where_clause = "AND " + " AND ".join(where_conditions) if where_conditions else ""
        
        # ğŸš€ æ‰§è¡Œç½‘æ ¼åŒ–åˆ†æSQLï¼ˆé»˜è®¤æ–¹æ³•ï¼‰
        print(f"ğŸ”¥ ä½¿ç”¨ç½‘æ ¼åŒ–åˆ†æï¼Œé¿å…è¿é”èšåˆé—®é¢˜...")
        
        # ğŸ¯ æ–°æ–¹æ³•ï¼šåªæ£€æŸ¥åŸå¸‚èŒƒå›´ï¼Œä¸é¢„ä¼°ç½‘æ ¼æ•°é‡ï¼ˆå› ä¸ºæˆ‘ä»¬æŒ‰éœ€ç”Ÿæˆï¼‰
        with engine.connect() as conn:
            city_check_sql = text(f"""
                WITH city_bbox AS (
                    SELECT ST_Envelope(ST_Union(geometry)) as city_envelope
                    FROM {view_name} 
                    WHERE city_id = '{args.city}' AND all_good = true
                )
                SELECT 
                    ST_XMax(city_envelope) - ST_XMin(city_envelope) as width_degrees,
                    ST_YMax(city_envelope) - ST_YMin(city_envelope) as height_degrees,
                    COUNT(*) as bbox_count
                FROM city_bbox, {view_name} 
                WHERE city_id = '{args.city}' AND all_good = true
                GROUP BY 1, 2;
            """)
            
            city_check = conn.execute(city_check_sql).fetchone()
            
            print(f"ğŸ“ åŸå¸‚èŒƒå›´: {city_check.width_degrees:.4f}Â° Ã— {city_check.height_degrees:.4f}Â°")
            print(f"ğŸ“¦ bboxæ•°é‡: {city_check.bbox_count:,} ä¸ª")
            print(f"ğŸ“Š ç½‘æ ¼å¤§å°: {args.grid_size}Â° Ã— {args.grid_size}Â° (çº¦200mÃ—200m)")
            print(f"ğŸ¯ åˆ†ææ–¹æ³•: bboxå¯†åº¦åˆ†æ")
        
        # ç¡®å®šå¯†åº¦é˜ˆå€¼ï¼ˆå…¼å®¹æ—§å‚æ•°ï¼‰
        if args.density_threshold:
            density_threshold_sql = f"AND COUNT(*) >= {args.density_threshold}"
            print(f"ğŸ“Š ä½¿ç”¨å›ºå®šé˜ˆå€¼: {args.density_threshold}")
        else:
            density_threshold_sql = f"AND COUNT(*) >= (SELECT PERCENTILE_CONT({args.percentile/100.0}) WITHIN GROUP (ORDER BY grid_density) FROM temp_grid_densities)"
            print(f"ğŸ“Š ä½¿ç”¨åŠ¨æ€é˜ˆå€¼: {args.percentile}åˆ†ä½æ•°")
        
        analysis_sql = f"""
            WITH bbox_bounds AS (
                -- ğŸš€ ç¬¬1æ­¥ï¼šæå–bboxè¾¹ç•Œï¼ˆä¸€æ¬¡æ€§å‡ ä½•è®¡ç®—ï¼‰
                SELECT 
                    id,
                    subdataset_name,
                    scene_token,
                    ST_XMin(geometry) as xmin,
                    ST_XMax(geometry) as xmax,
                    ST_YMin(geometry) as ymin,
                    ST_YMax(geometry) as ymax
                FROM {view_name}
                WHERE city_id = '{args.city}' AND all_good = true
                {where_clause.replace('a.', '').replace('b.', '').replace(' AND  AND', ' AND')}
            ),
            bbox_grid_coverage AS (
                -- ğŸ¯ ç¬¬2æ­¥ï¼šè®¡ç®—æ¯ä¸ªbboxè¦†ç›–çš„ç½‘æ ¼èŒƒå›´ï¼ˆçº¯æ•°å­¦è®¡ç®—ï¼‰
                SELECT 
                    id,
                    subdataset_name,
                    scene_token,
                    floor(xmin / {args.grid_size})::int as min_grid_x,
                    floor(xmax / {args.grid_size})::int as max_grid_x,
                    floor(ymin / {args.grid_size})::int as min_grid_y,
                    floor(ymax / {args.grid_size})::int as max_grid_y,
                    CASE 
                        WHEN {not args.calculate_area} THEN 1.0
                        ELSE (xmax - xmin) * (ymax - ymin)  -- bboxé¢ç§¯
                    END as bbox_area
                FROM bbox_bounds
            ),
            expanded_grid_coverage AS (
                -- ğŸ”§ ç¬¬3æ­¥ï¼šå±•å¼€æ¯ä¸ªbboxåˆ°å®ƒè¦†ç›–çš„æ‰€æœ‰ç½‘æ ¼
                SELECT 
                    id,
                    subdataset_name,
                    scene_token,
                    bbox_area,
                    grid_x,
                    grid_y
                FROM bbox_grid_coverage,
                LATERAL generate_series(min_grid_x, max_grid_x) as grid_x,
                LATERAL generate_series(min_grid_y, max_grid_y) as grid_y
            ),
            all_grid_densities AS (
                -- ğŸ“Š ç¬¬4aæ­¥ï¼šè®¡ç®—æ‰€æœ‰ç½‘æ ¼çš„å¯†åº¦ï¼ˆç”¨äºåˆ†ä½æ•°è®¡ç®—ï¼‰
                SELECT 
                    grid_x,
                    grid_y,
                    COUNT(*) as grid_density
                FROM expanded_grid_coverage
                GROUP BY grid_x, grid_y
            ),
            temp_grid_densities AS (
                -- ä¸´æ—¶è¡¨å­˜å‚¨å¯†åº¦å€¼ç”¨äºåˆ†ä½æ•°è®¡ç®—
                SELECT grid_density FROM all_grid_densities
            ),
            high_density_grids AS (
                -- ğŸ“Š ç¬¬4bæ­¥ï¼šç­›é€‰é«˜å¯†åº¦ç½‘æ ¼
                SELECT 
                    g.grid_x,
                    g.grid_y,
                    g.grid_density as bbox_count_in_grid,
                    COUNT(DISTINCT e.subdataset_name) as subdataset_count,
                    COUNT(DISTINCT e.scene_token) as scene_count,
                    ARRAY_AGG(DISTINCT e.subdataset_name) as involved_subdatasets,
                    ARRAY_AGG(DISTINCT e.scene_token) as involved_scenes,
                    SUM(e.bbox_area) as total_bbox_area,
                    -- ğŸ”§ ç”Ÿæˆç½‘æ ¼å‡ ä½•
                    ST_MakeEnvelope(
                        g.grid_x * {args.grid_size}, 
                        g.grid_y * {args.grid_size},
                        (g.grid_x + 1) * {args.grid_size}, 
                        (g.grid_y + 1) * {args.grid_size}, 
                        4326
                    ) as grid_geom
                FROM all_grid_densities g
                JOIN expanded_grid_coverage e ON (g.grid_x = e.grid_x AND g.grid_y = e.grid_y)
                WHERE g.grid_density >= COALESCE(
                    {args.density_threshold if args.density_threshold else 'NULL'},
                    (SELECT PERCENTILE_CONT({args.percentile/100.0}) WITHIN GROUP (ORDER BY grid_density) FROM temp_grid_densities)
                )
                {'AND SUM(e.bbox_area) >= ' + str(args.min_overlap_area) if args.calculate_area and args.min_overlap_area > 0 else ''}
                GROUP BY g.grid_x, g.grid_y, g.grid_density
            ),
            final_grids AS (
                -- ğŸ”— ç¬¬5æ­¥ï¼šæŒ‰ç™¾åˆ†æ¯”é€‰æ‹©æœ€å¯†é›†çš„ç½‘æ ¼
                SELECT 
                    grid_x,
                    grid_y,
                    bbox_count_in_grid,
                    subdataset_count,
                    scene_count,
                    involved_subdatasets,
                    involved_scenes,
                    total_bbox_area,
                    grid_geom,
                    ROW_NUMBER() OVER (ORDER BY bbox_count_in_grid DESC) as density_rank
                FROM high_density_grids
            ),
            top_percent_grids AS (
                -- ğŸ¯ ç¬¬6æ­¥ï¼šè®¡ç®—ç™¾åˆ†æ¯”é˜ˆå€¼å¹¶é™åˆ¶æ•°é‡
                SELECT 
                    grid_x,
                    grid_y,
                    bbox_count_in_grid,
                    subdataset_count,
                    scene_count,
                    involved_subdatasets,
                    involved_scenes,
                    total_bbox_area,
                    grid_geom,
                    density_rank
                FROM final_grids
                WHERE density_rank <= GREATEST(
                    ROUND((SELECT COUNT(*) FROM final_grids) * {args.top_percent} / 100.0),
                    1
                )
                AND density_rank <= {args.max_results}
            )
            INSERT INTO {analysis_table} 
            (analysis_id, hotspot_rank, overlap_count, total_overlap_area, 
             subdataset_count, scene_count, involved_subdatasets, involved_scenes, geometry, analysis_params)
            SELECT 
                '{analysis_id}' as analysis_id,
                density_rank as hotspot_rank,
                bbox_count_in_grid as overlap_count,
                total_bbox_area as total_overlap_area,
                subdataset_count,
                scene_count,
                involved_subdatasets,
                involved_scenes,
                grid_geom as geometry,
                json_build_object(
                    'analysis_type', 'top_percent_grids',
                    'city_filter', '{args.city}',
                    'grid_size', {args.grid_size},
                    'percentile_threshold', {args.percentile if not args.density_threshold else 'null'},
                    'fixed_threshold', {args.density_threshold if args.density_threshold else 'null'},
                    'top_percent', {args.top_percent},
                    'max_results', {args.max_results},
                    'calculate_area', {str(args.calculate_area).lower()},
                    'min_overlap_area', {args.min_overlap_area if args.calculate_area else 'null'}
                )::text as analysis_params
            FROM top_percent_grids
            ORDER BY density_rank;
            """
        
        print(f"âš¡ æ‰§è¡Œbboxå¯†åº¦åˆ†æSQL...")
        print(f"ğŸ’¡ å¯ä»¥ä½¿ç”¨ Ctrl+C å®‰å…¨é€€å‡º")
        
        analysis_start_time = datetime.now()
        check_shutdown()  # æ‰§è¡Œå‰æ£€æŸ¥
        
        with engine.connect() as conn:
            current_connection = conn  # ä¿å­˜è¿æ¥å¼•ç”¨
            
            print(f"ğŸš€ å¼€å§‹æ‰§è¡ŒSQL... ({analysis_start_time.strftime('%H:%M:%S')})")
            sql_start_time = datetime.now()
            
            conn.execute(text(analysis_sql))
            check_shutdown()  # SQLæ‰§è¡Œåæ£€æŸ¥
            
            sql_end_time = datetime.now()
            sql_duration = (sql_end_time - sql_start_time).total_seconds()
            
            conn.commit()
            commit_time = datetime.now()
            commit_duration = (commit_time - sql_end_time).total_seconds()
            
            print(f"âœ… SQLæ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {sql_duration:.2f}ç§’")
            print(f"âœ… æäº¤å®Œæˆï¼Œè€—æ—¶: {commit_duration:.2f}ç§’")
            print(f"ğŸ” æ­£åœ¨ç»Ÿè®¡ç»“æœ...")
            current_connection = None  # æ¸…é™¤è¿æ¥å¼•ç”¨
            
            # è·å–ç»“æœç»Ÿè®¡
            count_sql = text(f"SELECT COUNT(*) FROM {analysis_table} WHERE analysis_id = '{analysis_id}';")
            count_result = conn.execute(count_sql)
            inserted_count = count_result.scalar()
            
            # è®¡ç®—æ€»è€—æ—¶å’Œæ€§èƒ½ç»Ÿè®¡
            total_duration = (commit_time - analysis_start_time).total_seconds()
            
            print(f"âœ… ç½‘æ ¼å¯†åº¦åˆ†æå®Œæˆï¼Œå‘ç° {inserted_count} ä¸ªé«˜å¯†åº¦ç½‘æ ¼")
            print(f"â±ï¸ æ€»è€—æ—¶: {total_duration:.2f}ç§’ (SQL: {sql_duration:.2f}s + æäº¤: {commit_duration:.2f}s)")
            
            # æ€§èƒ½ç»Ÿè®¡
            bbox_count = city_check.bbox_count if 'city_check' in locals() else 0
            if bbox_count > 0:
                bbox_per_sec = bbox_count / max(sql_duration, 0.001)  # é¿å…é™¤é›¶
                print(f"ğŸ“Š å¤„ç†é€Ÿåº¦: {bbox_per_sec:,.0f} bbox/ç§’")
                
            # æ˜¾ç¤ºé˜ˆå€¼ä¿¡æ¯
            if not args.density_threshold:
                threshold_info_sql = text(f"""
                    SELECT PERCENTILE_CONT({args.percentile/100.0}) WITHIN GROUP (ORDER BY grid_density) as threshold
                    FROM (
                        SELECT COUNT(*) as grid_density
                        FROM (
                            SELECT grid_x, grid_y
                            FROM (
                                SELECT 
                                    floor(ST_XMin(geometry) / {args.grid_size})::int as grid_x,
                                    floor(ST_YMin(geometry) / {args.grid_size})::int as grid_y
                                FROM {view_name}
                                WHERE city_id = '{args.city}' AND all_good = true
                                {where_clause.replace('a.', '').replace('b.', '').replace(' AND  AND', ' AND')}
                            ) g1
                        ) g2
                        GROUP BY grid_x, grid_y
                    ) densities;
                """)
                try:
                    threshold_result = conn.execute(threshold_info_sql).scalar()
                    if threshold_result:
                        print(f"ğŸ“Š è®¡ç®—çš„åŠ¨æ€é˜ˆå€¼: {threshold_result:.1f} bbox/ç½‘æ ¼ ({args.percentile}åˆ†ä½æ•°)")
                except Exception:
                    pass  # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œè·³è¿‡é˜ˆå€¼æ˜¾ç¤º
            
            if inserted_count > 0:
                # æ˜¾ç¤ºTOPç»“æœ
                summary_sql = text(f"""
                    SELECT 
                        hotspot_rank as region_rank,
                        overlap_count as total_bbox_count,
                        ROUND(total_overlap_area::numeric, 4) as region_area,
                        subdataset_count,
                        scene_count,
                        CAST(analysis_params::json->>'grid_count' AS INTEGER) as grid_count,
                        ROUND(CAST(analysis_params::json->>'avg_grid_density' AS NUMERIC), 1) as avg_density
                    FROM {analysis_table}
                    WHERE analysis_id = '{analysis_id}'
                    ORDER BY hotspot_rank
                    LIMIT 10;
                """)
                
                result_df = pd.read_sql(summary_sql, engine)
                print(f"\nğŸ“Š TOP {inserted_count} é«˜å¯†åº¦ç½‘æ ¼:")
                print(result_df.to_string(index=False))
                
                # åˆ›å»ºQGISè§†å›¾
                print(f"\nğŸ¨ åˆ›å»ºQGISè§†å›¾...")
                qgis_view = "qgis_bbox_overlap_hotspots"
                
                # å…ˆåˆ é™¤æ—§è§†å›¾ï¼Œé¿å…åˆ—åå†²çª
                drop_view_sql = f"DROP VIEW IF EXISTS {qgis_view} CASCADE;"
                conn.execute(text(drop_view_sql))
                
                view_sql = f"""
                CREATE VIEW {qgis_view} AS
                SELECT 
                    id,
                    analysis_id,
                    hotspot_rank,
                    overlap_count,
                    total_overlap_area,
                    subdataset_count,
                    scene_count,
                    involved_subdatasets,
                    involved_scenes,
                    CASE 
                        WHEN overlap_count >= 10 THEN 'High Density'
                        WHEN overlap_count >= 5 THEN 'Medium Density'
                        ELSE 'Low Density'
                    END as density_level,
                    geometry,
                    created_at
                FROM {analysis_table}
                WHERE analysis_type = 'bbox_overlap'
                ORDER BY hotspot_rank;
                """
                
                conn.execute(text(view_sql))
                conn.commit()
                print(f"âœ… QGISè§†å›¾ {qgis_view} åˆ›å»ºæˆåŠŸ")
                
                # è¾“å‡ºQGISè¿æ¥ä¿¡æ¯
                print(f"\nğŸ¯ QGISå¯è§†åŒ–æŒ‡å¯¼")
                print(f"=" * 40)
                print(f"ğŸ“‹ æ•°æ®åº“è¿æ¥ä¿¡æ¯:")
                print(f"   host: local_pg")
                print(f"   port: 5432") 
                print(f"   database: postgres")
                print(f"   username: postgres")
                print(f"")
                print(f"ğŸ“Š æ¨èåŠ è½½çš„å›¾å±‚:")
                print(f"   1. {view_name} - æ‰€æœ‰bboxæ•°æ®ï¼ˆåº•å›¾ï¼‰")
                print(f"   2. {qgis_view} - é‡å çƒ­ç‚¹åŒºåŸŸ")
                print(f"")
                print(f"ğŸ¨ å¯è§†åŒ–å»ºè®®:")
                print(f"   â€¢ ä¸»é”®: id")
                print(f"   â€¢ å‡ ä½•åˆ—: geometry")
                print(f"   â€¢ æŒ‰ density_level å­—æ®µè®¾ç½®é¢œè‰²")
                print(f"   â€¢ æ˜¾ç¤º overlap_count æ ‡ç­¾")
                print(f"   â€¢ ä½¿ç”¨ analysis_id = '{analysis_id}' è¿‡æ»¤")
                
                print(f"\nğŸ”¥ æ™ºèƒ½åŒºåŸŸåˆ†æç‰¹åˆ«æç¤º:")
                print(f"   â€¢ æ¯ä¸ªåŒºåŸŸç”±å¤šä¸ªç›¸é‚»çš„é«˜å¯†åº¦ç½‘æ ¼ç»„æˆ")
                print(f"   â€¢ overlap_count = è¯¥åŒºåŸŸå†…çš„æ€»bboxæ•°é‡")
                
                if args.density_threshold:
                    print(f"   â€¢ ä½¿ç”¨å›ºå®šé˜ˆå€¼: >= {args.density_threshold} bbox/ç½‘æ ¼")
                else:
                    print(f"   â€¢ ä½¿ç”¨åŠ¨æ€é˜ˆå€¼: {args.percentile}åˆ†ä½æ•°")
                    
                print(f"   â€¢ æœ€å°åŒºåŸŸå¤§å°: >= {args.min_cluster_size} ä¸ªç½‘æ ¼")
                print(f"   â€¢ åŒºåŸŸåˆå¹¶æ–¹æ³•: {args.cluster_method}")
                if args.calculate_area and args.min_overlap_area > 0:
                    print(f"   â€¢ é¢ç§¯é˜ˆå€¼: >= {args.min_overlap_area} å¹³æ–¹åº¦")
                print(f"   â€¢ ğŸ¯ è¿™æ˜¯è¿é€šåŒºåŸŸåˆ†æï¼Œè¯†åˆ«æœ‰æ„ä¹‰çš„ç©ºé—´èšé›†")
                print(f"   â€¢ å»ºè®®ä½¿ç”¨å¡«å……æ ·å¼ + è¾¹ç•Œçº¿ + é€æ˜åº¦ 60%")
                print(f"   â€¢ å¯ä»¥å åŠ åŸå§‹bboxæ•°æ®å¯¹æ¯”æŸ¥çœ‹")
                print(f"   â€¢ åŒºåŸŸå†…çš„ç½‘æ ¼ä¿¡æ¯å­˜å‚¨åœ¨analysis_paramsä¸­")
                
            else:
                print(f"âš ï¸ æœªå‘ç°è¿é€šå¯†é›†åŒºåŸŸï¼Œå»ºè®®:")
                if not args.density_threshold:
                    print(f"   â€¢ é™ä½åˆ†ä½æ•°é˜ˆå€¼: --percentile 75")
                else:
                    print(f"   â€¢ é™ä½å›ºå®šé˜ˆå€¼: --density-threshold 3")
                if args.min_cluster_size > 1:
                    print(f"   â€¢ å‡å°æœ€å°åŒºåŸŸå¤§å°: --min-cluster-size 1")
                print(f"   â€¢ å¢å¤§ç½‘æ ¼å°ºå¯¸: --grid-size 0.005")
                if args.calculate_area and args.min_overlap_area > 0:
                    print(f"   â€¢ é™ä½é¢ç§¯é˜ˆå€¼: --min-overlap-area 0")
                print(f"   â€¢ æ£€æŸ¥æ•°æ®æ˜¯å¦åœ¨åŒä¸€åŒºåŸŸ")
                print(f"   â€¢ å°è¯•ä¸åŒçš„åŸå¸‚è¿‡æ»¤æ¡ä»¶")
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼åˆ†æID: {analysis_id}")
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
