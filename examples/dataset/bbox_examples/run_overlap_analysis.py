#!/usr/bin/env python3
"""
Dockerå…¼å®¹çš„bboxå ç½®åˆ†æå¯åŠ¨è„šæœ¬

è¿™ä¸ªè„šæœ¬ä¸“é—¨ä¸ºDockerç¯å¢ƒè®¾è®¡ï¼Œè‡ªåŠ¨å¤„ç†è·¯å¾„å’Œä¾èµ–å¯¼å…¥é—®é¢˜ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    # åœ¨Dockerå®¹å™¨ä¸­è¿è¡Œ
    python examples/dataset/bbox_examples/run_overlap_analysis.py

    # å¸¦å‚æ•°è¿è¡Œ
    python examples/dataset/bbox_examples/run_overlap_analysis.py \
        --city beijing --refresh-view --top-n 15
"""

import sys
import os
import signal
import atexit
from pathlib import Path

def setup_environment():
    """è®¾ç½®è¿è¡Œç¯å¢ƒï¼Œå¤„ç†è·¯å¾„å’Œå¯¼å…¥é—®é¢˜"""
    
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    script_path = Path(__file__).resolve()
    project_root = script_path.parent.parent.parent
    
    print(f"ğŸ”§ è„šæœ¬ä½ç½®: {script_path}")
    print(f"ğŸ”§ é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    
    # æ·»åŠ å¯èƒ½çš„è·¯å¾„
    paths_to_add = [
        str(project_root),
        str(project_root / "src"),
        "/workspace",
        "/workspace/src"
    ]
    
    for path in paths_to_add:
        if path not in sys.path:
            sys.path.insert(0, path)
    
    print(f"ğŸ”§ Pythonè·¯å¾„:")
    for i, path in enumerate(sys.path[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
        print(f"   {i}: {path}")
    
    # å°è¯•å¯¼å…¥æµ‹è¯•
    try:
        # å°è¯•æ–¹å¼1ï¼šç›´æ¥å¯¼å…¥
        from spdatalab.dataset.bbox import LOCAL_DSN
        print("âœ… å¯¼å…¥æ–¹å¼: ç›´æ¥å¯¼å…¥ spdatalab")
        return True
    except ImportError as e1:
        try:
            # å°è¯•æ–¹å¼2ï¼šä»srcå¯¼å…¥
            from src.spdatalab.dataset.bbox import LOCAL_DSN
            print("âœ… å¯¼å…¥æ–¹å¼: ä»srcå¯¼å…¥ spdatalab")
            return True
        except ImportError as e2:
            print(f"âŒ å¯¼å…¥å¤±è´¥:")
            print(f"   æ–¹å¼1é”™è¯¯: {e1}")
            print(f"   æ–¹å¼2é”™è¯¯: {e2}")
            
            # æ˜¾ç¤ºå½“å‰ç›®å½•ç»“æ„ä»¥ä¾›è°ƒè¯•
            print(f"\nğŸ” å½“å‰ç›®å½•ç»“æ„:")
            cwd = Path.cwd()
            print(f"   å½“å‰å·¥ä½œç›®å½•: {cwd}")
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨spdatalabæ¨¡å—
            possible_paths = [
                cwd / "spdatalab",
                cwd / "src" / "spdatalab", 
                project_root / "spdatalab",
                project_root / "src" / "spdatalab"
            ]
            
            for path in possible_paths:
                exists = path.exists()
                print(f"   {path}: {'å­˜åœ¨' if exists else 'ä¸å­˜åœ¨'}")
                
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Dockerå…¼å®¹çš„BBoxå ç½®åˆ†æ")
    print("=" * 50)
    
    # è®¾ç½®ä¼˜é›…é€€å‡ºå¤„ç†
    shutdown_requested = False
    current_connection = None
    analysis_start_time = None
    
    def signal_handler(signum, frame):
        nonlocal shutdown_requested, current_connection, analysis_start_time
        print(f"\n\nğŸ›‘ æ”¶åˆ°é€€å‡ºä¿¡å· ({signal.Signals(signum).name})")
        shutdown_requested = True
        print(f"ğŸ”„ æ­£åœ¨å®‰å…¨é€€å‡º...")
        
        if analysis_start_time:
            elapsed = datetime.now() - analysis_start_time
            print(f"â±ï¸ å·²è¿è¡Œæ—¶é—´: {elapsed}")
        
        if current_connection:
            try:
                current_connection.close()
                print(f"âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
            except Exception as e:
                print(f"âš ï¸ å…³é—­è¿æ¥æ—¶å‡ºé”™: {e}")
        
        print(f"âœ… ä¼˜é›…é€€å‡ºå®Œæˆ")
        sys.exit(0)
    
    def check_shutdown():
        if shutdown_requested:
            print(f"ğŸ›‘ æ£€æµ‹åˆ°é€€å‡ºè¯·æ±‚ï¼Œåœæ­¢æ‰§è¡Œ")
            sys.exit(0)
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    try:
        signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
        signal.signal(signal.SIGTERM, signal_handler)  # ç»ˆæ­¢ä¿¡å·
        if hasattr(signal, 'SIGBREAK'):  # Windows
            signal.signal(signal.SIGBREAK, signal_handler)
    except ValueError:
        print("âš ï¸ æ— æ³•æ³¨å†Œä¿¡å·å¤„ç†å™¨")
    
    # è®¾ç½®ç¯å¢ƒ
    if not setup_environment():
        print("\nâŒ ç¯å¢ƒè®¾ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        sys.exit(1)
    
    print("\nğŸš€ ç¯å¢ƒè®¾ç½®æˆåŠŸï¼Œå¼€å§‹å¯¼å…¥æ¨¡å—...")
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        import argparse
        from datetime import datetime
        
        # å°è¯•å¯¼å…¥åˆ†æå™¨ï¼ˆä½¿ç”¨ç»Ÿä¸€è§†å›¾ï¼‰
        try:
            from spdatalab.dataset.bbox import (
                create_unified_view,
                list_bbox_tables,
                LOCAL_DSN
            )
        except ImportError:
            from src.spdatalab.dataset.bbox import (
                create_unified_view,
                list_bbox_tables,
                LOCAL_DSN
            )
        
        from sqlalchemy import create_engine, text
        import pandas as pd
        
        print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        parser = argparse.ArgumentParser(description='Dockerå…¼å®¹çš„BBoxå ç½®åˆ†æï¼ˆä¼˜åŒ–ç‰ˆï¼‰')
        parser.add_argument('--city', required=False, help='åŸå¸‚è¿‡æ»¤ï¼ˆå¼ºçƒˆå»ºè®®æŒ‡å®šä»¥é¿å…æ€§èƒ½é—®é¢˜ï¼‰')
        parser.add_argument('--subdatasets', nargs='+', help='å­æ•°æ®é›†è¿‡æ»¤')
        parser.add_argument('--min-overlap-area', type=float, default=0.0, help='æœ€å°é‡å é¢ç§¯é˜ˆå€¼ï¼ˆåªåœ¨--calculate-areaæ—¶ç”Ÿæ•ˆï¼‰')
        parser.add_argument('--top-n', type=int, default=15, help='è¿”å›çš„çƒ­ç‚¹æ•°é‡')
        parser.add_argument('--analysis-id', help='è‡ªå®šä¹‰åˆ†æID')
        parser.add_argument('--refresh-view', action='store_true', help='å¼ºåˆ¶åˆ·æ–°ç»Ÿä¸€è§†å›¾')
        parser.add_argument('--test-only', action='store_true', help='åªè¿è¡Œæµ‹è¯•ï¼Œä¸æ‰§è¡Œåˆ†æ')
        parser.add_argument('--suggest-city', action='store_true', help='æ˜¾ç¤ºåŸå¸‚åˆ†æå»ºè®®å¹¶é€€å‡º')
        parser.add_argument('--estimate-time', action='store_true', help='ä¼°ç®—åˆ†ææ—¶é—´å¹¶é€€å‡º')
        # ğŸ”¥ ç½‘æ ¼åŒ–åˆ†æå‚æ•°ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
        parser.add_argument('--grid-size', type=float, default=0.002, help='ç½‘æ ¼å¤§å°ï¼ˆåº¦ï¼‰ï¼Œé»˜è®¤0.002åº¦çº¦200ç±³')
        parser.add_argument('--density-threshold', type=int, default=5, help='æ¯ç½‘æ ¼æœ€å°é‡å æ•°é‡é˜ˆå€¼ï¼Œé»˜è®¤5')
        parser.add_argument('--calculate-area', action='store_true', help='è®¡ç®—é‡å é¢ç§¯å¹¶åº”ç”¨min-overlap-areaé˜ˆå€¼ï¼ˆé»˜è®¤åªæ£€æŸ¥ç›¸äº¤ï¼‰')
        
        args = parser.parse_args()
        
        print(f"\nğŸ“‹ åˆ†æå‚æ•°:")
        print(f"   åŸå¸‚è¿‡æ»¤: {args.city}")
        print(f"   è¿”å›æ•°é‡: {args.top_n}")
        print(f"   å¼ºåˆ¶åˆ·æ–°è§†å›¾: {args.refresh_view}")
        print(f"   ğŸ”¥ ç½‘æ ¼åŒ–åˆ†æ: å·²å¯ç”¨ï¼ˆé»˜è®¤ï¼‰")
        print(f"   ğŸ“ ç½‘æ ¼å¤§å°: {args.grid_size}Â° Ã— {args.grid_size}Â° (çº¦200mÃ—200m)")
        print(f"   ğŸ“Š å¯†åº¦é˜ˆå€¼: {args.density_threshold} é‡å /ç½‘æ ¼")
        print(f"   ğŸ¯ åˆ†ææ¨¡å¼: {'é¢ç§¯è®¡ç®—æ¨¡å¼' if args.calculate_area else 'å¿«é€Ÿç›¸äº¤æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰'}")
        if args.calculate_area and args.min_overlap_area > 0:
            print(f"   ğŸ“ æœ€å°é‡å é¢ç§¯: {args.min_overlap_area}")
        elif args.calculate_area:
            print(f"   ğŸ“ è®¡ç®—é¢ç§¯ä½†ä¸è¿‡æ»¤")
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        print(f"\nğŸ”Œ è¿æ¥æ•°æ®åº“...")
        engine = create_engine(LOCAL_DSN, future=True)
        
        with engine.connect() as conn:
            result = conn.execute(text("SELECT 1 as test;"))
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        
        # æ£€æŸ¥bboxè¡¨
        print(f"\nğŸ“Š æ£€æŸ¥bboxåˆ†è¡¨...")
        tables = list_bbox_tables(engine)
        bbox_tables = [t for t in tables if t.startswith('clips_bbox_') and t != 'clips_bbox']
        print(f"âœ… å‘ç° {len(bbox_tables)} ä¸ªbboxåˆ†è¡¨")
        
        if len(bbox_tables) == 0:
            print("âŒ æ²¡æœ‰å‘ç°bboxåˆ†è¡¨ï¼Œæ— æ³•æ‰§è¡Œåˆ†æ")
            return
        
        # æ£€æŸ¥ç»Ÿä¸€è§†å›¾ï¼ˆä½¿ç”¨æ ‡å‡†è§†å›¾ï¼‰
        print(f"\nğŸ” æ£€æŸ¥ç»Ÿä¸€è§†å›¾...")
        view_name = "clips_bbox_unified"
        
        check_view_sql = text(f"""
            SELECT EXISTS (
                SELECT FROM information_schema.views 
                WHERE table_schema = 'public' 
                AND table_name = '{view_name}'
            );
        """)
        
        # åˆ›å»ºæŒä¹…è¿æ¥ç”¨äºæ•´ä¸ªåˆ†æè¿‡ç¨‹
        conn = engine.connect()
        
        try:
            result = conn.execute(check_view_sql)
            view_exists = result.scalar()
            
            if not view_exists or args.refresh_view:
                if args.refresh_view:
                    print(f"ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œé‡æ–°åˆ›å»ºè§†å›¾...")
                else:
                    print(f"ğŸ“Œ è§†å›¾ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è§†å›¾...")
                
                success = create_unified_view(engine, view_name)
                if not success:
                    print("âŒ ç»Ÿä¸€è§†å›¾åˆ›å»ºå¤±è´¥")
                    return
                print(f"âœ… ç»Ÿä¸€è§†å›¾åˆ›å»ºæˆåŠŸ")
            else:
                print(f"âœ… ç»Ÿä¸€è§†å›¾å·²å­˜åœ¨")
            
            # æ£€æŸ¥æ•°æ®é‡
            count_sql = text(f"SELECT COUNT(*) FROM {view_name};")
            count_result = conn.execute(count_sql)
            row_count = count_result.scalar()
            print(f"ğŸ“Š ç»Ÿä¸€è§†å›¾åŒ…å« {row_count:,} æ¡bboxè®°å½•")
            
            if row_count == 0:
                print("âš ï¸ ç»Ÿä¸€è§†å›¾ä¸ºç©ºï¼Œå¯èƒ½åˆ†è¡¨ä¸­æ²¡æœ‰æ•°æ®")
                return
            
            # å¦‚æœåªæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œåˆ°è¿™é‡Œå°±ç»“æŸ
            if args.test_only:
                print(f"\nâœ… æµ‹è¯•æ¨¡å¼å®Œæˆï¼Œæ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼")
                print(f"ğŸ’¡ ç§»é™¤ --test-only å‚æ•°å¯ä»¥æ‰§è¡Œå®Œæ•´åˆ†æ")
                return
            
            # å¦‚æœç”¨æˆ·æƒ³æŸ¥çœ‹åŸå¸‚å»ºè®®
            if args.suggest_city:
                print(f"\nğŸ™ï¸ åŸå¸‚åˆ†æå»ºè®®")
                print("-" * 40)
                
                city_stats_sql = f"""
                SELECT 
                    city_id,
                    COUNT(*) as total_count,
                    COUNT(*) FILTER (WHERE all_good = true) as good_count,
                    ROUND(100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*), 1) as good_percent,
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 50000 THEN '> 10åˆ†é’Ÿ'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 10000 THEN '2-10åˆ†é’Ÿ'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 1000 THEN '< 2åˆ†é’Ÿ'
                        ELSE '< 30ç§’'
                    END as estimated_time,
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) BETWEEN 1000 AND 20000 AND 
                             100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*) > 90 THEN 'â­â­â­ æ¨è'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) BETWEEN 500 AND 50000 AND 
                             100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*) > 85 THEN 'â­â­ è¾ƒå¥½'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 100 THEN 'â­ å¯ç”¨'
                        ELSE 'âŒ ä¸å»ºè®®'
                    END as recommendation
                FROM {view_name}
                WHERE city_id IS NOT NULL
                GROUP BY city_id
                HAVING COUNT(*) FILTER (WHERE all_good = true) > 0
                ORDER BY 
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) BETWEEN 1000 AND 20000 AND 
                             100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*) > 90 THEN 1
                        WHEN COUNT(*) FILTER (WHERE all_good = true) BETWEEN 500 AND 50000 AND 
                             100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*) > 85 THEN 2
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 100 THEN 3
                        ELSE 4
                    END,
                    COUNT(*) FILTER (WHERE all_good = true) DESC;
                """
                
                city_df = pd.read_sql(city_stats_sql, engine)
                if not city_df.empty:
                    print("ğŸ“Š åŸå¸‚åˆ†æå»ºè®®è¡¨:")
                    print(city_df.to_string(index=False))
                    
                    recommended = city_df[city_df['recommendation'].str.contains('â­â­â­')]
                    if not recommended.empty:
                        best_city = recommended.iloc[0]['city_id']
                        print(f"\nğŸ’¡ æ¨èåŸå¸‚: {best_city}")
                        print(f"   - å»ºè®®å‘½ä»¤: --city {best_city}")
                else:
                    print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„åŸå¸‚æ•°æ®")
                return
            
            # å¦‚æœç”¨æˆ·æƒ³ä¼°ç®—æ—¶é—´
            if args.estimate_time:
                print(f"\nâ±ï¸ åˆ†ææ—¶é—´ä¼°ç®—")
                print("-" * 40)
                
                where_condition = f"WHERE city_id = '{args.city}'" if args.city else "WHERE city_id IS NOT NULL"
                time_estimate_sql = f"""
                SELECT 
                    COUNT(*) FILTER (WHERE all_good = true) as analyzable_count,
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 100000 THEN 'âš ï¸ å¾ˆé•¿ (>30åˆ†é’Ÿ)'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 50000 THEN 'â³ è¾ƒé•¿ (10-30åˆ†é’Ÿ)'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 10000 THEN 'â° ä¸­ç­‰ (2-10åˆ†é’Ÿ)'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 1000 THEN 'âš¡ è¾ƒå¿« (<2åˆ†é’Ÿ)'
                        ELSE 'ğŸš€ å¾ˆå¿« (<30ç§’)'
                    END as time_estimate,
                    '{args.city if args.city else "å…¨éƒ¨åŸå¸‚"}' as scope
                FROM {view_name}
                {where_condition};
                """
                
                estimate_result = conn.execute(text(time_estimate_sql)).fetchone()
                print(f"ğŸ“Š åˆ†æèŒƒå›´: {estimate_result.scope}")
                print(f"ğŸ“ˆ å¯åˆ†ææ•°æ®: {estimate_result.analyzable_count:,} ä¸ªbbox")
                print(f"â±ï¸ é¢„ä¼°æ—¶é—´: {estimate_result.time_estimate}")
                
                if estimate_result.analyzable_count > 50000:
                    print(f"ğŸ’¡ å»ºè®®: æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®æŒ‡å®šå…·ä½“åŸå¸‚è¿›è¡Œåˆ†æ")
                return
        
        finally:
            # ç¡®ä¿è¿æ¥æ€»æ˜¯ä¼šè¢«å…³é—­
            if conn:
                conn.close()
        
        # åˆ›å»ºåˆ†æç»“æœè¡¨
        print(f"\nğŸ› ï¸ å‡†å¤‡åˆ†æè¡¨...")
        
        analysis_table = "bbox_overlap_analysis_results"
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        check_table_sql = text(f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = '{analysis_table}'
            );
        """)
        
        with engine.connect() as conn:
            result = conn.execute(check_table_sql)
            table_exists = result.scalar()
            
            if not table_exists:
                print(f"ğŸ“Œ åˆ†æè¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è¡¨...")
                
                # ç›´æ¥ä½¿ç”¨å†…ç½®SQLåˆ›å»ºè¡¨
                create_sql = f"""
                CREATE TABLE {analysis_table} (
                    id SERIAL PRIMARY KEY,
                    analysis_id VARCHAR(100) NOT NULL,
                    analysis_type VARCHAR(50) DEFAULT 'bbox_overlap',
                    analysis_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    hotspot_rank INTEGER,
                    overlap_count INTEGER,
                    total_overlap_area NUMERIC,
                    subdataset_count INTEGER,
                    scene_count INTEGER,
                    involved_subdatasets TEXT[],
                    involved_scenes TEXT[],
                    analysis_params TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );
                
                -- æ·»åŠ å‡ ä½•åˆ—
                SELECT AddGeometryColumn('public', '{analysis_table}', 'geometry', 4326, 'GEOMETRY', 2);
                
                -- åˆ›å»ºç´¢å¼•
                CREATE INDEX idx_{analysis_table}_analysis_id ON {analysis_table} (analysis_id);
                CREATE INDEX idx_{analysis_table}_rank ON {analysis_table} (hotspot_rank);
                CREATE INDEX idx_{analysis_table}_geom ON {analysis_table} USING GIST (geometry);
                """
                
                conn.execute(text(create_sql))
                conn.commit()
                print(f"âœ… åˆ†æè¡¨åˆ›å»ºæˆåŠŸ")
            else:
                print(f"âœ… åˆ†æè¡¨å·²å­˜åœ¨")
        
        # ç”Ÿæˆåˆ†æID
        if not args.analysis_id:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            analysis_id = f"overlap_docker_{timestamp}"
        else:
            analysis_id = args.analysis_id
        
        print(f"\nğŸš€ å¼€å§‹å ç½®åˆ†æ: {analysis_id}")
        
        # ğŸš¨ å¼ºåˆ¶åŸå¸‚è¿‡æ»¤æœºåˆ¶
        if not args.city:
            print("âŒ é”™è¯¯: å¿…é¡»æŒ‡å®šåŸå¸‚è¿‡æ»¤æ¡ä»¶ä»¥é¿å…æ€§èƒ½é—®é¢˜")
            print("ğŸ’¡ ä½¿ç”¨ --suggest-city æŸ¥çœ‹æ¨èçš„åŸå¸‚")
            print("ğŸ’¡ æˆ–ä½¿ç”¨ --city your_city_name æŒ‡å®šåŸå¸‚")
            print("ğŸ’¡ ç¤ºä¾‹: --city A72")
            return
        
        print(f"ğŸ™ï¸ åŸå¸‚è¿‡æ»¤: {args.city}")
        
        # æ„å»ºé¢å¤–è¿‡æ»¤æ¡ä»¶
        where_conditions = []
        if args.subdatasets:
            subdataset_list = "', '".join(args.subdatasets)
            where_conditions.append(f"a.subdataset_name IN ('{subdataset_list}') AND b.subdataset_name IN ('{subdataset_list}')")
            print(f"ğŸ“¦ å­æ•°æ®é›†è¿‡æ»¤: {len(args.subdatasets)} ä¸ª")
        
        where_clause = "AND " + " AND ".join(where_conditions) if where_conditions else ""
        
        # ğŸš€ æ‰§è¡Œç½‘æ ¼åŒ–åˆ†æSQLï¼ˆé»˜è®¤æ–¹æ³•ï¼‰
        print(f"ğŸ”¥ ä½¿ç”¨ç½‘æ ¼åŒ–åˆ†æï¼Œé¿å…è¿é”èšåˆé—®é¢˜...")
        
        # ğŸ¯ æ–°æ–¹æ³•ï¼šåªæ£€æŸ¥åŸå¸‚èŒƒå›´ï¼Œä¸é¢„ä¼°ç½‘æ ¼æ•°é‡ï¼ˆå› ä¸ºæˆ‘ä»¬æŒ‰éœ€ç”Ÿæˆï¼‰
        with engine.connect() as conn:
            city_check_sql = text(f"""
                WITH city_bbox AS (
                    SELECT ST_Envelope(ST_Union(geometry)) as city_envelope
                    FROM {view_name} 
                    WHERE city_id = '{args.city}' AND all_good = true
                )
                SELECT 
                    ST_XMax(city_envelope) - ST_XMin(city_envelope) as width_degrees,
                    ST_YMax(city_envelope) - ST_YMin(city_envelope) as height_degrees,
                    COUNT(*) as bbox_count
                FROM city_bbox, {view_name} 
                WHERE city_id = '{args.city}' AND all_good = true
                GROUP BY 1, 2;
            """)
            
            city_check = conn.execute(city_check_sql).fetchone()
            
            print(f"ğŸ“ åŸå¸‚èŒƒå›´: {city_check.width_degrees:.4f}Â° Ã— {city_check.height_degrees:.4f}Â°")
            print(f"ğŸ“¦ bboxæ•°é‡: {city_check.bbox_count:,} ä¸ª")
            print(f"ğŸ“Š ç½‘æ ¼å¤§å°: {args.grid_size}Â° Ã— {args.grid_size}Â° (çº¦200mÃ—200m)")
            print(f"ğŸ’¡ æ–°æ–¹æ³•ï¼šåªä¸ºæœ‰é‡å çš„åŒºåŸŸç”Ÿæˆç½‘æ ¼ï¼Œé¿å…ç©ºç½‘æ ¼è®¡ç®—")
        
        analysis_sql = f"""
            WITH overlap_pairs AS (
                -- ğŸš€ ç¬¬1æ­¥ï¼šè®¡ç®—é‡å å¯¹ï¼ˆè¿™ä¸ªæ— æ³•é¿å…ï¼‰
                SELECT 
                    a.id as bbox_a_id,
                    b.id as bbox_b_id,
                    a.subdataset_name as subdataset_a,
                    b.subdataset_name as subdataset_b,
                    a.scene_token as scene_a,
                    b.scene_token as scene_b,
                    ST_Intersection(a.geometry, b.geometry) as overlap_geom
                FROM {view_name} a
                JOIN {view_name} b ON a.id < b.id
                WHERE a.city_id = '{args.city}' AND b.city_id = '{args.city}'
                AND a.all_good = true AND b.all_good = true
                AND a.geometry && b.geometry  -- å¿«é€Ÿè¾¹ç•Œæ¡†æ£€æŸ¥
                AND ST_Intersects(a.geometry, b.geometry)
                AND NOT ST_Equals(a.geometry, b.geometry)
                {where_clause}
            ),
            grid_assigned_overlaps AS (
                -- ğŸ¯ ç¬¬2æ­¥ï¼šæ•°å­¦è®¡ç®—æ¯ä¸ªé‡å å±äºå“ªä¸ªç½‘æ ¼ï¼ˆé«˜æ•ˆï¼ï¼‰
                SELECT 
                    *,
                    -- ç›´æ¥è®¡ç®—ç½‘æ ¼åæ ‡ï¼Œæ— éœ€é¢„ç”Ÿæˆç½‘æ ¼
                    floor(ST_X(ST_Centroid(overlap_geom)) / {args.grid_size})::int as grid_x,
                    floor(ST_Y(ST_Centroid(overlap_geom)) / {args.grid_size})::int as grid_y,
                    CASE 
                        WHEN {not args.calculate_area} THEN 1.0
                        ELSE ST_Area(overlap_geom)
                    END as overlap_area
                FROM overlap_pairs
            ),
            grid_overlap_stats AS (
                -- ğŸ“Š ç¬¬3æ­¥ï¼šæŒ‰ç½‘æ ¼åˆ†ç»„ç»Ÿè®¡ï¼ˆåªå¤„ç†æœ‰æ•°æ®çš„ç½‘æ ¼ï¼‰
                SELECT 
                    grid_x,
                    grid_y,
                    COUNT(*) as overlap_count_in_grid,
                    ARRAY_AGG(DISTINCT subdataset_a) || ARRAY_AGG(DISTINCT subdataset_b) as involved_subdatasets,
                    ARRAY_AGG(DISTINCT scene_a) || ARRAY_AGG(DISTINCT scene_b) as involved_scenes,
                    SUM(overlap_area) as total_overlap_area,
                    -- ğŸ”§ æŒ‰éœ€ç”Ÿæˆç½‘æ ¼å‡ ä½•ï¼ˆåªä¸ºæœ‰æ•°æ®çš„ç½‘æ ¼ï¼‰
                    ST_MakeEnvelope(
                        grid_x * {args.grid_size}, 
                        grid_y * {args.grid_size},
                        (grid_x + 1) * {args.grid_size}, 
                        (grid_y + 1) * {args.grid_size}, 
                        4326
                    ) as grid_geom
                FROM grid_assigned_overlaps
                GROUP BY grid_x, grid_y
                HAVING COUNT(*) >= {args.density_threshold}
                   AND ({not args.calculate_area} OR SUM(overlap_area) >= {args.min_overlap_area})
            )
            INSERT INTO {analysis_table} 
            (analysis_id, hotspot_rank, overlap_count, total_overlap_area, 
             subdataset_count, scene_count, involved_subdatasets, involved_scenes, geometry, analysis_params)
            SELECT 
                '{analysis_id}' as analysis_id,
                ROW_NUMBER() OVER (ORDER BY overlap_count_in_grid DESC) as hotspot_rank,
                overlap_count_in_grid as overlap_count,
                total_overlap_area,
                ARRAY_LENGTH(involved_subdatasets, 1) as subdataset_count,
                ARRAY_LENGTH(involved_scenes, 1) as scene_count,
                involved_subdatasets,
                involved_scenes,
                grid_geom as geometry,
                '{{"city_filter": "{args.city}", "grid_size": {args.grid_size}, "density_threshold": {args.density_threshold}, "calculate_area": {args.calculate_area}, "grid_coords": "(" || grid_x || "," || grid_y || ")"}}' as analysis_params
            FROM grid_overlap_stats
            ORDER BY overlap_count_in_grid DESC
            LIMIT {args.top_n};
            """
        
        print(f"âš¡ æ‰§è¡Œç©ºé—´å ç½®åˆ†æSQL...")
        print(f"ğŸ’¡ å¯ä»¥ä½¿ç”¨ Ctrl+C å®‰å…¨é€€å‡º")
        analysis_start_time = datetime.now()
        check_shutdown()  # æ‰§è¡Œå‰æ£€æŸ¥
        
        with engine.connect() as conn:
            current_connection = conn  # ä¿å­˜è¿æ¥å¼•ç”¨
            
            conn.execute(text(analysis_sql))
            check_shutdown()  # SQLæ‰§è¡Œåæ£€æŸ¥
            
            conn.commit()
            print(f"âœ… SQLæ‰§è¡Œå®Œæˆï¼Œæ­£åœ¨ç»Ÿè®¡ç»“æœ...")
            current_connection = None  # æ¸…é™¤è¿æ¥å¼•ç”¨
            
            # è·å–ç»“æœç»Ÿè®¡
            count_sql = text(f"SELECT COUNT(*) FROM {analysis_table} WHERE analysis_id = '{analysis_id}';")
            count_result = conn.execute(count_sql)
            inserted_count = count_result.scalar()
            
            print(f"âœ… å ç½®åˆ†æå®Œæˆï¼Œå‘ç° {inserted_count} ä¸ªé‡å çƒ­ç‚¹")
            
            if inserted_count > 0:
                # æ˜¾ç¤ºTOPç»“æœ
                summary_sql = text(f"""
                    SELECT 
                        hotspot_rank,
                        overlap_count,
                        ROUND(total_overlap_area::numeric, 4) as total_overlap_area,
                        subdataset_count,
                        scene_count
                    FROM {analysis_table}
                    WHERE analysis_id = '{analysis_id}'
                    ORDER BY hotspot_rank
                    LIMIT 5;
                """)
                
                result_df = pd.read_sql(summary_sql, engine)
                print(f"\nğŸ“Š TOP 5 é‡å çƒ­ç‚¹:")
                print(result_df.to_string(index=False))
                
                # åˆ›å»ºQGISè§†å›¾
                print(f"\nğŸ¨ åˆ›å»ºQGISè§†å›¾...")
                qgis_view = "qgis_bbox_overlap_hotspots"
                
                # å…ˆåˆ é™¤æ—§è§†å›¾ï¼Œé¿å…åˆ—åå†²çª
                drop_view_sql = f"DROP VIEW IF EXISTS {qgis_view} CASCADE;"
                conn.execute(text(drop_view_sql))
                
                view_sql = f"""
                CREATE VIEW {qgis_view} AS
                SELECT 
                    id,
                    analysis_id,
                    hotspot_rank,
                    overlap_count,
                    total_overlap_area,
                    subdataset_count,
                    scene_count,
                    involved_subdatasets,
                    involved_scenes,
                    CASE 
                        WHEN overlap_count >= 10 THEN 'High Density'
                        WHEN overlap_count >= 5 THEN 'Medium Density'
                        ELSE 'Low Density'
                    END as density_level,
                    geometry,
                    created_at
                FROM {analysis_table}
                WHERE analysis_type = 'bbox_overlap'
                ORDER BY hotspot_rank;
                """
                
                conn.execute(text(view_sql))
                conn.commit()
                print(f"âœ… QGISè§†å›¾ {qgis_view} åˆ›å»ºæˆåŠŸ")
                
                # è¾“å‡ºQGISè¿æ¥ä¿¡æ¯
                print(f"\nğŸ¯ QGISå¯è§†åŒ–æŒ‡å¯¼")
                print(f"=" * 40)
                print(f"ğŸ“‹ æ•°æ®åº“è¿æ¥ä¿¡æ¯:")
                print(f"   host: local_pg")
                print(f"   port: 5432") 
                print(f"   database: postgres")
                print(f"   username: postgres")
                print(f"")
                print(f"ğŸ“Š æ¨èåŠ è½½çš„å›¾å±‚:")
                print(f"   1. {view_name} - æ‰€æœ‰bboxæ•°æ®ï¼ˆåº•å›¾ï¼‰")
                print(f"   2. {qgis_view} - é‡å çƒ­ç‚¹åŒºåŸŸ")
                print(f"")
                print(f"ğŸ¨ å¯è§†åŒ–å»ºè®®:")
                print(f"   â€¢ ä¸»é”®: id")
                print(f"   â€¢ å‡ ä½•åˆ—: geometry")
                print(f"   â€¢ æŒ‰ density_level å­—æ®µè®¾ç½®é¢œè‰²")
                print(f"   â€¢ æ˜¾ç¤º overlap_count æ ‡ç­¾")
                print(f"   â€¢ ä½¿ç”¨ analysis_id = '{analysis_id}' è¿‡æ»¤")
                
                print(f"\nğŸ”¥ ç½‘æ ¼åŒ–åˆ†æç‰¹åˆ«æç¤º:")
                print(f"   â€¢ æ¯ä¸ªçƒ­ç‚¹æ˜¯ {args.grid_size}Â° Ã— {args.grid_size}Â° çš„ç½‘æ ¼ (çº¦200mÃ—200m)")
                print(f"   â€¢ é¢œè‰²æ·±æµ…ä»£è¡¨ç½‘æ ¼å†…é‡å å¯†åº¦")
                print(f"   â€¢ å¯†åº¦é˜ˆå€¼: >= {args.density_threshold} é‡å /ç½‘æ ¼")
                if args.calculate_area and args.min_overlap_area > 0:
                    print(f"   â€¢ é¢ç§¯é˜ˆå€¼: >= {args.min_overlap_area} å¹³æ–¹åº¦")
                print(f"   â€¢ å»ºè®®ä½¿ç”¨å¡«å……æ ·å¼ + é€æ˜åº¦ 70%")
                print(f"   â€¢ å¯ä»¥å åŠ åŸå§‹bboxæ•°æ®å¯¹æ¯”æŸ¥çœ‹")
                
            else:
                print(f"âš ï¸ æœªå‘ç°é‡å çƒ­ç‚¹ï¼Œå»ºè®®:")
                print(f"   â€¢ é™ä½ --min-overlap-area é˜ˆå€¼")
                print(f"   â€¢ æ£€æŸ¥æ•°æ®æ˜¯å¦åœ¨åŒä¸€åŒºåŸŸ")
                print(f"   â€¢ å°è¯•ä¸åŒçš„åŸå¸‚è¿‡æ»¤æ¡ä»¶")
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼åˆ†æID: {analysis_id}")
        
    except Exception as e:
        print(f"\nâŒ åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
