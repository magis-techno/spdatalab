#!/usr/bin/env python3
"""
æ‰¹é‡åŸå¸‚Top1çƒ­ç‚¹åˆ†æè„šæœ¬
=======================

éå†æ‰€æœ‰åŸå¸‚ï¼Œåˆ†ææ¯ä¸ªåŸå¸‚çš„top1é‡å çƒ­ç‚¹åŒºåŸŸ

ä½¿ç”¨æ–¹æ³•ï¼š
    python examples/dataset/bbox_examples/batch_top1_analysis.py
    python examples/dataset/bbox_examples/batch_top1_analysis.py --min-bbox-count 1000
    python examples/dataset/bbox_examples/batch_top1_analysis.py --output-table city_top1_hotspots
"""

import sys
from pathlib import Path
import argparse
from datetime import datetime
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from spdatalab.dataset.bbox import LOCAL_DSN
except ImportError:
    sys.path.insert(0, str(project_root / "src"))
    from spdatalab.dataset.bbox import LOCAL_DSN

from sqlalchemy import create_engine, text
import pandas as pd

def get_all_cities(conn, min_bbox_count=500):
    """è·å–æ‰€æœ‰æœ‰è¶³å¤Ÿbboxæ•°æ®çš„åŸå¸‚"""
    
    print(f"ğŸ” æŸ¥æ‰¾æœ‰è¶³å¤Ÿæ•°æ®çš„åŸå¸‚ï¼ˆæœ€å°‘{min_bbox_count}ä¸ªbboxï¼‰...")
    
    cities_sql = text(f"""
        SELECT 
            city_id,
            COUNT(*) as bbox_count,
            COUNT(*) FILTER (WHERE all_good = true) as good_bbox_count,
            ROUND((COUNT(*) FILTER (WHERE all_good = true) * 100.0 / COUNT(*))::numeric, 1) as quality_rate
        FROM clips_bbox_unified
        WHERE city_id IS NOT NULL 
        GROUP BY city_id
        HAVING COUNT(*) >= {min_bbox_count}
        ORDER BY COUNT(*) DESC;
    """)
    
    cities_df = pd.read_sql(cities_sql, conn)
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(cities_df)} ä¸ªç¬¦åˆæ¡ä»¶çš„åŸå¸‚:")
    print(cities_df.to_string(index=False))
    
    return cities_df

def analyze_city_top1(conn, city_id, grid_size=0.002, density_threshold=5):
    """åˆ†æå•ä¸ªåŸå¸‚çš„top1çƒ­ç‚¹"""
    
    print(f"\nğŸ¯ åˆ†æåŸå¸‚: {city_id}")
    
    # ä½¿ç”¨ä¸run_overlap_analysis.pyç›¸åŒçš„ç½‘æ ¼åˆ†æé€»è¾‘
    analysis_sql = text(f"""
        WITH bbox_bounds AS (
            -- ğŸš€ ç¬¬1æ­¥ï¼šæå–bboxè¾¹ç•Œï¼ˆä¸€æ¬¡æ€§å‡ ä½•è®¡ç®—ï¼‰
            SELECT 
                id,
                subdataset_name,
                scene_token,
                ST_XMin(geometry) as xmin,
                ST_XMax(geometry) as xmax,
                ST_YMin(geometry) as ymin,
                ST_YMax(geometry) as ymax
            FROM clips_bbox_unified
            WHERE city_id = '{city_id}' AND all_good = true
        ),
        bbox_grid_coverage AS (
            -- ğŸ¯ ç¬¬2æ­¥ï¼šè®¡ç®—æ¯ä¸ªbboxè¦†ç›–çš„ç½‘æ ¼èŒƒå›´ï¼ˆçº¯æ•°å­¦è®¡ç®—ï¼‰
            SELECT 
                id,
                subdataset_name,
                scene_token,
                floor(xmin / {grid_size})::int as min_grid_x,
                floor(xmax / {grid_size})::int as max_grid_x,
                floor(ymin / {grid_size})::int as min_grid_y,
                floor(ymax / {grid_size})::int as max_grid_y,
                (xmax - xmin) * (ymax - ymin) as bbox_area
            FROM bbox_bounds
        ),
        expanded_grid_coverage AS (
            -- ğŸ”§ ç¬¬3æ­¥ï¼šå±•å¼€æ¯ä¸ªbboxåˆ°å®ƒè¦†ç›–çš„æ‰€æœ‰ç½‘æ ¼
            SELECT 
                id,
                subdataset_name,
                scene_token,
                bbox_area,
                grid_x,
                grid_y
            FROM bbox_grid_coverage,
            LATERAL generate_series(min_grid_x, max_grid_x) as grid_x,
            LATERAL generate_series(min_grid_y, max_grid_y) as grid_y
        ),
        grid_density_stats AS (
            -- ğŸ“Š ç¬¬4æ­¥ï¼šç»Ÿè®¡æ¯ä¸ªç½‘æ ¼çš„bboxå¯†åº¦
            SELECT 
                grid_x,
                grid_y,
                COUNT(*) as bbox_count_in_grid,
                COUNT(DISTINCT subdataset_name) as subdataset_count,
                COUNT(DISTINCT scene_token) as scene_count,
                ARRAY_AGG(DISTINCT subdataset_name) as involved_subdatasets,
                ARRAY_AGG(DISTINCT scene_token) as involved_scenes,
                SUM(bbox_area) as total_bbox_area,
                -- ğŸ”§ æŒ‰éœ€ç”Ÿæˆç½‘æ ¼å‡ ä½•
                ST_MakeEnvelope(
                    grid_x * {grid_size}, 
                    grid_y * {grid_size},
                    (grid_x + 1) * {grid_size}, 
                    (grid_y + 1) * {grid_size}, 
                    4326
                ) as grid_geom
            FROM expanded_grid_coverage
            GROUP BY grid_x, grid_y
            HAVING COUNT(*) >= {density_threshold}
        )
        -- åªè¿”å›TOP1çƒ­ç‚¹
        SELECT 
            '{city_id}' as city_id,
            grid_x,
            grid_y,
            bbox_count_in_grid,
            subdataset_count,
            scene_count,
            involved_subdatasets,
            involved_scenes,
            total_bbox_area,
            grid_geom,
            '({grid_x},{grid_y})' as grid_coords
        FROM grid_density_stats
        ORDER BY bbox_count_in_grid DESC
        LIMIT 1;
    """)
    
    result = conn.execute(analysis_sql).fetchone()
    
    if result:
        print(f"   âœ… Top1çƒ­ç‚¹: ç½‘æ ¼({result.grid_x},{result.grid_y}), å¯†åº¦={result.bbox_count_in_grid}")
        return result
    else:
        print(f"   âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„çƒ­ç‚¹ï¼ˆå¯†åº¦é˜ˆå€¼>={density_threshold}ï¼‰")
        return None

def create_top1_results_table(conn, table_name):
    """åˆ›å»ºtop1ç»“æœè¡¨"""
    
    print(f"ğŸ“‹ åˆ›å»ºç»“æœè¡¨: {table_name}")
    
    create_sql = text(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            id SERIAL PRIMARY KEY,
            city_id VARCHAR(50) NOT NULL,
            analysis_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            
            -- ç½‘æ ¼ä¿¡æ¯
            grid_x INTEGER,
            grid_y INTEGER,
            grid_coords VARCHAR(50),
            
            -- å¯†åº¦ç»Ÿè®¡
            bbox_count INTEGER,
            subdataset_count INTEGER,
            scene_count INTEGER,
            total_bbox_area NUMERIC,
            
            -- è¯¦ç»†ä¿¡æ¯
            involved_subdatasets TEXT[],
            involved_scenes TEXT[],
            
            -- åˆ†æå‚æ•°
            analysis_params TEXT
        );
        
        -- æ·»åŠ å‡ ä½•åˆ—
        DO $$
        BEGIN
            IF NOT EXISTS (
                SELECT 1 FROM information_schema.columns 
                WHERE table_name = '{table_name}' 
                AND column_name = 'geometry'
            ) THEN
                PERFORM AddGeometryColumn('public', '{table_name}', 'geometry', 4326, 'GEOMETRY', 2);
            END IF;
        END $$;
        
        -- åˆ›å»ºç´¢å¼•
        CREATE INDEX IF NOT EXISTS idx_{table_name}_city_id ON {table_name} (city_id);
        CREATE INDEX IF NOT EXISTS idx_{table_name}_bbox_count ON {table_name} (bbox_count);
        CREATE INDEX IF NOT EXISTS idx_{table_name}_geom ON {table_name} USING GIST (geometry);
        
        -- æ·»åŠ çº¦æŸï¼ˆæ¯ä¸ªåŸå¸‚æ¯å¤©åªèƒ½æœ‰ä¸€æ¡è®°å½•ï¼‰
        CREATE UNIQUE INDEX IF NOT EXISTS idx_{table_name}_unique_city_date 
        ON {table_name} (city_id, DATE(analysis_time));
    """)
    
    conn.execute(create_sql)
    conn.commit()
    print(f"âœ… è¡¨ {table_name} åˆ›å»ºæˆåŠŸ")

def save_top1_result(conn, table_name, city_result, analysis_params):
    """ä¿å­˜å•ä¸ªåŸå¸‚çš„top1ç»“æœ"""
    
    if not city_result:
        return
    
    insert_sql = text(f"""
        INSERT INTO {table_name} 
        (city_id, grid_x, grid_y, grid_coords, bbox_count, subdataset_count, scene_count, 
         total_bbox_area, involved_subdatasets, involved_scenes, geometry, analysis_params)
        VALUES 
        (:city_id, :grid_x, :grid_y, :grid_coords, :bbox_count, :subdataset_count, :scene_count,
         :total_bbox_area, :involved_subdatasets, :involved_scenes, :geometry, :analysis_params);
    """)
    
    conn.execute(insert_sql, {
        'city_id': city_result.city_id,
        'grid_x': city_result.grid_x,
        'grid_y': city_result.grid_y,
        'grid_coords': city_result.grid_coords,
        'bbox_count': city_result.bbox_count_in_grid,
        'subdataset_count': city_result.subdataset_count,
        'scene_count': city_result.scene_count,
        'total_bbox_area': city_result.total_bbox_area,
        'involved_subdatasets': city_result.involved_subdatasets,
        'involved_scenes': city_result.involved_scenes,
        'geometry': city_result.grid_geom,
        'analysis_params': analysis_params
    })

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡åŸå¸‚Top1çƒ­ç‚¹åˆ†æ')
    parser.add_argument('--min-bbox-count', type=int, default=500, 
                       help='åŸå¸‚æœ€å°‘bboxæ•°é‡é˜ˆå€¼ (é»˜è®¤: 500)')
    parser.add_argument('--grid-size', type=float, default=0.002,
                       help='ç½‘æ ¼å¤§å° (é»˜è®¤: 0.002åº¦)')
    parser.add_argument('--density-threshold', type=int, default=5,
                       help='å¯†åº¦é˜ˆå€¼ (é»˜è®¤: 5)')
    parser.add_argument('--output-table', default='city_top1_hotspots',
                       help='è¾“å‡ºè¡¨å (é»˜è®¤: city_top1_hotspots)')
    parser.add_argument('--cities', nargs='+', 
                       help='æŒ‡å®šåˆ†æçš„åŸå¸‚åˆ—è¡¨ï¼Œå¦‚: --cities A263 B001')
    
    args = parser.parse_args()
    
    print("ğŸš€ æ‰¹é‡åŸå¸‚Top1çƒ­ç‚¹åˆ†æ")
    print("=" * 50)
    print(f"å‚æ•°é…ç½®:")
    print(f"  æœ€å°‘bboxæ•°é‡: {args.min_bbox_count}")
    print(f"  ç½‘æ ¼å¤§å°: {args.grid_size}åº¦")
    print(f"  å¯†åº¦é˜ˆå€¼: {args.density_threshold}")
    print(f"  è¾“å‡ºè¡¨: {args.output_table}")
    
    engine = create_engine(LOCAL_DSN, future=True)
    
    try:
        with engine.connect() as conn:
            
            # åˆ›å»ºç»“æœè¡¨
            create_top1_results_table(conn, args.output_table)
            
            # è·å–åŸå¸‚åˆ—è¡¨
            if args.cities:
                print(f"\nğŸ¯ æŒ‡å®šåˆ†æåŸå¸‚: {args.cities}")
                cities_to_analyze = args.cities
            else:
                cities_df = get_all_cities(conn, args.min_bbox_count)
                cities_to_analyze = cities_df['city_id'].tolist()
            
            if not cities_to_analyze:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„åŸå¸‚")
                return 1
            
            # åˆ†æå‚æ•°
            analysis_params = f'{{"grid_size": {args.grid_size}, "density_threshold": {args.density_threshold}, "analysis_type": "top1_hotspot", "timestamp": "{datetime.now().isoformat()}"}}'
            
            # æ‰¹é‡åˆ†æ
            print(f"\nğŸ”„ å¼€å§‹æ‰¹é‡åˆ†æ {len(cities_to_analyze)} ä¸ªåŸå¸‚...")
            
            successful_cities = []
            failed_cities = []
            
            start_time = time.time()
            
            for i, city_id in enumerate(cities_to_analyze, 1):
                print(f"\n[{i}/{len(cities_to_analyze)}] å¤„ç†åŸå¸‚: {city_id}")
                
                try:
                    # åˆ†æåŸå¸‚top1
                    city_result = analyze_city_top1(
                        conn, city_id, 
                        args.grid_size, 
                        args.density_threshold
                    )
                    
                    if city_result:
                        # ä¿å­˜ç»“æœ
                        save_top1_result(conn, args.output_table, city_result, analysis_params)
                        successful_cities.append(city_id)
                        print(f"   âœ… å·²ä¿å­˜åˆ° {args.output_table}")
                    else:
                        failed_cities.append(city_id)
                        
                except Exception as e:
                    print(f"   âŒ åˆ†æå¤±è´¥: {str(e)}")
                    failed_cities.append(city_id)
            
            conn.commit()
            
            # ç»Ÿè®¡ç»“æœ
            total_time = time.time() - start_time
            
            print(f"\n" + "=" * 60)
            print(f"ğŸ“Š æ‰¹é‡åˆ†æå®Œæˆï¼")
            print(f"=" * 60)
            print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"æˆåŠŸåˆ†æ: {len(successful_cities)} ä¸ªåŸå¸‚")
            print(f"å¤±è´¥/æ— ç»“æœ: {len(failed_cities)} ä¸ªåŸå¸‚")
            
            if successful_cities:
                print(f"\nâœ… æˆåŠŸçš„åŸå¸‚: {', '.join(successful_cities)}")
            
            if failed_cities:
                print(f"\nâš ï¸ å¤±è´¥/æ— ç»“æœçš„åŸå¸‚: {', '.join(failed_cities)}")
            
            # æ˜¾ç¤ºç»“æœæ¦‚è§ˆ
            summary_sql = text(f"""
                SELECT 
                    city_id,
                    bbox_count,
                    subdataset_count,
                    scene_count,
                    grid_coords,
                    ROUND(total_bbox_area::numeric, 6) as total_bbox_area
                FROM {args.output_table}
                WHERE analysis_time::date = CURRENT_DATE
                ORDER BY bbox_count DESC;
            """)
            
            results_df = pd.read_sql(summary_sql, conn)
            
            if not results_df.empty:
                print(f"\nğŸ“‹ ä»Šæ—¥Top1çƒ­ç‚¹æ±‡æ€»:")
                print(results_df.to_string(index=False))
                
                print(f"\nğŸ¯ QGISå¯è§†åŒ–:")
                print(f"   è¡¨å: {args.output_table}")
                print(f"   ä¸»é”®: id")
                print(f"   å‡ ä½•åˆ—: geometry")
                print(f"   æŒ‰ bbox_count è®¾ç½®é¢œè‰²ï¼ˆå¯†åº¦è¶Šé«˜è¶Šçƒ­ï¼‰")
                print(f"   æ˜¾ç¤º city_id å’Œ bbox_count æ ‡ç­¾")
            
    except Exception as e:
        print(f"\nâŒ æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
