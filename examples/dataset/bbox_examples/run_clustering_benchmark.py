#!/usr/bin/env python3
"""
PGèšç±»æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
====================

è‡ªåŠ¨è¿è¡ŒPostgreSQLèšç±»æ€§èƒ½æµ‹è¯•å¹¶åˆ†æç»“æœ

ä½¿ç”¨æ–¹æ³•ï¼š
    python examples/dataset/bbox_examples/run_clustering_benchmark.py --city A263
"""

import sys
import time
from pathlib import Path
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from spdatalab.dataset.bbox import LOCAL_DSN
except ImportError:
    sys.path.insert(0, str(project_root / "src"))
    from spdatalab.dataset.bbox import LOCAL_DSN

from sqlalchemy import create_engine, text
import pandas as pd

def run_clustering_benchmark(city_id='A263'):
    """è¿è¡Œèšç±»æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    print("ğŸš€ PostgreSQLèšç±»æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    print(f"æµ‹è¯•åŸå¸‚: {city_id}")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    engine = create_engine(LOCAL_DSN, future=True)
    
    results = {}
    
    with engine.connect() as conn:
        
        # æµ‹è¯•1: åŸºç¡€æ•°æ®ç»Ÿè®¡
        print("\nğŸ“Š æµ‹è¯•1: åŸºç¡€æ•°æ®ç»Ÿè®¡")
        start_time = time.time()
        
        basic_stats_sql = text(f"""
            SELECT 
                COUNT(*) as total_bbox,
                COUNT(*) FILTER (WHERE all_good = true) as good_bbox,
                ROUND(AVG(ST_Area(geometry))::numeric, 8) as avg_area,
                ST_XMax(ST_Extent(geometry)) - ST_XMin(ST_Extent(geometry)) as width_deg,
                ST_YMax(ST_Extent(geometry)) - ST_YMin(ST_Extent(geometry)) as height_deg
            FROM clips_bbox_unified 
            WHERE city_id = '{city_id}' AND all_good = true;
        """)
        
        basic_stats = conn.execute(basic_stats_sql).fetchone()
        basic_time = time.time() - start_time
        
        print(f"   æ€»bboxæ•°: {basic_stats.total_bbox:,}")
        print(f"   ä¼˜è´¨bboxæ•°: {basic_stats.good_bbox:,}")
        print(f"   åŸå¸‚èŒƒå›´: {basic_stats.width_deg:.4f}Â° Ã— {basic_stats.height_deg:.4f}Â°")
        print(f"   æŸ¥è¯¢è€—æ—¶: {basic_time:.2f}ç§’")
        
        results['basic_stats'] = {
            'total_bbox': basic_stats.total_bbox,
            'good_bbox': basic_stats.good_bbox,
            'query_time': basic_time
        }
        
        # æµ‹è¯•2: DBSCANèšç±»æ€§èƒ½
        print("\nâš¡ æµ‹è¯•2: DBSCANèšç±»æ€§èƒ½")
        start_time = time.time()
        
        dbscan_sql = text(f"""
            WITH dbscan_clusters AS (
                SELECT 
                    ST_ClusterDBSCAN(geometry, 0.002, 5) OVER() as cluster_id
                FROM clips_bbox_unified 
                WHERE city_id = '{city_id}' AND all_good = true
            )
            SELECT 
                COUNT(DISTINCT cluster_id) FILTER (WHERE cluster_id IS NOT NULL) as cluster_count,
                COUNT(*) FILTER (WHERE cluster_id IS NULL) as noise_points,
                COUNT(*) as total_points
            FROM dbscan_clusters;
        """)
        
        dbscan_stats = conn.execute(dbscan_sql).fetchone()
        dbscan_time = time.time() - start_time
        
        print(f"   èšç±»æ•°é‡: {dbscan_stats.cluster_count}")
        print(f"   å™ªå£°ç‚¹æ•°: {dbscan_stats.noise_points}")
        print(f"   èšç±»ç‡: {(1 - dbscan_stats.noise_points/dbscan_stats.total_points)*100:.1f}%")
        print(f"   æŸ¥è¯¢è€—æ—¶: {dbscan_time:.2f}ç§’")
        
        results['dbscan'] = {
            'cluster_count': dbscan_stats.cluster_count,
            'noise_points': dbscan_stats.noise_points,
            'clustering_rate': (1 - dbscan_stats.noise_points/dbscan_stats.total_points)*100,
            'query_time': dbscan_time
        }
        
        # æµ‹è¯•3: ç½‘æ ¼æ–¹æ³•å¯¹æ¯”
        print("\nğŸ”² æµ‹è¯•3: ç½‘æ ¼æ–¹æ³•åŸºå‡†")
        start_time = time.time()
        
        grid_sql = text(f"""
            WITH grid_density AS (
                SELECT 
                    floor(ST_X(ST_Centroid(geometry)) / 0.002)::int as grid_x,
                    floor(ST_Y(ST_Centroid(geometry)) / 0.002)::int as grid_y,
                    COUNT(*) as bbox_count
                FROM clips_bbox_unified 
                WHERE city_id = '{city_id}' AND all_good = true
                GROUP BY grid_x, grid_y
                HAVING COUNT(*) >= 5
            )
            SELECT 
                COUNT(*) as total_grids,
                MAX(bbox_count) as max_density,
                ROUND(AVG(bbox_count)::numeric, 2) as avg_density
            FROM grid_density;
        """)
        
        grid_stats = conn.execute(grid_sql).fetchone()
        grid_time = time.time() - start_time
        
        print(f"   ç½‘æ ¼æ•°é‡: {grid_stats.total_grids}")
        print(f"   æœ€é«˜å¯†åº¦: {grid_stats.max_density}")
        print(f"   å¹³å‡å¯†åº¦: {grid_stats.avg_density}")
        print(f"   æŸ¥è¯¢è€—æ—¶: {grid_time:.2f}ç§’")
        
        results['grid'] = {
            'total_grids': grid_stats.total_grids,
            'max_density': grid_stats.max_density,
            'avg_density': float(grid_stats.avg_density),
            'query_time': grid_time
        }
        
        # æµ‹è¯•4: åˆ†å±‚èšç±»æ¨¡æ‹Ÿ
        print("\nğŸ—ï¸ æµ‹è¯•4: åˆ†å±‚èšç±»æ€§èƒ½")
        start_time = time.time()
        
        hierarchical_sql = text(f"""
            WITH coarse_clusters AS (
                SELECT 
                    geometry,
                    ST_ClusterDBSCAN(geometry, 0.01, 20) OVER() as coarse_id
                FROM clips_bbox_unified 
                WHERE city_id = '{city_id}' AND all_good = true
            ),
            fine_clusters AS (
                SELECT 
                    coarse_id,
                    ST_ClusterDBSCAN(geometry, 0.002, 5) OVER(PARTITION BY coarse_id) as fine_id
                FROM coarse_clusters 
                WHERE coarse_id IS NOT NULL
            )
            SELECT 
                COUNT(DISTINCT coarse_id) as coarse_clusters,
                COUNT(DISTINCT (coarse_id, fine_id)) FILTER (WHERE fine_id IS NOT NULL) as fine_clusters
            FROM fine_clusters;
        """)
        
        hierarchical_stats = conn.execute(hierarchical_sql).fetchone()
        hierarchical_time = time.time() - start_time
        
        print(f"   ç²—èšç±»æ•°: {hierarchical_stats.coarse_clusters}")
        print(f"   ç»†èšç±»æ•°: {hierarchical_stats.fine_clusters}")
        print(f"   æŸ¥è¯¢è€—æ—¶: {hierarchical_time:.2f}ç§’")
        
        results['hierarchical'] = {
            'coarse_clusters': hierarchical_stats.coarse_clusters,
            'fine_clusters': hierarchical_stats.fine_clusters,
            'query_time': hierarchical_time
        }
    
    # æ€§èƒ½åˆ†æå’Œå»ºè®®
    print("\n" + "=" * 50)
    print("ğŸ“ˆ æ€§èƒ½åˆ†ææŠ¥å‘Š")
    print("=" * 50)
    
    total_bbox = results['basic_stats']['good_bbox']
    
    print(f"\nğŸ¯ æ•°æ®è§„æ¨¡: {total_bbox:,} bbox")
    
    # æ•ˆç‡å¯¹æ¯”
    print(f"\nâ±ï¸ æ–¹æ³•è€—æ—¶å¯¹æ¯”:")
    print(f"   ç½‘æ ¼æ–¹æ³•:     {results['grid']['query_time']:.2f}ç§’")
    print(f"   DBSCANèšç±»:   {results['dbscan']['query_time']:.2f}ç§’")
    print(f"   åˆ†å±‚èšç±»:     {results['hierarchical']['query_time']:.2f}ç§’")
    
    # ç»“æœè´¨é‡å¯¹æ¯”
    print(f"\nğŸ“Š ç»“æœè´¨é‡å¯¹æ¯”:")
    print(f"   ç½‘æ ¼çƒ­ç‚¹æ•°:   {results['grid']['total_grids']}")
    print(f"   DBSCANèšç±»æ•°: {results['dbscan']['cluster_count']}")
    print(f"   åˆ†å±‚ç»†èšç±»æ•°: {results['hierarchical']['fine_clusters']}")
    
    # æ€§èƒ½å»ºè®®
    print(f"\nğŸ’¡ æ€§èƒ½å»ºè®®:")
    if total_bbox < 10000:
        print("   âœ… æ•°æ®é‡è¾ƒå°ï¼Œå»ºè®®ä½¿ç”¨ç½‘æ ¼æ–¹æ³•ï¼ˆæœ€å¿«ï¼‰")
    elif total_bbox < 50000:
        print("   âš–ï¸ æ•°æ®é‡é€‚ä¸­ï¼ŒDBSCANå’Œç½‘æ ¼æ–¹æ³•æ€§èƒ½æ¥è¿‘")
        print("   ğŸ’¡ å»ºè®®ï¼šç½‘æ ¼æ–¹æ³•ç”¨äºå¯†åº¦åˆ†æï¼ŒDBSCANç”¨äºå½¢çŠ¶è¯†åˆ«")
    else:
        print("   ğŸš€ æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨åˆ†å±‚èšç±»æ–¹æ³•")
        print("   ğŸ’¡ ç­–ç•¥ï¼šDBSCANç²—ç­› + ç½‘æ ¼ç²¾åŒ–")
    
    # æ•ˆç‡æ¯”è¾ƒ
    grid_speed = total_bbox / results['grid']['query_time']
    dbscan_speed = total_bbox / results['dbscan']['query_time']
    
    print(f"\nğŸ“Š å¤„ç†é€Ÿåº¦:")
    print(f"   ç½‘æ ¼æ–¹æ³•: {grid_speed:,.0f} bbox/ç§’")
    print(f"   DBSCAN:   {dbscan_speed:,.0f} bbox/ç§’")
    
    if dbscan_speed > grid_speed * 0.8:
        print("   ğŸ¯ DBSCANæ€§èƒ½å¯æ¥å—ï¼Œå¯è€ƒè™‘ç»“åˆä½¿ç”¨")
    else:
        print("   âš¡ ç½‘æ ¼æ–¹æ³•æ˜æ˜¾æ›´å¿«ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨")
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='PostgreSQLèšç±»æ€§èƒ½åŸºå‡†æµ‹è¯•')
    parser.add_argument('--city', default='A263', help='æµ‹è¯•åŸå¸‚ID')
    
    args = parser.parse_args()
    
    try:
        results = run_clustering_benchmark(args.city)
        
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°å†…å­˜ï¼Œå¯ç”¨äºè¿›ä¸€æ­¥åˆ†æ")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main())
