#!/usr/bin/env python3
"""
BBoxå ç½®åˆ†æç¤ºä¾‹
===============

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•å¯¹bboxåˆ†è¡¨æ•°æ®è¿›è¡Œç©ºé—´å ç½®åˆ†æï¼Œæ‰¾å‡ºé‡å æ•°é‡æœ€é«˜çš„ä½ç½®ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
- åŸºäºç»Ÿä¸€è§†å›¾çš„ç©ºé—´å ç½®åˆ†æ
- é‡å çƒ­ç‚¹è¯†åˆ«å’Œæ’åº
- QGISå…¼å®¹çš„ç»“æœå¯¼å‡º
- æ”¯æŒå¤šç§è¿‡æ»¤å’Œåˆ†ç»„æ¡ä»¶

å·¥ä½œæµç¨‹ï¼š
1. ç¡®ä¿bboxç»Ÿä¸€è§†å›¾å­˜åœ¨
2. æ‰§è¡Œç©ºé—´å ç½®åˆ†æSQL
3. ä¿å­˜ç»“æœåˆ°åˆ†æç»“æœè¡¨
4. åˆ›å»ºQGISå…¼å®¹è§†å›¾
5. æä¾›å¯è§†åŒ–æŒ‡å¯¼

ä½¿ç”¨æ–¹æ³•ï¼š
    python examples/dataset/bbox_examples/bbox_overlap_analysis.py
    
    # æˆ–æŒ‡å®šå‚æ•°
    python examples/dataset/bbox_examples/bbox_overlap_analysis.py --city beijing --min-overlap-area 0.0001
"""

import sys
import os
from pathlib import Path
import argparse
from datetime import datetime
import pandas as pd
import logging
from typing import Optional, List, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„ï¼Œæ”¯æŒå¤šç§ç¯å¢ƒ
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å°è¯•ç›´æ¥å¯¼å…¥ï¼Œå¦‚æœå¤±è´¥åˆ™æ·»åŠ srcè·¯å¾„
try:
    from spdatalab.dataset.bbox import (
        create_qgis_compatible_unified_view,
        list_bbox_tables,
        LOCAL_DSN
    )
except ImportError:
    # å¦‚æœç›´æ¥å¯¼å…¥å¤±è´¥ï¼Œå°è¯•æ·»åŠ srcè·¯å¾„
    sys.path.insert(0, str(project_root / "src"))
    from spdatalab.dataset.bbox import (
        create_qgis_compatible_unified_view,
        list_bbox_tables,
        LOCAL_DSN
    )

from sqlalchemy import create_engine, text

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BBoxOverlapAnalyzer:
    """BBoxå ç½®åˆ†æå™¨"""
    
    def __init__(self, dsn: str = LOCAL_DSN):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.dsn = dsn
        self.engine = create_engine(dsn, future=True)
        self.analysis_table = "bbox_overlap_analysis_results"
        self.unified_view = "clips_bbox_unified_qgis"
        self.qgis_view = "qgis_bbox_overlap_hotspots"
        
    def ensure_unified_view(self, force_refresh: bool = False) -> bool:
        """ç¡®ä¿ç»Ÿä¸€è§†å›¾å­˜åœ¨å¹¶ä¸”æ˜¯æœ€æ–°çš„
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°è§†å›¾
        """
        print("ğŸ” æ£€æŸ¥bboxç»Ÿä¸€è§†å›¾...")
        
        try:
            with self.engine.connect() as conn:
                # 1. æ£€æŸ¥è§†å›¾æ˜¯å¦å­˜åœ¨
                check_view_sql = text(f"""
                    SELECT EXISTS (
                        SELECT FROM information_schema.views 
                        WHERE table_schema = 'public' 
                        AND table_name = '{self.unified_view}'
                    );
                """)
                
                result = conn.execute(check_view_sql)
                view_exists = result.scalar()
                
                # 2. è·å–å½“å‰åˆ†è¡¨æ•°é‡
                current_tables = list_bbox_tables(self.engine)
                bbox_partition_tables = [t for t in current_tables if t.startswith('clips_bbox_') and t != 'clips_bbox']
                current_table_count = len(bbox_partition_tables)
                
                print(f"ğŸ“‹ å‘ç° {current_table_count} ä¸ªbboxåˆ†è¡¨")
                
                # 3. æ£€æŸ¥è§†å›¾æ˜¯å¦éœ€è¦æ›´æ–°
                view_needs_update = False
                
                if not view_exists:
                    print(f"ğŸ“Œ è§†å›¾ {self.unified_view} ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»º")
                    view_needs_update = True
                elif force_refresh:
                    print(f"ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œå°†é‡æ–°åˆ›å»ºè§†å›¾")
                    view_needs_update = True
                elif current_table_count == 0:
                    print(f"âš ï¸ æ²¡æœ‰å‘ç°bboxåˆ†è¡¨ï¼Œæ— æ³•åˆ›å»ºç»Ÿä¸€è§†å›¾")
                    return False
                else:
                    # æ£€æŸ¥è§†å›¾çš„è¡¨æ•°é‡æ˜¯å¦åŒ¹é…å½“å‰åˆ†è¡¨æ•°é‡
                    try:
                        # å°è¯•ä»è§†å›¾å®šä¹‰ä¸­è·å–è¡¨æ•°é‡ï¼ˆç®€åŒ–æ£€æŸ¥ï¼‰
                        check_count_sql = text(f"SELECT COUNT(DISTINCT source_table) FROM {self.unified_view} LIMIT 1;")
                        view_table_result = conn.execute(check_count_sql)
                        view_table_count = view_table_result.scalar()
                        
                        if view_table_count != current_table_count:
                            print(f"ğŸ”„ è§†å›¾åŒ…å« {view_table_count} ä¸ªè¡¨ï¼Œå½“å‰æœ‰ {current_table_count} ä¸ªåˆ†è¡¨ï¼Œéœ€è¦æ›´æ–°")
                            view_needs_update = True
                    except Exception as e:
                        print(f"âš ï¸ æ£€æŸ¥è§†å›¾çŠ¶æ€å¤±è´¥: {str(e)[:100]}...")
                        print(f"ğŸ”„ ä¸ºå®‰å…¨èµ·è§ï¼Œå°†é‡æ–°åˆ›å»ºè§†å›¾")
                        view_needs_update = True
                
                # 4. åˆ›å»ºæˆ–æ›´æ–°è§†å›¾
                if view_needs_update:
                    if current_table_count == 0:
                        print(f"âŒ æ— æ³•åˆ›å»ºè§†å›¾ï¼šæ²¡æœ‰å¯ç”¨çš„bboxåˆ†è¡¨")
                        return False
                    
                    print(f"ğŸ› ï¸ æ­£åœ¨åˆ›å»º/æ›´æ–°ç»Ÿä¸€è§†å›¾...")
                    success = create_qgis_compatible_unified_view(self.engine, self.unified_view)
                    if not success:
                        print("âŒ åˆ›å»ºç»Ÿä¸€è§†å›¾å¤±è´¥")
                        return False
                    print(f"âœ… ç»Ÿä¸€è§†å›¾ {self.unified_view} åˆ›å»º/æ›´æ–°æˆåŠŸ")
                else:
                    print(f"âœ… ç»Ÿä¸€è§†å›¾ {self.unified_view} å·²æ˜¯æœ€æ–°çŠ¶æ€")
                
                # 5. éªŒè¯è§†å›¾æ•°æ®
                try:
                    count_sql = text(f"SELECT COUNT(*) FROM {self.unified_view};")
                    count_result = conn.execute(count_sql)
                    row_count = count_result.scalar()
                    print(f"ğŸ“Š ç»Ÿä¸€è§†å›¾åŒ…å« {row_count:,} æ¡bboxè®°å½•")
                    
                    if row_count == 0:
                        print(f"âš ï¸ ç»Ÿä¸€è§†å›¾ä¸ºç©ºï¼Œå¯èƒ½åˆ†è¡¨ä¸­æ²¡æœ‰æ•°æ®")
                        return False
                    
                    # æ˜¾ç¤ºæ•°æ®åˆ†å¸ƒæ¦‚å†µ
                    sample_sql = text(f"""
                        SELECT 
                            COUNT(DISTINCT subdataset_name) as subdataset_count,
                            COUNT(DISTINCT city_id) as city_count,
                            COUNT(*) FILTER (WHERE all_good = true) as good_quality_count,
                            COUNT(*) FILTER (WHERE all_good = false OR all_good IS NULL) as poor_quality_count,
                            ROUND(100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*), 2) as good_quality_percent,
                            MIN(created_at) as earliest_data,
                            MAX(created_at) as latest_data
                        FROM {self.unified_view} 
                        WHERE city_id IS NOT NULL;
                    """)
                    sample_result = conn.execute(sample_sql).fetchone()
                    if sample_result:
                        print(f"ğŸ“ˆ æ•°æ®æ¦‚å†µ: {sample_result.subdataset_count} ä¸ªå­æ•°æ®é›†, {sample_result.city_count} ä¸ªåŸå¸‚")
                        print(f"ğŸ“Š è´¨é‡åˆ†å¸ƒ: {sample_result.good_quality_count:,} åˆæ ¼ ({sample_result.good_quality_percent}%), {sample_result.poor_quality_count:,} ä¸åˆæ ¼")
                        
                        # æ˜¾ç¤ºæŒ‰åŸå¸‚çš„è´¨é‡åˆ†å¸ƒ
                        city_quality_sql = text(f"""
                            SELECT 
                                city_id,
                                COUNT(*) as total_count,
                                COUNT(*) FILTER (WHERE all_good = true) as good_count,
                                ROUND(100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*), 1) as good_percent
                            FROM {self.unified_view} 
                            WHERE city_id IS NOT NULL
                            GROUP BY city_id
                            ORDER BY total_count DESC
                            LIMIT 5;
                        """)
                        city_results = conn.execute(city_quality_sql).fetchall()
                        if city_results:
                            print(f"ğŸ™ï¸ TOP 5åŸå¸‚è´¨é‡åˆ†å¸ƒ:")
                            for city_result in city_results:
                                print(f"   {city_result.city_id}: {city_result.good_count:,}/{city_result.total_count:,} ({city_result.good_percent}%)")
                            print(f"ğŸ’¡ åªæœ‰all_good=trueçš„æ•°æ®ä¼šå‚ä¸å ç½®åˆ†æ")
                    
                    return True
                    
                except Exception as e:
                    print(f"âš ï¸ è§†å›¾æ•°æ®éªŒè¯å¤±è´¥: {str(e)[:100]}...")
                    return False
                
        except Exception as e:
            print(f"âŒ æ£€æŸ¥ç»Ÿä¸€è§†å›¾å¤±è´¥: {str(e)}")
            return False
    
    def create_analysis_table(self) -> bool:
        """åˆ›å»ºåˆ†æç»“æœè¡¨"""
        print("ğŸ› ï¸ åˆ›å»ºåˆ†æç»“æœè¡¨...")
        
        # è¯»å–åˆ›å»ºè¡¨çš„SQLè„šæœ¬
        sql_file = Path(__file__).parent / "sql" / "create_analysis_tables.sql"
        
        try:
            if sql_file.exists():
                # å¦‚æœSQLæ–‡ä»¶å­˜åœ¨ï¼Œä½¿ç”¨æ–‡ä»¶ä¸­çš„SQL
                with open(sql_file, 'r', encoding='utf-8') as f:
                    create_sql = f.read()
            else:
                # å¦åˆ™ä½¿ç”¨å†…ç½®çš„SQL
                create_sql = f"""
                -- åˆ›å»ºbboxå ç½®åˆ†æç»“æœè¡¨
                CREATE TABLE IF NOT EXISTS {self.analysis_table} (
                    id SERIAL PRIMARY KEY,
                    analysis_id VARCHAR(100) NOT NULL,
                    analysis_type VARCHAR(50) DEFAULT 'bbox_overlap',
                    analysis_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    hotspot_rank INTEGER,
                    overlap_count INTEGER,
                    total_overlap_area NUMERIC,
                    subdataset_count INTEGER,
                    scene_count INTEGER,
                    involved_subdatasets TEXT[],
                    involved_scenes TEXT[],
                    analysis_params TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );

                -- æ·»åŠ PostGISå‡ ä½•åˆ—
                DO $$
                BEGIN
                    -- æ£€æŸ¥å‡ ä½•åˆ—æ˜¯å¦å·²å­˜åœ¨
                    IF NOT EXISTS (
                        SELECT 1 FROM information_schema.columns 
                        WHERE table_name = '{self.analysis_table}' 
                        AND column_name = 'geometry'
                    ) THEN
                        PERFORM AddGeometryColumn('public', '{self.analysis_table}', 'geometry', 4326, 'GEOMETRY', 2);
                    END IF;
                END $$;

                -- åˆ›å»ºç´¢å¼•
                CREATE INDEX IF NOT EXISTS idx_bbox_overlap_analysis_id ON {self.analysis_table} (analysis_id);
                CREATE INDEX IF NOT EXISTS idx_bbox_overlap_rank ON {self.analysis_table} (hotspot_rank);
                CREATE INDEX IF NOT EXISTS idx_bbox_overlap_count ON {self.analysis_table} (overlap_count);
                CREATE INDEX IF NOT EXISTS idx_bbox_overlap_geom ON {self.analysis_table} USING GIST (geometry);
                """
            
            with self.engine.connect() as conn:
                conn.execute(text(create_sql))
                conn.commit()
                
            print(f"âœ… åˆ†æç»“æœè¡¨ {self.analysis_table} åˆ›å»ºæˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºåˆ†æç»“æœè¡¨å¤±è´¥: {str(e)}")
            return False
    
    def run_overlap_analysis(
        self, 
        analysis_id: Optional[str] = None,
        city_filter: Optional[str] = None,
        subdataset_filter: Optional[List[str]] = None,
        min_overlap_area: float = 0.0,
        top_n: int = 20
    ) -> str:
        """æ‰§è¡Œå ç½®åˆ†æ
        
        Args:
            analysis_id: åˆ†æIDï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            city_filter: åŸå¸‚è¿‡æ»¤
            subdataset_filter: å­æ•°æ®é›†è¿‡æ»¤
            min_overlap_area: æœ€å°é‡å é¢ç§¯é˜ˆå€¼
            top_n: è¿”å›çš„çƒ­ç‚¹æ•°é‡
            
        Returns:
            åˆ†æID
        """
        if not analysis_id:
            analysis_id = f"bbox_overlap_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print(f"ğŸš€ å¼€å§‹å ç½®åˆ†æ: {analysis_id}")
        print(f"å‚æ•°: city_filter={city_filter}, min_overlap_area={min_overlap_area}, top_n={top_n}")
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where_conditions = []
        
        # ğŸ¯ åŸå¸‚è¿‡æ»¤ï¼ˆæ³¨æ„ï¼šç°åœ¨åŸºç¡€WHEREæ¡ä»¶å·²åŒ…å«a.city_id = b.city_idï¼‰
        if city_filter:
            # åŸå¸‚è¿‡æ»¤åªéœ€è¦é™åˆ¶å…¶ä¸­ä¸€ä¸ªè¡¨å³å¯ï¼Œå› ä¸ºå·²ç»æœ‰ç›¸åŒåŸå¸‚çº¦æŸ
            where_conditions.append(f"a.city_id = '{city_filter}'")
            print(f"ğŸ™ï¸ åŸå¸‚è¿‡æ»¤: {city_filter}")
        
        if subdataset_filter:
            subdataset_list = "', '".join(subdataset_filter)
            where_conditions.append(f"a.subdataset_name IN ('{subdataset_list}') AND b.subdataset_name IN ('{subdataset_list}')")
            print(f"ğŸ“¦ å­æ•°æ®é›†è¿‡æ»¤: {len(subdataset_filter)} ä¸ª")
        
        where_clause = "AND " + " AND ".join(where_conditions) if where_conditions else ""
        
        # è¯»å–åˆ†æSQLè„šæœ¬
        sql_file = Path(__file__).parent / "sql" / "overlap_analysis.sql"
        
        try:
            if sql_file.exists():
                # å¦‚æœSQLæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–å¹¶æ›¿æ¢å‚æ•°
                with open(sql_file, 'r', encoding='utf-8') as f:
                    analysis_sql_template = f.read()
                
                # æ›¿æ¢å‚æ•°
                analysis_sql = analysis_sql_template.format(
                    unified_view=self.unified_view,
                    analysis_table=self.analysis_table,
                    analysis_id=analysis_id,
                    where_clause=where_clause,
                    min_overlap_area=min_overlap_area,
                    top_n=top_n
                )
            else:
                # å†…ç½®SQL
                analysis_sql = f"""
                WITH overlapping_areas AS (
                    SELECT 
                        a.qgis_id as bbox_a_id,
                        b.qgis_id as bbox_b_id,
                        a.subdataset_name as subdataset_a,
                        b.subdataset_name as subdataset_b,
                        a.scene_token as scene_a,
                        b.scene_token as scene_b,
                        ST_Intersection(a.geometry, b.geometry) as overlap_geometry,
                        ST_Area(ST_Intersection(a.geometry, b.geometry)) as overlap_area
                    FROM {self.unified_view} a
                    JOIN {self.unified_view} b ON a.qgis_id < b.qgis_id
                    WHERE ST_Intersects(a.geometry, b.geometry)
                    AND ST_Area(ST_Intersection(a.geometry, b.geometry)) > {min_overlap_area}
                    AND NOT ST_Equals(a.geometry, b.geometry)
                    -- ğŸ¯ åªåˆ†æç›¸åŒåŸå¸‚çš„bbox
                    AND a.city_id = b.city_id
                    AND a.city_id IS NOT NULL
                    -- ğŸ¯ åªåˆ†æè´¨é‡åˆæ ¼çš„æ•°æ®
                    AND a.all_good = true
                    AND b.all_good = true
                    {where_clause}
                ),
                overlap_hotspots AS (
                    SELECT 
                        ST_Union(overlap_geometry) as hotspot_geometry,
                        COUNT(*) as overlap_count,
                        ARRAY_AGG(DISTINCT subdataset_a) || ARRAY_AGG(DISTINCT subdataset_b) as involved_subdatasets,
                        ARRAY_AGG(DISTINCT scene_a) || ARRAY_AGG(DISTINCT scene_b) as involved_scenes,
                        SUM(overlap_area) as total_overlap_area
                    FROM overlapping_areas
                    GROUP BY ST_SnapToGrid(overlap_geometry, 0.001)
                    HAVING COUNT(*) >= 2
                )
                INSERT INTO {self.analysis_table} 
                (analysis_id, hotspot_rank, overlap_count, total_overlap_area, 
                 subdataset_count, scene_count, involved_subdatasets, involved_scenes, geometry, analysis_params)
                SELECT 
                    '{analysis_id}' as analysis_id,
                    ROW_NUMBER() OVER (ORDER BY overlap_count DESC) as hotspot_rank,
                    overlap_count,
                    total_overlap_area,
                    ARRAY_LENGTH(involved_subdatasets, 1) as subdataset_count,
                    ARRAY_LENGTH(involved_scenes, 1) as scene_count,
                    involved_subdatasets,
                    involved_scenes,
                    hotspot_geometry as geometry,
                    '{{"city_filter": "{city_filter}", "min_overlap_area": {min_overlap_area}, "top_n": {top_n}}}' as analysis_params
                FROM overlap_hotspots
                ORDER BY overlap_count DESC
                LIMIT {top_n};
                """
            
            with self.engine.connect() as conn:
                result = conn.execute(text(analysis_sql))
                conn.commit()
                
                # è·å–æ’å…¥çš„è®°å½•æ•°
                count_sql = text(f"SELECT COUNT(*) FROM {self.analysis_table} WHERE analysis_id = '{analysis_id}';")
                count_result = conn.execute(count_sql)
                inserted_count = count_result.scalar()
                
            print(f"âœ… å ç½®åˆ†æå®Œæˆï¼Œå‘ç° {inserted_count} ä¸ªé‡å çƒ­ç‚¹")
            return analysis_id
            
        except Exception as e:
            print(f"âŒ å ç½®åˆ†æå¤±è´¥: {str(e)}")
            raise
    
    def create_qgis_view(self, analysis_id: Optional[str] = None) -> bool:
        """åˆ›å»ºQGISå…¼å®¹è§†å›¾"""
        print("ğŸ¨ åˆ›å»ºQGISå…¼å®¹è§†å›¾...")
        
        # è¯»å–è§†å›¾åˆ›å»ºSQL
        sql_file = Path(__file__).parent / "sql" / "qgis_views.sql"
        
        try:
            if sql_file.exists():
                with open(sql_file, 'r', encoding='utf-8') as f:
                    view_sql = f.read()
                    view_sql = view_sql.format(
                        qgis_view=self.qgis_view,
                        analysis_table=self.analysis_table
                    )
            else:
                # å†…ç½®SQL
                where_clause = f"WHERE analysis_id = '{analysis_id}'" if analysis_id else ""
                
                view_sql = f"""
                CREATE OR REPLACE VIEW {self.qgis_view} AS
                SELECT 
                    id as qgis_id,
                    analysis_id,
                    hotspot_rank,
                    overlap_count,
                    total_overlap_area,
                    subdataset_count,
                    scene_count,
                    involved_subdatasets,
                    involved_scenes,
                    CASE 
                        WHEN overlap_count >= 10 THEN 'High Density'
                        WHEN overlap_count >= 5 THEN 'Medium Density'
                        ELSE 'Low Density'
                    END as density_level,
                    geometry,
                    created_at
                FROM {self.analysis_table}
                WHERE analysis_type = 'bbox_overlap'
                {where_clause}
                ORDER BY hotspot_rank;
                """
            
            with self.engine.connect() as conn:
                conn.execute(text(view_sql))
                conn.commit()
                
            print(f"âœ… QGISè§†å›¾ {self.qgis_view} åˆ›å»ºæˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºQGISè§†å›¾å¤±è´¥: {str(e)}")
            return False
    
    def get_city_analysis_suggestions(self) -> pd.DataFrame:
        """è·å–åŸå¸‚åˆ†æå»ºè®®ï¼Œå¸®åŠ©ç”¨æˆ·é€‰æ‹©åˆé€‚çš„åŸå¸‚"""
        print("ğŸ” åˆ†æå„åŸå¸‚çš„æ•°æ®åˆ†å¸ƒï¼Œç”Ÿæˆåˆ†æå»ºè®®...")
        
        sql = text(f"""
            WITH city_stats AS (
                SELECT 
                    city_id,
                    COUNT(*) as total_bbox_count,
                    COUNT(*) FILTER (WHERE all_good = true) as good_bbox_count,
                    COUNT(DISTINCT subdataset_name) as subdataset_count,
                    ROUND(100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*), 1) as good_percent,
                    -- ä¼°ç®—å¯èƒ½çš„é‡å å¯¹æ•°é‡ï¼ˆåŸºäºæ•°æ®å¯†åº¦ï¼‰
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 10000 THEN 'High'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 1000 THEN 'Medium' 
                        ELSE 'Low'
                    END as analysis_complexity,
                    -- é¢„ä¼°åˆ†ææ—¶é—´ï¼ˆåŸºäºæ•°æ®é‡ï¼‰
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 50000 THEN '> 10åˆ†é’Ÿ'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 10000 THEN '2-10åˆ†é’Ÿ'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 1000 THEN '< 2åˆ†é’Ÿ'
                        ELSE '< 30ç§’'
                    END as estimated_time
                FROM {self.unified_view}
                WHERE city_id IS NOT NULL
                GROUP BY city_id
                HAVING COUNT(*) FILTER (WHERE all_good = true) > 0
            )
            SELECT 
                city_id,
                total_bbox_count,
                good_bbox_count,
                subdataset_count,
                good_percent,
                analysis_complexity,
                estimated_time,
                -- æ¨èåº¦è¯„åˆ†
                CASE 
                    WHEN good_bbox_count BETWEEN 1000 AND 20000 AND good_percent > 90 THEN 'â­â­â­ æ¨è'
                    WHEN good_bbox_count BETWEEN 500 AND 50000 AND good_percent > 85 THEN 'â­â­ è¾ƒå¥½'
                    WHEN good_bbox_count > 100 THEN 'â­ å¯ç”¨'
                    ELSE 'âŒ ä¸å»ºè®®'
                END as recommendation
            FROM city_stats
            ORDER BY 
                CASE 
                    WHEN good_bbox_count BETWEEN 1000 AND 20000 AND good_percent > 90 THEN 1
                    WHEN good_bbox_count BETWEEN 500 AND 50000 AND good_percent > 85 THEN 2
                    WHEN good_bbox_count > 100 THEN 3
                    ELSE 4
                END,
                good_bbox_count DESC;
        """)
        
        try:
            result_df = pd.read_sql(sql, self.engine)
            
            if not result_df.empty:
                print(f"\nğŸ“Š åŸå¸‚åˆ†æå»ºè®®è¡¨:")
                print(result_df.to_string(index=False))
                
                # æä¾›å…·ä½“å»ºè®®
                recommended = result_df[result_df['recommendation'].str.contains('â­â­â­')]
                if not recommended.empty:
                    best_city = recommended.iloc[0]['city_id']
                    print(f"\nğŸ’¡ æ¨èåŸå¸‚: {best_city}")
                    print(f"   - æ•°æ®é‡é€‚ä¸­ï¼Œè´¨é‡è¾ƒé«˜")
                    print(f"   - é¢„ä¼°åˆ†ææ—¶é—´: {recommended.iloc[0]['estimated_time']}")
                    print(f"   - å»ºè®®å‘½ä»¤: --city {best_city}")
                
                return result_df
            else:
                print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„åŸå¸‚æ•°æ®")
                return pd.DataFrame()
                
        except Exception as e:
            print(f"âŒ è·å–åŸå¸‚å»ºè®®å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def estimate_analysis_time(self, city_filter: str = None) -> dict:
        """ä¼°ç®—åˆ†ææ—¶é—´å’Œæ•°æ®é‡"""
        print("â±ï¸ ä¼°ç®—åˆ†ææ—¶é—´...")
        
        where_condition = f"WHERE city_id = '{city_filter}'" if city_filter else "WHERE city_id IS NOT NULL"
        
        sql = text(f"""
            SELECT 
                COUNT(*) FILTER (WHERE all_good = true) as analyzable_count,
                -- ä¼°ç®—å¯èƒ½çš„é‡å å¯¹æ•°é‡ï¼ˆn*(n-1)/2çš„ç®€åŒ–ä¼°ç®—ï¼‰
                CASE 
                    WHEN COUNT(*) FILTER (WHERE all_good = true) > 0 THEN
                        LEAST(
                            COUNT(*) FILTER (WHERE all_good = true) * (COUNT(*) FILTER (WHERE all_good = true) - 1) / 2,
                            1000000  -- é™åˆ¶æœ€å¤§ä¼°ç®—æ•°
                        )
                    ELSE 0
                END as estimated_pairs,
                CASE 
                    WHEN COUNT(*) FILTER (WHERE all_good = true) > 100000 THEN 'âš ï¸ å¾ˆé•¿ (>30åˆ†é’Ÿ)'
                    WHEN COUNT(*) FILTER (WHERE all_good = true) > 50000 THEN 'â³ è¾ƒé•¿ (10-30åˆ†é’Ÿ)'
                    WHEN COUNT(*) FILTER (WHERE all_good = true) > 10000 THEN 'â° ä¸­ç­‰ (2-10åˆ†é’Ÿ)'
                    WHEN COUNT(*) FILTER (WHERE all_good = true) > 1000 THEN 'âš¡ è¾ƒå¿« (<2åˆ†é’Ÿ)'
                    ELSE 'ğŸš€ å¾ˆå¿« (<30ç§’)'
                END as time_estimate,
                {f"'{city_filter}'" if city_filter else "'å…¨éƒ¨åŸå¸‚'"} as scope
            FROM {self.unified_view}
            {where_condition};
        """)
        
        try:
            result = self.engine.execute(sql).fetchone()
            
            estimate = {
                'analyzable_count': result.analyzable_count,
                'estimated_pairs': result.estimated_pairs,
                'time_estimate': result.time_estimate,
                'scope': result.scope
            }
            
            print(f"ğŸ“Š åˆ†æèŒƒå›´: {estimate['scope']}")
            print(f"ğŸ“ˆ å¯åˆ†ææ•°æ®: {estimate['analyzable_count']:,} ä¸ªbbox")
            print(f"ğŸ”— é¢„ä¼°é…å¯¹æ•°: {estimate['estimated_pairs']:,}")
            print(f"â±ï¸ é¢„ä¼°æ—¶é—´: {estimate['time_estimate']}")
            
            if estimate['analyzable_count'] > 50000:
                print(f"ğŸ’¡ å»ºè®®: æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®æŒ‡å®šå…·ä½“åŸå¸‚è¿›è¡Œåˆ†æ")
                print(f"ğŸ’¡ å‘½ä»¤: --city your_city_name")
            
            return estimate
            
        except Exception as e:
            print(f"âŒ æ—¶é—´ä¼°ç®—å¤±è´¥: {str(e)}")
            return {}

    def get_analysis_summary(self, analysis_id: str) -> pd.DataFrame:
        """è·å–åˆ†æç»“æœæ‘˜è¦"""
        sql = text(f"""
            SELECT 
                hotspot_rank,
                overlap_count,
                ROUND(total_overlap_area::numeric, 4) as total_overlap_area,
                subdataset_count,
                scene_count,
                involved_subdatasets,
                density_level
            FROM {self.qgis_view}
            WHERE analysis_id = :analysis_id
            ORDER BY hotspot_rank
            LIMIT 10;
        """)
        
        return pd.read_sql(sql, self.engine, params={'analysis_id': analysis_id})
    
    def export_for_qgis(self, analysis_id: str) -> Dict[str, Any]:
        """å¯¼å‡ºQGISå¯è§†åŒ–ä¿¡æ¯"""
        return {
            'analysis_id': analysis_id,
            'qgis_view': self.qgis_view,
            'unified_view': self.unified_view,
            'connection_info': {
                'host': 'local_pg',
                'port': 5432,
                'database': 'postgres',
                'username': 'postgres'
            },
            'visualization_tips': {
                'primary_key': 'qgis_id',
                'geometry_column': 'geometry',
                'style_column': 'density_level',
                'label_column': 'overlap_count',
                'filter_column': 'analysis_id'
            }
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='BBoxå ç½®åˆ†æ')
    parser.add_argument('--city', help='åŸå¸‚è¿‡æ»¤')
    parser.add_argument('--subdatasets', nargs='+', help='å­æ•°æ®é›†è¿‡æ»¤')
    parser.add_argument('--min-overlap-area', type=float, default=0.0, help='æœ€å°é‡å é¢ç§¯é˜ˆå€¼')
    parser.add_argument('--top-n', type=int, default=20, help='è¿”å›çš„çƒ­ç‚¹æ•°é‡')
    parser.add_argument('--analysis-id', help='è‡ªå®šä¹‰åˆ†æID')
    parser.add_argument('--refresh-view', action='store_true', help='å¼ºåˆ¶åˆ·æ–°ç»Ÿä¸€è§†å›¾ï¼ˆé€‚ç”¨äºæ•°æ®æ›´æ–°åï¼‰')
    parser.add_argument('--suggest-city', action='store_true', help='æ˜¾ç¤ºåŸå¸‚åˆ†æå»ºè®®å¹¶é€€å‡º')
    parser.add_argument('--estimate-time', action='store_true', help='ä¼°ç®—åˆ†ææ—¶é—´å¹¶é€€å‡º')
    
    args = parser.parse_args()
    
    print("ğŸ¯ BBoxå ç½®åˆ†æç¤ºä¾‹")
    print("=" * 60)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = BBoxOverlapAnalyzer()
    
    try:
        # 1. ç¡®ä¿ç»Ÿä¸€è§†å›¾å­˜åœ¨
        print("\nğŸ“‹ æ­¥éª¤1: æ£€æŸ¥æ•°æ®å‡†å¤‡")
        # å¯¹äºå¤§é‡æ•°æ®çš„æƒ…å†µï¼Œæˆ‘ä»¬ä¼˜å…ˆä½¿ç”¨ç°æœ‰è§†å›¾ï¼Œåªåœ¨å¿…è¦æ—¶åˆ·æ–°
        force_refresh = args.refresh_view
        if not analyzer.ensure_unified_view(force_refresh=force_refresh):
            print("âŒ ç»Ÿä¸€è§†å›¾æ£€æŸ¥å¤±è´¥ï¼Œé€€å‡º")
            return
        
        # å¦‚æœç”¨æˆ·åªæƒ³æŸ¥çœ‹åŸå¸‚å»ºè®®
        if args.suggest_city:
            print("\nğŸ™ï¸ åŸå¸‚åˆ†æå»ºè®®")
            print("-" * 40)
            analyzer.get_city_analysis_suggestions()
            return
        
        # å¦‚æœç”¨æˆ·åªæƒ³ä¼°ç®—æ—¶é—´
        if args.estimate_time:
            print("\nâ±ï¸ åˆ†ææ—¶é—´ä¼°ç®—")
            print("-" * 40)
            analyzer.estimate_analysis_time(args.city)
            return
        
        # 2. åˆ›å»ºåˆ†æç»“æœè¡¨
        print("\nğŸ› ï¸ æ­¥éª¤2: å‡†å¤‡åˆ†æç¯å¢ƒ")
        if not analyzer.create_analysis_table():
            print("âŒ åˆ†æè¡¨åˆ›å»ºå¤±è´¥ï¼Œé€€å‡º")
            return
        
        # 3. åˆ†æå‰çš„æ—¶é—´ä¼°ç®—å’Œç¡®è®¤
        print("\nâ±ï¸ æ­¥éª¤3a: åˆ†æå‰ä¼°ç®—")
        print("-" * 40)
        estimate = analyzer.estimate_analysis_time(args.city)
        
        # å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œç»™å‡ºè­¦å‘Šå’Œå»ºè®®
        if estimate and estimate.get('analyzable_count', 0) > 50000:
            print(f"\nâš ï¸ æ•°æ®é‡è­¦å‘Š:")
            print(f"   å½“å‰åˆ†æèŒƒå›´åŒ…å« {estimate['analyzable_count']:,} ä¸ªbbox")
            print(f"   é¢„ä¼°åˆ†ææ—¶é—´: {estimate.get('time_estimate', 'æœªçŸ¥')}")
            print(f"   ğŸ’¡ å»ºè®®: ä½¿ç”¨ --city å‚æ•°ç¼©å°åˆ†æèŒƒå›´")
            print(f"   ğŸ’¡ è·å–åŸå¸‚å»ºè®®: --suggest-city")
            
            if not args.city:
                print(f"\nğŸ¤” æ˜¯å¦ç»§ç»­å…¨é‡åˆ†æï¼Ÿè¿™å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´...")
                print(f"ğŸ’¡ å»ºè®®å…ˆè¿è¡Œ: --suggest-city æŸ¥çœ‹æ¨èåŸå¸‚")
        
        # 3b. æ‰§è¡Œå ç½®åˆ†æ
        print(f"\nğŸš€ æ­¥éª¤3b: æ‰§è¡Œå ç½®åˆ†æ")
        print("-" * 40)
        analysis_id = analyzer.run_overlap_analysis(
            analysis_id=args.analysis_id,
            city_filter=args.city,
            subdataset_filter=args.subdatasets,
            min_overlap_area=args.min_overlap_area,
            top_n=args.top_n
        )
        
        # 4. åˆ›å»ºQGISè§†å›¾
        print("\nğŸ¨ æ­¥éª¤4: åˆ›å»ºQGISè§†å›¾")
        if not analyzer.create_qgis_view(analysis_id):
            print("âŒ QGISè§†å›¾åˆ›å»ºå¤±è´¥")
            return
        
        # 5. æ˜¾ç¤ºåˆ†æç»“æœæ‘˜è¦
        print("\nğŸ“Š æ­¥éª¤5: åˆ†æç»“æœæ‘˜è¦")
        summary = analyzer.get_analysis_summary(analysis_id)
        if not summary.empty:
            print("TOP 10 é‡å çƒ­ç‚¹:")
            print(summary.to_string(index=False))
        else:
            print("æœªå‘ç°é‡å çƒ­ç‚¹")
        
        # 6. æä¾›QGISå¯è§†åŒ–æŒ‡å¯¼
        print("\nğŸ¯ æ­¥éª¤6: QGISå¯è§†åŒ–æŒ‡å¯¼")
        qgis_info = analyzer.export_for_qgis(analysis_id)
        
        print("ğŸ“‹ æ•°æ®åº“è¿æ¥ä¿¡æ¯:")
        conn_info = qgis_info['connection_info']
        for key, value in conn_info.items():
            print(f"   {key}: {value}")
        
        print(f"\nğŸ“Š åœ¨QGISä¸­åŠ è½½ä»¥ä¸‹å›¾å±‚:")
        print(f"   1. {qgis_info['unified_view']} - æ‰€æœ‰bboxæ•°æ®ï¼ˆåº•å›¾ï¼‰")
        print(f"   2. {qgis_info['qgis_view']} - é‡å çƒ­ç‚¹åŒºåŸŸ")
        
        print(f"\nğŸ¨ å¯è§†åŒ–å»ºè®®:")
        vis_tips = qgis_info['visualization_tips']
        print(f"   â€¢ ä¸»é”®: {vis_tips['primary_key']}")
        print(f"   â€¢ å‡ ä½•åˆ—: {vis_tips['geometry_column']}")
        print(f"   â€¢ æŒ‰ {vis_tips['style_column']} å­—æ®µè®¾ç½®é¢œè‰²")
        print(f"   â€¢ æ˜¾ç¤º {vis_tips['label_column']} æ ‡ç­¾")
        print(f"   â€¢ ä½¿ç”¨ {vis_tips['filter_column']} = '{analysis_id}' è¿‡æ»¤")
        
        print(f"\nâœ… å ç½®åˆ†æå®Œæˆï¼åˆ†æID: {analysis_id}")
        print(f"ç°åœ¨å¯ä»¥åœ¨QGISä¸­è¿æ¥æ•°æ®åº“å¹¶åŠ è½½è¿™äº›å›¾å±‚è¿›è¡Œå¯è§†åŒ–åˆ†æã€‚")
        
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    main()
