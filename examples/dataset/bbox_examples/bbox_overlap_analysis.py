#!/usr/bin/env python3
"""
BBoxå ç½®åˆ†æç¤ºä¾‹
===============

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•å¯¹bboxåˆ†è¡¨æ•°æ®è¿›è¡Œç©ºé—´å ç½®åˆ†æï¼Œæ‰¾å‡ºé‡å æ•°é‡æœ€é«˜çš„ä½ç½®ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
- åŸºäºç»Ÿä¸€è§†å›¾çš„ç©ºé—´å ç½®åˆ†æ
- é‡å çƒ­ç‚¹è¯†åˆ«å’Œæ’åº
- QGISå…¼å®¹çš„ç»“æœå¯¼å‡º
- æ”¯æŒå¤šç§è¿‡æ»¤å’Œåˆ†ç»„æ¡ä»¶

å·¥ä½œæµç¨‹ï¼š
1. ç¡®ä¿bboxç»Ÿä¸€è§†å›¾å­˜åœ¨
2. æ‰§è¡Œç©ºé—´å ç½®åˆ†æSQL
3. ä¿å­˜ç»“æœåˆ°åˆ†æç»“æœè¡¨
4. åˆ›å»ºQGISå…¼å®¹è§†å›¾
5. æä¾›å¯è§†åŒ–æŒ‡å¯¼

ä½¿ç”¨æ–¹æ³•ï¼š
    python examples/dataset/bbox_examples/bbox_overlap_analysis.py
    
    # æˆ–æŒ‡å®šå‚æ•°
    python examples/dataset/bbox_examples/bbox_overlap_analysis.py --city beijing --min-overlap-area 0.0001
"""

import sys
import os
import signal
import atexit
from pathlib import Path
import argparse
from datetime import datetime
import pandas as pd
import logging
from typing import Optional, List, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„ï¼Œæ”¯æŒå¤šç§ç¯å¢ƒ
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# å°è¯•ç›´æ¥å¯¼å…¥ï¼Œå¦‚æœå¤±è´¥åˆ™æ·»åŠ srcè·¯å¾„
try:
    from spdatalab.dataset.bbox import (
        create_qgis_compatible_unified_view,
        list_bbox_tables,
        LOCAL_DSN
    )
except ImportError:
    # å¦‚æœç›´æ¥å¯¼å…¥å¤±è´¥ï¼Œå°è¯•æ·»åŠ srcè·¯å¾„
    sys.path.insert(0, str(project_root / "src"))
    from spdatalab.dataset.bbox import (
        create_qgis_compatible_unified_view,
        list_bbox_tables,
        LOCAL_DSN
    )

from sqlalchemy import create_engine, text

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class BBoxOverlapAnalyzer:
    """BBoxå ç½®åˆ†æå™¨"""
    
    def __init__(self, dsn: str = LOCAL_DSN):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.dsn = dsn
        self.engine = create_engine(dsn, future=True)
        self.analysis_table = "bbox_overlap_analysis_results"
        self.unified_view = "clips_bbox_unified_qgis"
        self.qgis_view = "qgis_bbox_overlap_hotspots"
        
        # ä¼˜é›…é€€å‡ºæ§åˆ¶
        self.shutdown_requested = False
        self.current_connection = None
        self.current_analysis_id = None
        self.analysis_start_time = None
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨å’Œæ¸…ç†å‡½æ•°
        self._setup_signal_handlers()
        atexit.register(self._cleanup_on_exit)
    
    def _setup_signal_handlers(self):
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
        def signal_handler(signum, frame):
            print(f"\n\nğŸ›‘ æ”¶åˆ°é€€å‡ºä¿¡å· ({signal.Signals(signum).name})")
            self._initiate_graceful_shutdown()
        
        # æ³¨å†Œå¸¸è§çš„é€€å‡ºä¿¡å·
        try:
            signal.signal(signal.SIGINT, signal_handler)   # Ctrl+C
            signal.signal(signal.SIGTERM, signal_handler)  # ç»ˆæ­¢ä¿¡å·
            if hasattr(signal, 'SIGBREAK'):  # Windows
                signal.signal(signal.SIGBREAK, signal_handler)
        except ValueError:
            # åœ¨æŸäº›ç¯å¢ƒä¸­å¯èƒ½æ— æ³•æ³¨å†Œä¿¡å·
            logger.warning("æ— æ³•æ³¨å†Œä¿¡å·å¤„ç†å™¨")
    
    def _initiate_graceful_shutdown(self):
        """å¯åŠ¨ä¼˜é›…é€€å‡ºæµç¨‹"""
        self.shutdown_requested = True
        print(f"ğŸ”„ æ­£åœ¨å®‰å…¨é€€å‡º...")
        
        if self.current_analysis_id:
            print(f"ğŸ“ å½“å‰åˆ†æID: {self.current_analysis_id}")
            
        if self.analysis_start_time:
            elapsed = datetime.now() - self.analysis_start_time
            print(f"â±ï¸ å·²è¿è¡Œæ—¶é—´: {elapsed}")
        
        print(f"ğŸ§¹ æ¸…ç†èµ„æºä¸­...")
        self._cleanup_resources()
        
        print(f"âœ… ä¼˜é›…é€€å‡ºå®Œæˆ")
        sys.exit(0)
    
    def _cleanup_resources(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.current_connection:
                self.current_connection.close()
                print(f"âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
                self.current_connection = None
        except Exception as e:
            print(f"âš ï¸ å…³é—­è¿æ¥æ—¶å‡ºé”™: {e}")
    
    def _cleanup_on_exit(self):
        """ç¨‹åºé€€å‡ºæ—¶çš„æ¸…ç†å‡½æ•°"""
        if not self.shutdown_requested:
            self._cleanup_resources()
    
    def _check_shutdown(self):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡º"""
        if self.shutdown_requested:
            print(f"ğŸ›‘ æ£€æµ‹åˆ°é€€å‡ºè¯·æ±‚ï¼Œåœæ­¢æ‰§è¡Œ")
            raise KeyboardInterrupt("ç”¨æˆ·è¯·æ±‚é€€å‡º")
        
    def ensure_unified_view(self, force_refresh: bool = False) -> bool:
        """ç¡®ä¿ç»Ÿä¸€è§†å›¾å­˜åœ¨å¹¶ä¸”æ˜¯æœ€æ–°çš„
        
        Args:
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°è§†å›¾
        """
        print("ğŸ” æ£€æŸ¥bboxç»Ÿä¸€è§†å›¾...")
        
        try:
            with self.engine.connect() as conn:
                # 1. æ£€æŸ¥è§†å›¾æ˜¯å¦å­˜åœ¨
                check_view_sql = text(f"""
                    SELECT EXISTS (
                        SELECT FROM information_schema.views 
                        WHERE table_schema = 'public' 
                        AND table_name = '{self.unified_view}'
                    );
                """)
                
                result = conn.execute(check_view_sql)
                view_exists = result.scalar()
                
                # 2. è·å–å½“å‰åˆ†è¡¨æ•°é‡
                current_tables = list_bbox_tables(self.engine)
                bbox_partition_tables = [t for t in current_tables if t.startswith('clips_bbox_') and t != 'clips_bbox']
                current_table_count = len(bbox_partition_tables)
                
                print(f"ğŸ“‹ å‘ç° {current_table_count} ä¸ªbboxåˆ†è¡¨")
                
                # 3. æ£€æŸ¥è§†å›¾æ˜¯å¦éœ€è¦æ›´æ–°
                view_needs_update = False
                
                if not view_exists:
                    print(f"ğŸ“Œ è§†å›¾ {self.unified_view} ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»º")
                    view_needs_update = True
                elif force_refresh:
                    print(f"ğŸ”„ å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œå°†é‡æ–°åˆ›å»ºè§†å›¾")
                    view_needs_update = True
                elif current_table_count == 0:
                    print(f"âš ï¸ æ²¡æœ‰å‘ç°bboxåˆ†è¡¨ï¼Œæ— æ³•åˆ›å»ºç»Ÿä¸€è§†å›¾")
                    return False
                else:
                    # æ£€æŸ¥è§†å›¾çš„è¡¨æ•°é‡æ˜¯å¦åŒ¹é…å½“å‰åˆ†è¡¨æ•°é‡
                    try:
                        # å°è¯•ä»è§†å›¾å®šä¹‰ä¸­è·å–è¡¨æ•°é‡ï¼ˆç®€åŒ–æ£€æŸ¥ï¼‰
                        check_count_sql = text(f"SELECT COUNT(DISTINCT source_table) FROM {self.unified_view} LIMIT 1;")
                        view_table_result = conn.execute(check_count_sql)
                        view_table_count = view_table_result.scalar()
                        
                        if view_table_count != current_table_count:
                            print(f"ğŸ”„ è§†å›¾åŒ…å« {view_table_count} ä¸ªè¡¨ï¼Œå½“å‰æœ‰ {current_table_count} ä¸ªåˆ†è¡¨ï¼Œéœ€è¦æ›´æ–°")
                            view_needs_update = True
                    except Exception as e:
                        print(f"âš ï¸ æ£€æŸ¥è§†å›¾çŠ¶æ€å¤±è´¥: {str(e)[:100]}...")
                        print(f"ğŸ”„ ä¸ºå®‰å…¨èµ·è§ï¼Œå°†é‡æ–°åˆ›å»ºè§†å›¾")
                        view_needs_update = True
                
                # 4. åˆ›å»ºæˆ–æ›´æ–°è§†å›¾
                if view_needs_update:
                    if current_table_count == 0:
                        print(f"âŒ æ— æ³•åˆ›å»ºè§†å›¾ï¼šæ²¡æœ‰å¯ç”¨çš„bboxåˆ†è¡¨")
                        return False
                    
                    print(f"ğŸ› ï¸ æ­£åœ¨åˆ›å»º/æ›´æ–°ç»Ÿä¸€è§†å›¾...")
                    success = create_qgis_compatible_unified_view(self.engine, self.unified_view)
                    if not success:
                        print("âŒ åˆ›å»ºç»Ÿä¸€è§†å›¾å¤±è´¥")
                        return False
                    print(f"âœ… ç»Ÿä¸€è§†å›¾ {self.unified_view} åˆ›å»º/æ›´æ–°æˆåŠŸ")
                else:
                    print(f"âœ… ç»Ÿä¸€è§†å›¾ {self.unified_view} å·²æ˜¯æœ€æ–°çŠ¶æ€")
                
                # 5. éªŒè¯è§†å›¾æ•°æ®
                try:
                    count_sql = text(f"SELECT COUNT(*) FROM {self.unified_view};")
                    count_result = conn.execute(count_sql)
                    row_count = count_result.scalar()
                    print(f"ğŸ“Š ç»Ÿä¸€è§†å›¾åŒ…å« {row_count:,} æ¡bboxè®°å½•")
                    
                    if row_count == 0:
                        print(f"âš ï¸ ç»Ÿä¸€è§†å›¾ä¸ºç©ºï¼Œå¯èƒ½åˆ†è¡¨ä¸­æ²¡æœ‰æ•°æ®")
                        return False
                    
                    # æ˜¾ç¤ºæ•°æ®åˆ†å¸ƒæ¦‚å†µ
                    sample_sql = text(f"""
                        SELECT 
                            COUNT(DISTINCT subdataset_name) as subdataset_count,
                            COUNT(DISTINCT city_id) as city_count,
                            COUNT(*) FILTER (WHERE all_good = true) as good_quality_count,
                            COUNT(*) FILTER (WHERE all_good = false OR all_good IS NULL) as poor_quality_count,
                            ROUND(100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*), 2) as good_quality_percent,
                            MIN(created_at) as earliest_data,
                            MAX(created_at) as latest_data
                        FROM {self.unified_view} 
                        WHERE city_id IS NOT NULL;
                    """)
                    sample_result = conn.execute(sample_sql).fetchone()
                    if sample_result:
                        print(f"ğŸ“ˆ æ•°æ®æ¦‚å†µ: {sample_result.subdataset_count} ä¸ªå­æ•°æ®é›†, {sample_result.city_count} ä¸ªåŸå¸‚")
                        print(f"ğŸ“Š è´¨é‡åˆ†å¸ƒ: {sample_result.good_quality_count:,} åˆæ ¼ ({sample_result.good_quality_percent}%), {sample_result.poor_quality_count:,} ä¸åˆæ ¼")
                        
                        # æ˜¾ç¤ºæŒ‰åŸå¸‚çš„è´¨é‡åˆ†å¸ƒ
                        city_quality_sql = text(f"""
                            SELECT 
                                city_id,
                                COUNT(*) as total_count,
                                COUNT(*) FILTER (WHERE all_good = true) as good_count,
                                ROUND(100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*), 1) as good_percent
                            FROM {self.unified_view} 
                            WHERE city_id IS NOT NULL
                            GROUP BY city_id
                            ORDER BY total_count DESC
                            LIMIT 5;
                        """)
                        city_results = conn.execute(city_quality_sql).fetchall()
                        if city_results:
                            print(f"ğŸ™ï¸ TOP 5åŸå¸‚è´¨é‡åˆ†å¸ƒ:")
                            for city_result in city_results:
                                print(f"   {city_result.city_id}: {city_result.good_count:,}/{city_result.total_count:,} ({city_result.good_percent}%)")
                            print(f"ğŸ’¡ åªæœ‰all_good=trueçš„æ•°æ®ä¼šå‚ä¸å ç½®åˆ†æ")
                    
                    return True
                    
                except Exception as e:
                    print(f"âš ï¸ è§†å›¾æ•°æ®éªŒè¯å¤±è´¥: {str(e)[:100]}...")
                    return False
                
        except Exception as e:
            print(f"âŒ æ£€æŸ¥ç»Ÿä¸€è§†å›¾å¤±è´¥: {str(e)}")
            return False
    
    def create_analysis_table(self) -> bool:
        """åˆ›å»ºåˆ†æç»“æœè¡¨"""
        print("ğŸ› ï¸ åˆ›å»ºåˆ†æç»“æœè¡¨...")
        
        # è¯»å–åˆ›å»ºè¡¨çš„SQLè„šæœ¬
        sql_file = Path(__file__).parent / "sql" / "create_analysis_tables.sql"
        
        try:
            if sql_file.exists():
                # å¦‚æœSQLæ–‡ä»¶å­˜åœ¨ï¼Œä½¿ç”¨æ–‡ä»¶ä¸­çš„SQL
                with open(sql_file, 'r', encoding='utf-8') as f:
                    create_sql = f.read()
            else:
                # å¦åˆ™ä½¿ç”¨å†…ç½®çš„SQL
                create_sql = f"""
                -- åˆ›å»ºbboxå ç½®åˆ†æç»“æœè¡¨
                CREATE TABLE IF NOT EXISTS {self.analysis_table} (
                    id SERIAL PRIMARY KEY,
                    analysis_id VARCHAR(100) NOT NULL,
                    analysis_type VARCHAR(50) DEFAULT 'bbox_overlap',
                    analysis_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    hotspot_rank INTEGER,
                    overlap_count INTEGER,
                    total_overlap_area NUMERIC,
                    subdataset_count INTEGER,
                    scene_count INTEGER,
                    involved_subdatasets TEXT[],
                    involved_scenes TEXT[],
                    analysis_params TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                );

                -- æ·»åŠ PostGISå‡ ä½•åˆ—
                DO $$
                BEGIN
                    -- æ£€æŸ¥å‡ ä½•åˆ—æ˜¯å¦å·²å­˜åœ¨
                    IF NOT EXISTS (
                        SELECT 1 FROM information_schema.columns 
                        WHERE table_name = '{self.analysis_table}' 
                        AND column_name = 'geometry'
                    ) THEN
                        PERFORM AddGeometryColumn('public', '{self.analysis_table}', 'geometry', 4326, 'GEOMETRY', 2);
                    END IF;
                END $$;

                -- åˆ›å»ºç´¢å¼•
                CREATE INDEX IF NOT EXISTS idx_bbox_overlap_analysis_id ON {self.analysis_table} (analysis_id);
                CREATE INDEX IF NOT EXISTS idx_bbox_overlap_rank ON {self.analysis_table} (hotspot_rank);
                CREATE INDEX IF NOT EXISTS idx_bbox_overlap_count ON {self.analysis_table} (overlap_count);
                CREATE INDEX IF NOT EXISTS idx_bbox_overlap_geom ON {self.analysis_table} USING GIST (geometry);
                """
            
            with self.engine.connect() as conn:
                conn.execute(text(create_sql))
                conn.commit()
                
            print(f"âœ… åˆ†æç»“æœè¡¨ {self.analysis_table} åˆ›å»ºæˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºåˆ†æç»“æœè¡¨å¤±è´¥: {str(e)}")
            return False
    
    def run_overlap_analysis(
        self, 
        analysis_id: Optional[str] = None,
        city_filter: Optional[str] = None,
        subdataset_filter: Optional[List[str]] = None,
        min_overlap_area: float = 0.0,
        top_n: int = 20,
        intersect_only: bool = False
    ) -> str:
        """æ‰§è¡Œå ç½®åˆ†æ
        
        Args:
            analysis_id: åˆ†æIDï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            city_filter: åŸå¸‚è¿‡æ»¤
            subdataset_filter: å­æ•°æ®é›†è¿‡æ»¤
            min_overlap_area: æœ€å°é‡å é¢ç§¯é˜ˆå€¼
            top_n: è¿”å›çš„çƒ­ç‚¹æ•°é‡
            
        Returns:
            åˆ†æID
        """
        if not analysis_id:
            analysis_id = f"bbox_overlap_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # è®¾ç½®å½“å‰åˆ†æçŠ¶æ€
        self.current_analysis_id = analysis_id
        self.analysis_start_time = datetime.now()
        
        print(f"ğŸš€ å¼€å§‹å ç½®åˆ†æ: {analysis_id}")
        print(f"å‚æ•°: city_filter={city_filter}, min_overlap_area={min_overlap_area}, top_n={top_n}")
        if intersect_only:
            print(f"ğŸ¯ ç®€åŒ–æ¨¡å¼: åªè¦ç›¸äº¤å°±ç®—é‡å ï¼ˆå¿½ç•¥é¢ç§¯é˜ˆå€¼ï¼‰")
        print(f"ğŸ’¡ å¯ä»¥ä½¿ç”¨ Ctrl+C å®‰å…¨é€€å‡º")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡º
        self._check_shutdown()
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        where_conditions = []
        
        # ğŸ¯ åŸå¸‚è¿‡æ»¤ï¼ˆæ³¨æ„ï¼šç°åœ¨åŸºç¡€WHEREæ¡ä»¶å·²åŒ…å«a.city_id = b.city_idï¼‰
        if city_filter:
            # åŸå¸‚è¿‡æ»¤åªéœ€è¦é™åˆ¶å…¶ä¸­ä¸€ä¸ªè¡¨å³å¯ï¼Œå› ä¸ºå·²ç»æœ‰ç›¸åŒåŸå¸‚çº¦æŸ
            where_conditions.append(f"a.city_id = '{city_filter}'")
            print(f"ğŸ™ï¸ åŸå¸‚è¿‡æ»¤: {city_filter}")
        
        if subdataset_filter:
            subdataset_list = "', '".join(subdataset_filter)
            where_conditions.append(f"a.subdataset_name IN ('{subdataset_list}') AND b.subdataset_name IN ('{subdataset_list}')")
            print(f"ğŸ“¦ å­æ•°æ®é›†è¿‡æ»¤: {len(subdataset_filter)} ä¸ª")
        
        where_clause = "AND " + " AND ".join(where_conditions) if where_conditions else ""
        
        # è¯»å–åˆ†æSQLè„šæœ¬
        sql_file = Path(__file__).parent / "sql" / "overlap_analysis.sql"
        
        try:
            if sql_file.exists():
                # å¦‚æœSQLæ–‡ä»¶å­˜åœ¨ï¼Œè¯»å–å¹¶æ›¿æ¢å‚æ•°
                with open(sql_file, 'r', encoding='utf-8') as f:
                    analysis_sql_template = f.read()
                
                # æ ¹æ®intersect_onlyå‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨é¢ç§¯é˜ˆå€¼
                if intersect_only:
                    # å¦‚æœæ˜¯ç®€åŒ–æ¨¡å¼ï¼Œæ³¨é‡Šæ‰é¢ç§¯é˜ˆå€¼æ¡ä»¶
                    analysis_sql_template = analysis_sql_template.replace(
                        "AND ST_Area(ST_Intersection(a.geometry, b.geometry)) > {min_overlap_area}",
                        "-- ğŸ¯ ç®€åŒ–æ¨¡å¼ï¼šå¿½ç•¥é¢ç§¯é˜ˆå€¼\n        -- AND ST_Area(ST_Intersection(a.geometry, b.geometry)) > {min_overlap_area}"
                    )
                
                # æ›¿æ¢å‚æ•°
                analysis_sql = analysis_sql_template.format(
                    unified_view=self.unified_view,
                    analysis_table=self.analysis_table,
                    analysis_id=analysis_id,
                    where_clause=where_clause,
                    min_overlap_area=min_overlap_area,
                    top_n=top_n
                )
            else:
                # å†…ç½®SQL
                analysis_sql = f"""
                WITH overlapping_areas AS (
                    SELECT 
                        a.qgis_id as bbox_a_id,
                        b.qgis_id as bbox_b_id,
                        a.subdataset_name as subdataset_a,
                        b.subdataset_name as subdataset_b,
                        a.scene_token as scene_a,
                        b.scene_token as scene_b,
                        ST_Intersection(a.geometry, b.geometry) as overlap_geometry,
                        ST_Area(ST_Intersection(a.geometry, b.geometry)) as overlap_area
                    FROM {self.unified_view} a
                    JOIN {self.unified_view} b ON a.qgis_id < b.qgis_id
                    WHERE ST_Intersects(a.geometry, b.geometry)
                    {"" if intersect_only else f"AND ST_Area(ST_Intersection(a.geometry, b.geometry)) > {min_overlap_area}"}
                    AND NOT ST_Equals(a.geometry, b.geometry)
                    -- ğŸ¯ åªåˆ†æç›¸åŒåŸå¸‚çš„bbox
                    AND a.city_id = b.city_id
                    AND a.city_id IS NOT NULL
                    -- ğŸ¯ åªåˆ†æè´¨é‡åˆæ ¼çš„æ•°æ®
                    AND a.all_good = true
                    AND b.all_good = true
                    {where_clause}
                ),
                overlap_hotspots AS (
                    SELECT 
                        ST_Union(overlap_geometry) as hotspot_geometry,
                        COUNT(*) as overlap_count,
                        ARRAY_AGG(DISTINCT subdataset_a) || ARRAY_AGG(DISTINCT subdataset_b) as involved_subdatasets,
                        ARRAY_AGG(DISTINCT scene_a) || ARRAY_AGG(DISTINCT scene_b) as involved_scenes,
                        SUM(overlap_area) as total_overlap_area
                    FROM overlapping_areas
                    GROUP BY ST_SnapToGrid(overlap_geometry, 0.001)
                    HAVING COUNT(*) >= 2
                )
                INSERT INTO {self.analysis_table} 
                (analysis_id, hotspot_rank, overlap_count, total_overlap_area, 
                 subdataset_count, scene_count, involved_subdatasets, involved_scenes, geometry, analysis_params)
                SELECT 
                    '{analysis_id}' as analysis_id,
                    ROW_NUMBER() OVER (ORDER BY overlap_count DESC) as hotspot_rank,
                    overlap_count,
                    total_overlap_area,
                    ARRAY_LENGTH(involved_subdatasets, 1) as subdataset_count,
                    ARRAY_LENGTH(involved_scenes, 1) as scene_count,
                    involved_subdatasets,
                    involved_scenes,
                    hotspot_geometry as geometry,
                    '{{"city_filter": "{city_filter}", "min_overlap_area": {min_overlap_area}, "top_n": {top_n}}}' as analysis_params
                FROM overlap_hotspots
                ORDER BY overlap_count DESC
                LIMIT {top_n};
                """
            
            print(f"âš¡ æ‰§è¡Œç©ºé—´å ç½®åˆ†æSQL...")
            self._check_shutdown()  # æ‰§è¡Œå‰æ£€æŸ¥
            
            with self.engine.connect() as conn:
                self.current_connection = conn  # ä¿å­˜è¿æ¥å¼•ç”¨
                
                result = conn.execute(text(analysis_sql))
                self._check_shutdown()  # SQLæ‰§è¡Œåæ£€æŸ¥
                
                conn.commit()
                print(f"âœ… SQLæ‰§è¡Œå®Œæˆï¼Œæ­£åœ¨ç»Ÿè®¡ç»“æœ...")
                
                # è·å–æ’å…¥çš„è®°å½•æ•°
                count_sql = text(f"SELECT COUNT(*) FROM {self.analysis_table} WHERE analysis_id = '{analysis_id}';")
                count_result = conn.execute(count_sql)
                inserted_count = count_result.scalar()
                
                self.current_connection = None  # æ¸…é™¤è¿æ¥å¼•ç”¨
                
            print(f"âœ… å ç½®åˆ†æå®Œæˆï¼Œå‘ç° {inserted_count} ä¸ªé‡å çƒ­ç‚¹")
            elapsed = datetime.now() - self.analysis_start_time
            print(f"â±ï¸ æ€»è€—æ—¶: {elapsed}")
            return analysis_id
            
        except Exception as e:
            print(f"âŒ å ç½®åˆ†æå¤±è´¥: {str(e)}")
            raise
    
    def create_qgis_view(self, analysis_id: Optional[str] = None) -> bool:
        """åˆ›å»ºQGISå…¼å®¹è§†å›¾"""
        print("ğŸ¨ åˆ›å»ºQGISå…¼å®¹è§†å›¾...")
        
        # è¯»å–è§†å›¾åˆ›å»ºSQL
        sql_file = Path(__file__).parent / "sql" / "qgis_views.sql"
        
        try:
            if sql_file.exists():
                with open(sql_file, 'r', encoding='utf-8') as f:
                    view_sql = f.read()
                    view_sql = view_sql.format(
                        qgis_view=self.qgis_view,
                        analysis_table=self.analysis_table
                    )
            else:
                # å†…ç½®SQL
                where_clause = f"WHERE analysis_id = '{analysis_id}'" if analysis_id else ""
                
                view_sql = f"""
                CREATE OR REPLACE VIEW {self.qgis_view} AS
                SELECT 
                    id as qgis_id,
                    analysis_id,
                    hotspot_rank,
                    overlap_count,
                    total_overlap_area,
                    subdataset_count,
                    scene_count,
                    involved_subdatasets,
                    involved_scenes,
                    CASE 
                        WHEN overlap_count >= 10 THEN 'High Density'
                        WHEN overlap_count >= 5 THEN 'Medium Density'
                        ELSE 'Low Density'
                    END as density_level,
                    geometry,
                    created_at
                FROM {self.analysis_table}
                WHERE analysis_type = 'bbox_overlap'
                {where_clause}
                ORDER BY hotspot_rank;
                """
            
            with self.engine.connect() as conn:
                conn.execute(text(view_sql))
                conn.commit()
                
            print(f"âœ… QGISè§†å›¾ {self.qgis_view} åˆ›å»ºæˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºQGISè§†å›¾å¤±è´¥: {str(e)}")
            return False
    
    def get_city_analysis_suggestions(self) -> pd.DataFrame:
        """è·å–åŸå¸‚åˆ†æå»ºè®®ï¼Œå¸®åŠ©ç”¨æˆ·é€‰æ‹©åˆé€‚çš„åŸå¸‚"""
        print("ğŸ” åˆ†æå„åŸå¸‚çš„æ•°æ®åˆ†å¸ƒï¼Œç”Ÿæˆåˆ†æå»ºè®®...")
        
        sql = text(f"""
            WITH city_stats AS (
                SELECT 
                    city_id,
                    COUNT(*) as total_bbox_count,
                    COUNT(*) FILTER (WHERE all_good = true) as good_bbox_count,
                    COUNT(DISTINCT subdataset_name) as subdataset_count,
                    ROUND(100.0 * COUNT(*) FILTER (WHERE all_good = true) / COUNT(*), 1) as good_percent,
                    -- ä¼°ç®—å¯èƒ½çš„é‡å å¯¹æ•°é‡ï¼ˆåŸºäºæ•°æ®å¯†åº¦ï¼‰
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 10000 THEN 'High'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 1000 THEN 'Medium' 
                        ELSE 'Low'
                    END as analysis_complexity,
                    -- é¢„ä¼°åˆ†ææ—¶é—´ï¼ˆåŸºäºæ•°æ®é‡ï¼‰
                    CASE 
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 50000 THEN '> 10åˆ†é’Ÿ'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 10000 THEN '2-10åˆ†é’Ÿ'
                        WHEN COUNT(*) FILTER (WHERE all_good = true) > 1000 THEN '< 2åˆ†é’Ÿ'
                        ELSE '< 30ç§’'
                    END as estimated_time
                FROM {self.unified_view}
                WHERE city_id IS NOT NULL
                GROUP BY city_id
                HAVING COUNT(*) FILTER (WHERE all_good = true) > 0
            )
            SELECT 
                city_id,
                total_bbox_count,
                good_bbox_count,
                subdataset_count,
                good_percent,
                analysis_complexity,
                estimated_time,
                -- æ¨èåº¦è¯„åˆ†
                CASE 
                    WHEN good_bbox_count BETWEEN 1000 AND 20000 AND good_percent > 90 THEN 'â­â­â­ æ¨è'
                    WHEN good_bbox_count BETWEEN 500 AND 50000 AND good_percent > 85 THEN 'â­â­ è¾ƒå¥½'
                    WHEN good_bbox_count > 100 THEN 'â­ å¯ç”¨'
                    ELSE 'âŒ ä¸å»ºè®®'
                END as recommendation
            FROM city_stats
            ORDER BY 
                CASE 
                    WHEN good_bbox_count BETWEEN 1000 AND 20000 AND good_percent > 90 THEN 1
                    WHEN good_bbox_count BETWEEN 500 AND 50000 AND good_percent > 85 THEN 2
                    WHEN good_bbox_count > 100 THEN 3
                    ELSE 4
                END,
                good_bbox_count DESC;
        """)
        
        try:
            result_df = pd.read_sql(sql, self.engine)
            
            if not result_df.empty:
                print(f"\nğŸ“Š åŸå¸‚åˆ†æå»ºè®®è¡¨:")
                print(result_df.to_string(index=False))
                
                # æä¾›å…·ä½“å»ºè®®
                recommended = result_df[result_df['recommendation'].str.contains('â­â­â­')]
                if not recommended.empty:
                    best_city = recommended.iloc[0]['city_id']
                    print(f"\nğŸ’¡ æ¨èåŸå¸‚: {best_city}")
                    print(f"   - æ•°æ®é‡é€‚ä¸­ï¼Œè´¨é‡è¾ƒé«˜")
                    print(f"   - é¢„ä¼°åˆ†ææ—¶é—´: {recommended.iloc[0]['estimated_time']}")
                    print(f"   - å»ºè®®å‘½ä»¤: --city {best_city}")
                
                return result_df
            else:
                print("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„åŸå¸‚æ•°æ®")
                return pd.DataFrame()
                
        except Exception as e:
            print(f"âŒ è·å–åŸå¸‚å»ºè®®å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def estimate_analysis_time(self, city_filter: str = None) -> dict:
        """ä¼°ç®—åˆ†ææ—¶é—´å’Œæ•°æ®é‡"""
        print("â±ï¸ ä¼°ç®—åˆ†ææ—¶é—´...")
        
        where_condition = f"WHERE city_id = '{city_filter}'" if city_filter else "WHERE city_id IS NOT NULL"
        
        sql = text(f"""
            SELECT 
                COUNT(*) FILTER (WHERE all_good = true) as analyzable_count,
                -- ä¼°ç®—å¯èƒ½çš„é‡å å¯¹æ•°é‡ï¼ˆn*(n-1)/2çš„ç®€åŒ–ä¼°ç®—ï¼‰
                CASE 
                    WHEN COUNT(*) FILTER (WHERE all_good = true) > 0 THEN
                        LEAST(
                            COUNT(*) FILTER (WHERE all_good = true) * (COUNT(*) FILTER (WHERE all_good = true) - 1) / 2,
                            1000000  -- é™åˆ¶æœ€å¤§ä¼°ç®—æ•°
                        )
                    ELSE 0
                END as estimated_pairs,
                CASE 
                    WHEN COUNT(*) FILTER (WHERE all_good = true) > 100000 THEN 'âš ï¸ å¾ˆé•¿ (>30åˆ†é’Ÿ)'
                    WHEN COUNT(*) FILTER (WHERE all_good = true) > 50000 THEN 'â³ è¾ƒé•¿ (10-30åˆ†é’Ÿ)'
                    WHEN COUNT(*) FILTER (WHERE all_good = true) > 10000 THEN 'â° ä¸­ç­‰ (2-10åˆ†é’Ÿ)'
                    WHEN COUNT(*) FILTER (WHERE all_good = true) > 1000 THEN 'âš¡ è¾ƒå¿« (<2åˆ†é’Ÿ)'
                    ELSE 'ğŸš€ å¾ˆå¿« (<30ç§’)'
                END as time_estimate,
                {f"'{city_filter}'" if city_filter else "'å…¨éƒ¨åŸå¸‚'"} as scope
            FROM {self.unified_view}
            {where_condition};
        """)
        
        try:
            result = self.engine.execute(sql).fetchone()
            
            estimate = {
                'analyzable_count': result.analyzable_count,
                'estimated_pairs': result.estimated_pairs,
                'time_estimate': result.time_estimate,
                'scope': result.scope
            }
            
            print(f"ğŸ“Š åˆ†æèŒƒå›´: {estimate['scope']}")
            print(f"ğŸ“ˆ å¯åˆ†ææ•°æ®: {estimate['analyzable_count']:,} ä¸ªbbox")
            print(f"ğŸ”— é¢„ä¼°é…å¯¹æ•°: {estimate['estimated_pairs']:,}")
            print(f"â±ï¸ é¢„ä¼°æ—¶é—´: {estimate['time_estimate']}")
            
            if estimate['analyzable_count'] > 50000:
                print(f"ğŸ’¡ å»ºè®®: æ•°æ®é‡è¾ƒå¤§ï¼Œå»ºè®®æŒ‡å®šå…·ä½“åŸå¸‚è¿›è¡Œåˆ†æ")
                print(f"ğŸ’¡ å‘½ä»¤: --city your_city_name")
            
            return estimate
            
        except Exception as e:
            print(f"âŒ æ—¶é—´ä¼°ç®—å¤±è´¥: {str(e)}")
            return {}

    def get_analysis_summary(self, analysis_id: str) -> pd.DataFrame:
        """è·å–åˆ†æç»“æœæ‘˜è¦"""
        sql = text(f"""
            SELECT 
                hotspot_rank,
                overlap_count,
                ROUND(total_overlap_area::numeric, 4) as total_overlap_area,
                subdataset_count,
                scene_count,
                involved_subdatasets,
                density_level
            FROM {self.qgis_view}
            WHERE analysis_id = :analysis_id
            ORDER BY hotspot_rank
            LIMIT 10;
        """)
        
        return pd.read_sql(sql, self.engine, params={'analysis_id': analysis_id})
    
    def export_for_qgis(self, analysis_id: str) -> Dict[str, Any]:
        """å¯¼å‡ºQGISå¯è§†åŒ–ä¿¡æ¯"""
        return {
            'analysis_id': analysis_id,
            'qgis_view': self.qgis_view,
            'unified_view': self.unified_view,
            'connection_info': {
                'host': 'local_pg',
                'port': 5432,
                'database': 'postgres',
                'username': 'postgres'
            },
            'visualization_tips': {
                'primary_key': 'qgis_id',
                'geometry_column': 'geometry',
                'style_column': 'density_level',
                'label_column': 'overlap_count',
                'filter_column': 'analysis_id'
            }
        }
    
    def list_analysis_results(self, pattern: str = None) -> pd.DataFrame:
        """åˆ—å‡ºæ‰€æœ‰åˆ†æç»“æœ
        
        Args:
            pattern: å¯é€‰çš„analysis_idè¿‡æ»¤æ¨¡å¼ï¼ˆæ”¯æŒSQL LIKEè¯­æ³•ï¼‰
            
        Returns:
            åŒ…å«åˆ†æç»“æœæ‘˜è¦çš„DataFrame
        """
        print("ğŸ“‹ æŸ¥è¯¢åˆ†æç»“æœ...")
        
        where_clause = ""
        if pattern:
            where_clause = f"WHERE analysis_id LIKE '{pattern}'"
            print(f"ğŸ” è¿‡æ»¤æ¡ä»¶: analysis_id LIKE '{pattern}'")
        
        sql = text(f"""
            SELECT 
                analysis_id,
                analysis_type,
                analysis_time,
                COUNT(*) as hotspot_count,
                MAX(hotspot_rank) as max_rank,
                SUM(overlap_count) as total_overlaps,
                ROUND(SUM(total_overlap_area)::numeric, 6) as total_area,
                STRING_AGG(DISTINCT UNNEST(involved_subdatasets), ', ') as subdatasets,
                MIN(created_at) as created_at
            FROM {self.analysis_table}
            {where_clause}
            GROUP BY analysis_id, analysis_type, analysis_time
            ORDER BY created_at DESC;
        """)
        
        try:
            with self.engine.connect() as conn:
                result_df = pd.read_sql(sql, conn)
                
                if not result_df.empty:
                    print(f"ğŸ“Š æ‰¾åˆ° {len(result_df)} ä¸ªåˆ†æç»“æœ:")
                    print(result_df.to_string(index=False))
                else:
                    print("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°åˆ†æç»“æœ")
                
                return result_df
                
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢åˆ†æç»“æœå¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def cleanup_analysis_results(
        self, 
        analysis_ids: Optional[List[str]] = None,
        pattern: str = None,
        older_than_days: int = None,
        dry_run: bool = True
    ) -> Dict[str, Any]:
        """æ¸…ç†åˆ†æç»“æœ
        
        Args:
            analysis_ids: æŒ‡å®šè¦åˆ é™¤çš„analysis_idåˆ—è¡¨
            pattern: æŒ‰æ¨¡å¼åˆ é™¤ï¼ˆæ”¯æŒSQL LIKEè¯­æ³•ï¼Œå¦‚'bbox_overlap_2023%'ï¼‰
            older_than_days: åˆ é™¤Nå¤©å‰çš„ç»“æœ
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼ï¼ˆä¸å®é™…åˆ é™¤ï¼‰
            
        Returns:
            æ¸…ç†ç»“æœç»Ÿè®¡
        """
        print("ğŸ§¹ å¼€å§‹æ¸…ç†åˆ†æç»“æœ...")
        
        # æ„å»ºåˆ é™¤æ¡ä»¶
        where_conditions = []
        
        if analysis_ids:
            id_list = "', '".join(analysis_ids)
            where_conditions.append(f"analysis_id IN ('{id_list}')")
            print(f"ğŸ¯ æŒ‰IDåˆ é™¤: {len(analysis_ids)} ä¸ª")
            
        if pattern:
            where_conditions.append(f"analysis_id LIKE '{pattern}'")
            print(f"ğŸ” æŒ‰æ¨¡å¼åˆ é™¤: '{pattern}'")
            
        if older_than_days:
            where_conditions.append(f"created_at < NOW() - INTERVAL '{older_than_days} days'")
            print(f"ğŸ“… åˆ é™¤ {older_than_days} å¤©å‰çš„ç»“æœ")
        
        if not where_conditions:
            print("âš ï¸ æœªæŒ‡å®šåˆ é™¤æ¡ä»¶ï¼Œä¸ºå®‰å…¨èµ·è§ä¸æ‰§è¡Œæ¸…ç†")
            return {"deleted_count": 0, "error": "æœªæŒ‡å®šåˆ é™¤æ¡ä»¶"}
        
        where_clause = "WHERE " + " AND ".join(where_conditions)
        
        try:
            with self.engine.connect() as conn:
                # å…ˆæŸ¥è¯¢è¦åˆ é™¤çš„è®°å½•
                preview_sql = text(f"""
                    SELECT 
                        analysis_id,
                        analysis_type,
                        COUNT(*) as record_count,
                        MIN(created_at) as earliest,
                        MAX(created_at) as latest
                    FROM {self.analysis_table}
                    {where_clause}
                    GROUP BY analysis_id, analysis_type
                    ORDER BY earliest DESC;
                """)
                
                preview_df = pd.read_sql(preview_sql, conn)
                
                if preview_df.empty:
                    print("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è®°å½•")
                    return {"deleted_count": 0, "preview": preview_df}
                
                print(f"\nğŸ“‹ å°†è¦æ¸…ç†çš„è®°å½•:")
                print(preview_df.to_string(index=False))
                
                total_records = preview_df['record_count'].sum()
                total_analyses = len(preview_df)
                
                print(f"\nğŸ“Š æ¸…ç†æ‘˜è¦:")
                print(f"   åˆ†ææ•°é‡: {total_analyses}")
                print(f"   è®°å½•æ€»æ•°: {total_records}")
                
                if dry_run:
                    print(f"\nğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - æœªå®é™…åˆ é™¤")
                    print(f"ğŸ’¡ ä½¿ç”¨ dry_run=False æ‰§è¡Œå®é™…åˆ é™¤")
                    return {
                        "deleted_count": 0,
                        "would_delete": total_records,
                        "analysis_count": total_analyses,
                        "preview": preview_df
                    }
                
                # å®é™…åˆ é™¤
                print(f"\nğŸ—‘ï¸ æ‰§è¡Œåˆ é™¤...")
                delete_sql = text(f"DELETE FROM {self.analysis_table} {where_clause};")
                result = conn.execute(delete_sql)
                deleted_count = result.rowcount
                conn.commit()
                
                print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} æ¡è®°å½•")
                
                return {
                    "deleted_count": deleted_count,
                    "analysis_count": total_analyses,
                    "preview": preview_df
                }
                
        except Exception as e:
            print(f"âŒ æ¸…ç†å¤±è´¥: {str(e)}")
            return {"deleted_count": 0, "error": str(e)}
    
    def cleanup_qgis_views(self, confirm: bool = False) -> bool:
        """æ¸…ç†QGISè§†å›¾
        
        Args:
            confirm: æ˜¯å¦ç¡®è®¤åˆ é™¤
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print("ğŸ¨ æ¸…ç†QGISè§†å›¾...")
        
        views_to_check = [
            self.qgis_view,
            "qgis_bbox_overlap_hotspots"
        ]
        
        try:
            with self.engine.connect() as conn:
                existing_views = []
                
                for view_name in views_to_check:
                    check_sql = text(f"""
                        SELECT EXISTS (
                            SELECT FROM information_schema.views 
                            WHERE table_schema = 'public' 
                            AND table_name = '{view_name}'
                        );
                    """)
                    
                    if conn.execute(check_sql).scalar():
                        existing_views.append(view_name)
                
                if not existing_views:
                    print("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„QGISè§†å›¾")
                    return True
                
                print(f"ğŸ“‹ æ‰¾åˆ°ä»¥ä¸‹è§†å›¾:")
                for view in existing_views:
                    print(f"   - {view}")
                
                if not confirm:
                    print(f"\nğŸ§ª è¯•è¿è¡Œæ¨¡å¼ - æœªå®é™…åˆ é™¤")
                    print(f"ğŸ’¡ ä½¿ç”¨ confirm=True æ‰§è¡Œå®é™…åˆ é™¤")
                    return False
                
                # åˆ é™¤è§†å›¾
                for view_name in existing_views:
                    drop_sql = text(f"DROP VIEW IF EXISTS {view_name};")
                    conn.execute(drop_sql)
                    print(f"âœ… åˆ é™¤è§†å›¾: {view_name}")
                
                conn.commit()
                print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {len(existing_views)} ä¸ªè§†å›¾")
                return True
                
        except Exception as e:
            print(f"âŒ æ¸…ç†è§†å›¾å¤±è´¥: {str(e)}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='BBoxå ç½®åˆ†æ')
    parser.add_argument('--city', help='åŸå¸‚è¿‡æ»¤')
    parser.add_argument('--subdatasets', nargs='+', help='å­æ•°æ®é›†è¿‡æ»¤')
    parser.add_argument('--min-overlap-area', type=float, default=0.0, help='æœ€å°é‡å é¢ç§¯é˜ˆå€¼')
    parser.add_argument('--top-n', type=int, default=20, help='è¿”å›çš„çƒ­ç‚¹æ•°é‡')
    parser.add_argument('--analysis-id', help='è‡ªå®šä¹‰åˆ†æID')
    parser.add_argument('--refresh-view', action='store_true', help='å¼ºåˆ¶åˆ·æ–°ç»Ÿä¸€è§†å›¾ï¼ˆé€‚ç”¨äºæ•°æ®æ›´æ–°åï¼‰')
    parser.add_argument('--suggest-city', action='store_true', help='æ˜¾ç¤ºåŸå¸‚åˆ†æå»ºè®®å¹¶é€€å‡º')
    parser.add_argument('--estimate-time', action='store_true', help='ä¼°ç®—åˆ†ææ—¶é—´å¹¶é€€å‡º')
    
    # æ¸…ç†ç›¸å…³å‚æ•°
    parser.add_argument('--list-results', action='store_true', help='åˆ—å‡ºæ‰€æœ‰åˆ†æç»“æœ')
    parser.add_argument('--cleanup', action='store_true', help='æ¸…ç†åˆ†æç»“æœ')
    parser.add_argument('--cleanup-pattern', help='æŒ‰æ¨¡å¼æ¸…ç†ï¼ˆå¦‚"bbox_overlap_2023%"ï¼‰')
    parser.add_argument('--cleanup-ids', nargs='+', help='æŒ‰IDæ¸…ç†ï¼ˆå¯æŒ‡å®šå¤šä¸ªï¼‰')
    parser.add_argument('--cleanup-older-than', type=int, help='æ¸…ç†Nå¤©å‰çš„ç»“æœ')
    parser.add_argument('--cleanup-views', action='store_true', help='æ¸…ç†QGISè§†å›¾')
    parser.add_argument('--confirm-cleanup', action='store_true', help='ç¡®è®¤æ‰§è¡Œæ¸…ç†ï¼ˆé»˜è®¤ä¸ºè¯•è¿è¡Œï¼‰')
    parser.add_argument('--intersect-only', action='store_true', help='ç®€åŒ–æ¨¡å¼ï¼šåªè¦ç›¸äº¤å°±ç®—é‡å ï¼ˆå¿½ç•¥é¢ç§¯é˜ˆå€¼ï¼‰')
    
    args = parser.parse_args()
    
    print("ğŸ¯ BBoxå ç½®åˆ†æç¤ºä¾‹")
    print("=" * 60)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = BBoxOverlapAnalyzer()
    
    try:
        # 1. ç¡®ä¿ç»Ÿä¸€è§†å›¾å­˜åœ¨
        print("\nğŸ“‹ æ­¥éª¤1: æ£€æŸ¥æ•°æ®å‡†å¤‡")
        # å¯¹äºå¤§é‡æ•°æ®çš„æƒ…å†µï¼Œæˆ‘ä»¬ä¼˜å…ˆä½¿ç”¨ç°æœ‰è§†å›¾ï¼Œåªåœ¨å¿…è¦æ—¶åˆ·æ–°
        force_refresh = args.refresh_view
        if not analyzer.ensure_unified_view(force_refresh=force_refresh):
            print("âŒ ç»Ÿä¸€è§†å›¾æ£€æŸ¥å¤±è´¥ï¼Œé€€å‡º")
            return
        
        # å¦‚æœç”¨æˆ·åªæƒ³æŸ¥çœ‹åŸå¸‚å»ºè®®
        if args.suggest_city:
            print("\nğŸ™ï¸ åŸå¸‚åˆ†æå»ºè®®")
            print("-" * 40)
            analyzer.get_city_analysis_suggestions()
            return
        
        # å¦‚æœç”¨æˆ·åªæƒ³ä¼°ç®—æ—¶é—´
        if args.estimate_time:
            print("\nâ±ï¸ åˆ†ææ—¶é—´ä¼°ç®—")
            print("-" * 40)
            analyzer.estimate_analysis_time(args.city)
            return
        
        # å¦‚æœç”¨æˆ·æƒ³åˆ—å‡ºåˆ†æç»“æœ
        if args.list_results:
            print("\nğŸ“‹ åˆ†æç»“æœåˆ—è¡¨")
            print("-" * 40)
            analyzer.list_analysis_results(args.cleanup_pattern)
            return
        
        # å¦‚æœç”¨æˆ·æƒ³æ¸…ç†åˆ†æç»“æœ
        if args.cleanup:
            print("\nğŸ§¹ æ¸…ç†åˆ†æç»“æœ")
            print("-" * 40)
            
            result = analyzer.cleanup_analysis_results(
                analysis_ids=args.cleanup_ids,
                pattern=args.cleanup_pattern,
                older_than_days=args.cleanup_older_than,
                dry_run=not args.confirm_cleanup
            )
            
            if not args.confirm_cleanup and result.get("would_delete", 0) > 0:
                print(f"\nğŸ’¡ å¦‚è¦å®é™…æ‰§è¡Œåˆ é™¤ï¼Œè¯·æ·»åŠ  --confirm-cleanup å‚æ•°")
            
            return
        
        # å¦‚æœç”¨æˆ·æƒ³æ¸…ç†QGISè§†å›¾
        if args.cleanup_views:
            print("\nğŸ¨ æ¸…ç†QGISè§†å›¾")
            print("-" * 40)
            analyzer.cleanup_qgis_views(confirm=args.confirm_cleanup)
            return
        
        # 2. åˆ›å»ºåˆ†æç»“æœè¡¨
        print("\nğŸ› ï¸ æ­¥éª¤2: å‡†å¤‡åˆ†æç¯å¢ƒ")
        if not analyzer.create_analysis_table():
            print("âŒ åˆ†æè¡¨åˆ›å»ºå¤±è´¥ï¼Œé€€å‡º")
            return
        
        # 3. åˆ†æå‰çš„æ—¶é—´ä¼°ç®—å’Œç¡®è®¤
        print("\nâ±ï¸ æ­¥éª¤3a: åˆ†æå‰ä¼°ç®—")
        print("-" * 40)
        estimate = analyzer.estimate_analysis_time(args.city)
        
        # å¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œç»™å‡ºè­¦å‘Šå’Œå»ºè®®
        if estimate and estimate.get('analyzable_count', 0) > 50000:
            print(f"\nâš ï¸ æ•°æ®é‡è­¦å‘Š:")
            print(f"   å½“å‰åˆ†æèŒƒå›´åŒ…å« {estimate['analyzable_count']:,} ä¸ªbbox")
            print(f"   é¢„ä¼°åˆ†ææ—¶é—´: {estimate.get('time_estimate', 'æœªçŸ¥')}")
            print(f"   ğŸ’¡ å»ºè®®: ä½¿ç”¨ --city å‚æ•°ç¼©å°åˆ†æèŒƒå›´")
            print(f"   ğŸ’¡ è·å–åŸå¸‚å»ºè®®: --suggest-city")
            
            if not args.city:
                print(f"\nğŸ¤” æ˜¯å¦ç»§ç»­å…¨é‡åˆ†æï¼Ÿè¿™å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´...")
                print(f"ğŸ’¡ å»ºè®®å…ˆè¿è¡Œ: --suggest-city æŸ¥çœ‹æ¨èåŸå¸‚")
        
        # 3b. æ‰§è¡Œå ç½®åˆ†æ
        print(f"\nğŸš€ æ­¥éª¤3b: æ‰§è¡Œå ç½®åˆ†æ")
        print("-" * 40)
        analysis_id = analyzer.run_overlap_analysis(
            analysis_id=args.analysis_id,
            city_filter=args.city,
            subdataset_filter=args.subdatasets,
            min_overlap_area=args.min_overlap_area,
            top_n=args.top_n,
            intersect_only=args.intersect_only
        )
        
        # 4. åˆ›å»ºQGISè§†å›¾
        print("\nğŸ¨ æ­¥éª¤4: åˆ›å»ºQGISè§†å›¾")
        if not analyzer.create_qgis_view(analysis_id):
            print("âŒ QGISè§†å›¾åˆ›å»ºå¤±è´¥")
            return
        
        # 5. æ˜¾ç¤ºåˆ†æç»“æœæ‘˜è¦
        print("\nğŸ“Š æ­¥éª¤5: åˆ†æç»“æœæ‘˜è¦")
        summary = analyzer.get_analysis_summary(analysis_id)
        if not summary.empty:
            print("TOP 10 é‡å çƒ­ç‚¹:")
            print(summary.to_string(index=False))
        else:
            print("æœªå‘ç°é‡å çƒ­ç‚¹")
        
        # 6. æä¾›QGISå¯è§†åŒ–æŒ‡å¯¼
        print("\nğŸ¯ æ­¥éª¤6: QGISå¯è§†åŒ–æŒ‡å¯¼")
        qgis_info = analyzer.export_for_qgis(analysis_id)
        
        print("ğŸ“‹ æ•°æ®åº“è¿æ¥ä¿¡æ¯:")
        conn_info = qgis_info['connection_info']
        for key, value in conn_info.items():
            print(f"   {key}: {value}")
        
        print(f"\nğŸ“Š åœ¨QGISä¸­åŠ è½½ä»¥ä¸‹å›¾å±‚:")
        print(f"   1. {qgis_info['unified_view']} - æ‰€æœ‰bboxæ•°æ®ï¼ˆåº•å›¾ï¼‰")
        print(f"   2. {qgis_info['qgis_view']} - é‡å çƒ­ç‚¹åŒºåŸŸ")
        
        print(f"\nğŸ¨ å¯è§†åŒ–å»ºè®®:")
        vis_tips = qgis_info['visualization_tips']
        print(f"   â€¢ ä¸»é”®: {vis_tips['primary_key']}")
        print(f"   â€¢ å‡ ä½•åˆ—: {vis_tips['geometry_column']}")
        print(f"   â€¢ æŒ‰ {vis_tips['style_column']} å­—æ®µè®¾ç½®é¢œè‰²")
        print(f"   â€¢ æ˜¾ç¤º {vis_tips['label_column']} æ ‡ç­¾")
        print(f"   â€¢ ä½¿ç”¨ {vis_tips['filter_column']} = '{analysis_id}' è¿‡æ»¤")
        
        print(f"\nâœ… å ç½®åˆ†æå®Œæˆï¼åˆ†æID: {analysis_id}")
        print(f"ç°åœ¨å¯ä»¥åœ¨QGISä¸­è¿æ¥æ•°æ®åº“å¹¶åŠ è½½è¿™äº›å›¾å±‚è¿›è¡Œå¯è§†åŒ–åˆ†æã€‚")
        
    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    main()
