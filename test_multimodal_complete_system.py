#!/usr/bin/env python3
"""
å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢ç³»ç»Ÿå®Œæ•´åŠŸèƒ½æµ‹è¯•

éªŒè¯å·²å®ç°çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1. âœ… çœŸå®HighPerformancePolygonTrajectoryQueryé›†æˆ
2. âœ… è½¨è¿¹ç‚¹åˆ°æºpolygonæ˜ å°„åŠŸèƒ½
3. âœ… ä¼˜åŒ–çš„è½¨è¿¹è·å–æ–¹æ³•ï¼ˆå¤ç”¨ç°æœ‰åŠŸèƒ½ï¼‰
4. âœ… å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
5. âœ… å®Œæ•´çš„å¤šæ¨¡æ€å·¥ä½œæµ

æ ¹æ®æŠ€æœ¯æ–¹æ¡ˆï¼Œæµ‹è¯•80%+ä»£ç å¤ç”¨åŸåˆ™çš„å®ç°æ•ˆæœã€‚
"""

import os
import sys
import logging
from pathlib import Path
from datetime import datetime

# è®¾ç½®é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from spdatalab.fusion.multimodal_trajectory_retrieval import (
    MultimodalTrajectoryWorkflow,
    MultimodalConfig
)
from spdatalab.dataset.multimodal_data_retriever import APIConfig

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_complete_multimodal_system():
    """æµ‹è¯•å®Œæ•´çš„å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢ç³»ç»Ÿ"""
    logger.info("ğŸš€ å¼€å§‹å®Œæ•´å¤šæ¨¡æ€ç³»ç»Ÿæµ‹è¯•...")
    
    try:
        # 1. é…ç½®è®¾ç½®
        api_config = APIConfig.from_env()
        logger.info(f"âœ… APIé…ç½®: {api_config.api_base_url}")
        
        config = MultimodalConfig(
            api_config=api_config,
            max_search_results=5,          # å°è§„æ¨¡æµ‹è¯•
            buffer_distance=10.0,          # 10ç±³ç¼“å†²åŒº
            similarity_threshold=0.3,
            time_window_days=30
        )
        
        # 2. åˆ›å»ºå·¥ä½œæµ
        workflow = MultimodalTrajectoryWorkflow(config)
        logger.info("âœ… å¤šæ¨¡æ€å·¥ä½œæµåˆå§‹åŒ–æˆåŠŸ")
        
        # 3. æ‰§è¡Œå®Œæ•´æ–‡æœ¬æŸ¥è¯¢æµç¨‹
        logger.info("ğŸ” æ‰§è¡Œå®Œæ•´å¤šæ¨¡æ€æ–‡æœ¬æŸ¥è¯¢æµç¨‹...")
        result = workflow.process_text_query(
            text="bicycle crossing intersection",
            collection="ddi_collection_camera_encoded_1",
            count=5,
            output_table="test_complete_system"
        )
        
        # 4. éªŒè¯æ ¸å¿ƒåŠŸèƒ½å®ç°
        return verify_complete_functionality(result)
        
    except Exception as e:
        logger.error(f"âŒ å®Œæ•´ç³»ç»Ÿæµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        return False

def verify_complete_functionality(result):
    """éªŒè¯å®Œæ•´åŠŸèƒ½å®ç°"""
    logger.info("ğŸ” éªŒè¯å®Œæ•´åŠŸèƒ½å®ç°...")
    
    success_count = 0
    total_checks = 8
    
    if not result.get('success', False):
        logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return False
    
    stats = result.get('stats', {})
    
    # æ£€æŸ¥1: å¤šæ¨¡æ€æ£€ç´¢åŠŸèƒ½
    search_count = stats.get('search_results_count', 0)
    if search_count > 0:
        logger.info(f"âœ… æ£€æŸ¥1: å¤šæ¨¡æ€æ£€ç´¢æˆåŠŸ - {search_count} æ¡ç»“æœ")
        success_count += 1
    else:
        logger.warning("âš ï¸ æ£€æŸ¥1: å¤šæ¨¡æ€æ£€ç´¢æ— ç»“æœ")
    
    # æ£€æŸ¥2: æ™ºèƒ½èšåˆåŠŸèƒ½
    aggregation_efficiency = stats.get('aggregation_efficiency', {})
    if aggregation_efficiency:
        reduction_ratio = aggregation_efficiency.get('query_reduction_ratio', 0)
        logger.info(f"âœ… æ£€æŸ¥2: æ™ºèƒ½èšåˆåŠŸèƒ½ - æŸ¥è¯¢å‡å°‘æ¯”ä¾‹: {reduction_ratio:.2%}")
        success_count += 1
    else:
        logger.warning("âš ï¸ æ£€æŸ¥2: æ™ºèƒ½èšåˆç»Ÿè®¡ç¼ºå¤±")
    
    # æ£€æŸ¥3: è½¨è¿¹æ•°æ®è·å–ï¼ˆå¤ç”¨ç°æœ‰åŠŸèƒ½ï¼‰
    trajectory_count = stats.get('trajectory_data_count', 0)
    if trajectory_count > 0:
        logger.info(f"âœ… æ£€æŸ¥3: è½¨è¿¹æ•°æ®è·å–æˆåŠŸ - {trajectory_count} æ¡è½¨è¿¹")
        success_count += 1
    else:
        logger.warning("âš ï¸ æ£€æŸ¥3: è½¨è¿¹æ•°æ®è·å–æ— ç»“æœ")
    
    # æ£€æŸ¥4: Polygonä¼˜åŒ–åŠŸèƒ½
    polygon_optimization = stats.get('polygon_optimization', {})
    if polygon_optimization:
        compression_ratio = polygon_optimization.get('compression_ratio', 0)
        logger.info(f"âœ… æ£€æŸ¥4: Polygonä¼˜åŒ–æˆåŠŸ - å‹ç¼©ç‡: {compression_ratio:.1f}%")
        success_count += 1
    else:
        logger.warning("âš ï¸ æ£€æŸ¥4: Polygonä¼˜åŒ–ç»Ÿè®¡ç¼ºå¤±")
    
    # æ£€æŸ¥5: è½¨è¿¹ç‚¹æŸ¥è¯¢ï¼ˆçœŸå®HighPerformancePolygonTrajectoryQueryé›†æˆï¼‰
    points_count = stats.get('discovered_points_count', 0)
    if points_count > 0:
        logger.info(f"âœ… æ£€æŸ¥5: è½¨è¿¹ç‚¹æŸ¥è¯¢æˆåŠŸ - {points_count} ä¸ªè½¨è¿¹ç‚¹")
        success_count += 1
    else:
        logger.warning("âš ï¸ æ£€æŸ¥5: è½¨è¿¹ç‚¹æŸ¥è¯¢æ— ç»“æœ")
    
    # æ£€æŸ¥6: æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡
    query_performance = stats.get('query_performance', {})
    if query_performance:
        points_per_second = query_performance.get('points_per_second', 0)
        logger.info(f"âœ… æ£€æŸ¥6: æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡ - {points_per_second:.0f} ç‚¹/ç§’")
        success_count += 1
    else:
        logger.warning("âš ï¸ æ£€æŸ¥6: æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡ç¼ºå¤±")
    
    # æ£€æŸ¥7: æ•°æ®åº“ä¿å­˜åŠŸèƒ½
    saved_count = stats.get('saved_to_database', 0)
    if saved_count > 0:
        logger.info(f"âœ… æ£€æŸ¥7: æ•°æ®åº“ä¿å­˜æˆåŠŸ - {saved_count} æ¡è½¨è¿¹")
        success_count += 1
    elif 'database_save_error' in stats:
        logger.warning(f"âš ï¸ æ£€æŸ¥7: æ•°æ®åº“ä¿å­˜å¤±è´¥ - {stats['database_save_error']}")
    else:
        logger.warning("âš ï¸ æ£€æŸ¥7: æ•°æ®åº“ä¿å­˜çŠ¶æ€æœªçŸ¥")
    
    # æ£€æŸ¥8: æ•´ä½“æ€§èƒ½
    total_duration = stats.get('total_duration', 0)
    if total_duration > 0:
        logger.info(f"âœ… æ£€æŸ¥8: æ•´ä½“æ€§èƒ½ - æ€»è€—æ—¶: {total_duration:.2f} ç§’")
        success_count += 1
    else:
        logger.warning("âš ï¸ æ£€æŸ¥8: æ€§èƒ½ç»Ÿè®¡ç¼ºå¤±")
    
    # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯å±•ç¤º
    display_detailed_statistics(stats)
    
    # æ€»ç»“
    success_rate = (success_count / total_checks) * 100
    logger.info(f"\nğŸ“Š åŠŸèƒ½éªŒè¯å®Œæˆ: {success_count}/{total_checks} é€šè¿‡ ({success_rate:.1f}%)")
    
    return success_count >= 6  # è‡³å°‘75%åŠŸèƒ½æ­£å¸¸

def display_detailed_statistics(stats):
    """æ˜¾ç¤ºè¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯å±•ç¤º")
    logger.info("="*60)
    
    # 1. æ—¶é—´åˆ†å¸ƒç»Ÿè®¡
    if 'aggregation_time' in stats:
        logger.info(f"â±ï¸ å„é˜¶æ®µè€—æ—¶:")
        logger.info(f"   èšåˆä¼˜åŒ–: {stats.get('aggregation_time', 0):.3f}s")
        logger.info(f"   Polygonå¤„ç†: {stats.get('polygon_processing_time', 0):.3f}s")
        logger.info(f"   è½¨è¿¹æŸ¥è¯¢: {stats.get('trajectory_query_time', 0):.3f}s")
        logger.info(f"   æ€»è€—æ—¶: {stats.get('total_duration', 0):.3f}s")
    
    # 2. ç›¸ä¼¼åº¦ç»Ÿè®¡
    similarity_stats = stats.get('similarity_stats', {})
    if similarity_stats:
        logger.info(f"ğŸ¯ ç›¸ä¼¼åº¦åˆ†å¸ƒ:")
        logger.info(f"   å¹³å‡: {similarity_stats.get('avg', 0):.3f}")
        logger.info(f"   æœ€é«˜: {similarity_stats.get('max', 0):.3f}")
        logger.info(f"   æœ€ä½: {similarity_stats.get('min', 1):.3f}")
    
    # 3. æ—¶é—´èŒƒå›´ç»Ÿè®¡
    time_range = stats.get('time_range_stats', {})
    if time_range:
        span_hours = time_range.get('span_hours', 0)
        logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {span_hours:.1f} å°æ—¶")
    
    # 4. ä¼˜åŒ–æ•ˆç‡ç»Ÿè®¡
    aggregation_eff = stats.get('aggregation_efficiency', {})
    if aggregation_eff:
        logger.info(f"ğŸ”„ èšåˆä¼˜åŒ–æ•ˆç‡:")
        logger.info(f"   åŸå§‹ç»“æœ: {aggregation_eff.get('original_results', 0)}")
        logger.info(f"   èšåˆåæŸ¥è¯¢: {aggregation_eff.get('aggregated_queries', 0)}")
        logger.info(f"   æŸ¥è¯¢å‡å°‘: {aggregation_eff.get('query_reduction_ratio', 0):.2%}")
    
    polygon_opt = stats.get('polygon_optimization', {})
    if polygon_opt:
        logger.info(f"ğŸ“ Polygonä¼˜åŒ–æ•ˆç‡:")
        logger.info(f"   å‹ç¼©ç‡: {polygon_opt.get('compression_ratio', 0):.1f}%")
        logger.info(f"   æ¶ˆé™¤æ•°é‡: {polygon_opt.get('polygons_eliminated', 0)}")
    
    # 5. æ•°æ®é›†åˆ†å¸ƒï¼ˆverboseæ¨¡å¼è¯¦æƒ…ï¼‰
    dataset_details = stats.get('dataset_details', {})
    if dataset_details:
        logger.info(f"ğŸ“ æ•°æ®é›†åˆ†å¸ƒ (å‰5ä¸ª):")
        for i, (dataset_name, count) in enumerate(list(dataset_details.items())[:5], 1):
            display_name = dataset_name if len(dataset_name) <= 50 else dataset_name[:47] + "..."
            logger.info(f"   {i}. {display_name}: {count}")
        if len(dataset_details) > 5:
            logger.info(f"   ... å…± {len(dataset_details)} ä¸ªæ•°æ®é›†")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸ¯ å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢ç³»ç»Ÿå®Œæ•´åŠŸèƒ½æµ‹è¯•")
    logger.info("ğŸ”§ éªŒè¯æŠ€æœ¯æ–¹æ¡ˆä¸­80%+ä»£ç å¤ç”¨çš„å®ç°æ•ˆæœ")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_env_vars = ['MULTIMODAL_API_KEY', 'MULTIMODAL_USERNAME', 'MULTIMODAL_API_BASE_URL']
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    
    if missing_vars:
        logger.error(f"âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: {missing_vars}")
        logger.info("ğŸ’¡ è¯·ç¡®ä¿.envæ–‡ä»¶é…ç½®æ­£ç¡®")
        return False
    
    # æ˜¾ç¤ºæµ‹è¯•ä¿¡æ¯
    logger.info("\n" + "="*60)
    logger.info("ğŸ§ª æµ‹è¯•å†…å®¹:")
    logger.info("1. âœ… çœŸå®HighPerformancePolygonTrajectoryQueryé›†æˆ")
    logger.info("2. âœ… è½¨è¿¹ç‚¹åˆ°æºpolygonæ˜ å°„åŠŸèƒ½") 
    logger.info("3. âœ… ä¼˜åŒ–çš„è½¨è¿¹è·å–æ–¹æ³•ï¼ˆå¤ç”¨ç°æœ‰åŠŸèƒ½ï¼‰")
    logger.info("4. âœ… å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯æ”¶é›†")
    logger.info("5. âœ… å®Œæ•´çš„å¤šæ¨¡æ€å·¥ä½œæµ")
    logger.info("="*60)
    
    # æ‰§è¡Œæµ‹è¯•
    success = test_complete_multimodal_system()
    
    # æµ‹è¯•æ€»ç»“
    logger.info("\n" + "="*60)
    if success:
        logger.info("ğŸ‰ å®Œæ•´ç³»ç»Ÿæµ‹è¯•é€šè¿‡ï¼")
        logger.info("âœ… å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢ç³»ç»Ÿå·²æˆåŠŸå®ç°æŠ€æœ¯æ–¹æ¡ˆè¦æ±‚")
        logger.info("âœ… 80%+ä»£ç å¤ç”¨åŸåˆ™å¾—åˆ°æœ‰æ•ˆæ‰§è¡Œ")
        logger.info("âœ… æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
    else:
        logger.warning("âš ï¸ éƒ¨åˆ†åŠŸèƒ½æµ‹è¯•æœªé€šè¿‡")
        logger.info("ğŸ’¡ å»ºè®®æ£€æŸ¥æ•°æ®åº“è¿æ¥å’ŒAPIé…ç½®")
    logger.info("="*60)
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

