"""Â§öÊ®°ÊÄÅËΩ®ËøπÊ£ÄÁ¥¢Á≥ªÁªü - ‰∏ªËûçÂêàÂàÜÊûêÊ®°Âùó

Ê†∏ÂøÉÂäüËÉΩÔºö
1. MultimodalTrajectoryWorkflow - ËΩªÈáèÂåñÂçèË∞ÉÂô®ÔºåÊô∫ËÉΩËÅöÂêàÁ≠ñÁï•
2. ResultAggregator - Êô∫ËÉΩËÅöÂêàÂô®Ôºàdataset_name + Êó∂Èó¥Á™óÂè£ËÅöÂêàÔºâ
3. PolygonMerger - PolygonÂêàÂπ∂‰ºòÂåñÂô®ÔºàÈáçÂè†ÂêàÂπ∂Ôºâ
4. ËΩªÈáèÂåñÂ∑•‰ΩúÊµÅÔºöËøîÂõûËΩ®ËøπÁÇπËÄåÈùûÂÆåÊï¥ËΩ®ËøπÁ∫ø
5. Êò†Â∞Ñ‰øùÊåÅÔºö‰øùÁïôpolygonÂà∞Ê∫êÊï∞ÊçÆÁöÑÂØπÂ∫îÂÖ≥Á≥ª

Â§çÁî®Áé∞ÊúâÊ®°ÂùóÁöÑ80%ÂäüËÉΩÔºå‰∏ìÊ≥®‰∫éËûçÂêàÂàÜÊûêÂíåÊô∫ËÉΩ‰ºòÂåñ„ÄÇ
"""

import argparse
import json
import logging
import sys
import time
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import warnings

import geopandas as gpd
import pandas as pd
from shapely.geometry import LineString, Point, Polygon, shape
from shapely.ops import unary_union

# ÂØºÂÖ•Âü∫Á°ÄÊï∞ÊçÆÂ§ÑÁêÜÁªÑ‰ª∂
from spdatalab.dataset.multimodal_data_retriever import (
    APIConfig,
    MultimodalRetriever,
    TrajectoryToPolygonConverter
)

# ÂØºÂÖ•Áé∞ÊúâÈ´òÊÄßËÉΩÊü•ËØ¢ÂºïÊìéÔºàÂ§çÁî®80%ÂäüËÉΩÔºâ
from spdatalab.dataset.polygon_trajectory_query import (
    HighPerformancePolygonTrajectoryQuery,
    PolygonTrajectoryConfig
)

# ÊäëÂà∂Ë≠¶Âëä
warnings.filterwarnings('ignore', category=UserWarning)

# Êó•ÂøóÈÖçÁΩÆ
logger = logging.getLogger(__name__)


@dataclass
class MultimodalConfig:
    """ËΩªÈáèÂåñÁ†îÂèëÂ∑•ÂÖ∑ÈÖçÁΩÆ"""
    
    # APIÈÖçÁΩÆ
    api_config: APIConfig
    
    # Ê†∏ÂøÉÂèÇÊï∞ÔºàÁ†îÂèëÂàÜÊûê‰ºòÂåñÔºâ
    max_search_results: int = 5             # ÈÄÇÂêàÁ†îÂèëÂàÜÊûêÁöÑÈªòËÆ§ÂÄºÔºà‰∏éAPIÁ§∫‰æã‰∏ÄËá¥Ôºâ
    time_window_days: int = 30              # 30Â§©Êó∂Èó¥Á™óÂè£
    buffer_distance: float = 10.0           # 10Á±≥Á≤æÁ°ÆÁºìÂÜ≤
    
    # APIÈôêÂà∂ÔºàÁ°¨Á∫¶ÊùüÔºâ
    max_single_query: int = 10000           # ÂçïÊ¨°Êü•ËØ¢‰∏äÈôê
    max_total_query: int = 100000           # Á¥ØËÆ°Êü•ËØ¢‰∏äÈôê
    
    # ËÅöÂêà‰ºòÂåñÂèÇÊï∞
    overlap_threshold: float = 0.7          # PolygonÈáçÂè†Â∫¶ÈòàÂÄº
    time_window_hours: int = 24             # Êó∂Èó¥Á™óÂè£ËÅöÂêàÔºàÂ∞èÊó∂Ôºâ
    
    # Â§çÁî®Áé∞ÊúâÈÖçÁΩÆÔºàÁÆÄÂåñÔºâ
    polygon_config: Optional[PolygonTrajectoryConfig] = None
    
    # ËæìÂá∫ÈÖçÁΩÆ
    output_table: Optional[str] = None
    output_geojson: Optional[str] = None
    
    def __post_init__(self):
        """ÂàùÂßãÂåñÂêéÂ§ÑÁêÜ"""
        if self.polygon_config is None:
            # ‰ΩøÁî®ÈªòËÆ§ÁöÑÈ´òÊÄßËÉΩÈÖçÁΩÆ
            self.polygon_config = PolygonTrajectoryConfig(
                batch_threshold=50,
                chunk_size=20,
                limit_per_polygon=15000
            )


class ResultAggregator:
    """Â§öÊ®°ÊÄÅÊü•ËØ¢ÁªìÊûúËÅöÂêàÂô®
    
    ÂäüËÉΩÔºö
    - Êåâdataset_nameËÅöÂêàÔºåÈÅøÂÖçÈáçÂ§çÊü•ËØ¢
    - ÊåâÊó∂Èó¥Á™óÂè£ËÅöÂêàÔºåÂêàÂπ∂Áõ∏ËøëÊó∂Èó¥ÁöÑÊü•ËØ¢
    """
    
    def __init__(self, time_window_hours: int = 24):
        self.time_window_hours = time_window_hours
    
    def aggregate_by_dataset(self, search_results: List[Dict]) -> Dict[str, List[Dict]]:
        """Êåâdataset_nameËÅöÂêàÔºåÈÅøÂÖçÈáçÂ§çÊü•ËØ¢
        
        Args:
            search_results: Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢ÁªìÊûú
            
        Returns:
            Êåâdataset_nameÂàÜÁªÑÁöÑÁªìÊûú
        """
        if not search_results:
            return {}
        
        dataset_groups = defaultdict(list)
        for result in search_results:
            dataset_name = result.get('dataset_name', 'unknown')
            dataset_groups[dataset_name].append(result)
        
        logger.info(f"üìä DatasetËÅöÂêà: {len(search_results)}Êù°ÁªìÊûú ‚Üí {len(dataset_groups)}‰∏™Êï∞ÊçÆÈõÜ")
        
        # Âú®Ë∞ÉËØïÊ®°Âºè‰∏ãÊòæÁ§∫ËØ¶ÁªÜÁöÑÊï∞ÊçÆÈõÜ‰ø°ÊÅØ
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug("üìã ËÅöÂêàÁöÑÊï∞ÊçÆÈõÜËØ¶ÊÉÖ:")
            for dataset_name, items in dataset_groups.items():
                logger.debug(f"   üìÅ {dataset_name}: {len(items)}Êù°ÁªìÊûú")
                for i, item in enumerate(items[:3]):  # Âè™ÊòæÁ§∫Ââç3Êù°
                    similarity = item.get('similarity', 0)
                    timestamp = item.get('timestamp', 0)
                    logger.debug(f"      {i+1}. Áõ∏‰ººÂ∫¶={similarity:.3f}, Êó∂Èó¥Êà≥={timestamp}")
                if len(items) > 3:
                    logger.debug(f"      ... ËøòÊúâ{len(items)-3}Êù°ÁªìÊûú")
        
        return dict(dataset_groups)
    
    def aggregate_by_timewindow(self, dataset_groups: Dict[str, List[Dict]]) -> Dict[str, Dict]:
        """ÊåâÊó∂Èó¥Á™óÂè£ËÅöÂêàÔºåÂêàÂπ∂Áõ∏ËøëÊó∂Èó¥ÁöÑÊü•ËØ¢
        
        Args:
            dataset_groups: ÊåâdatasetÂàÜÁªÑÁöÑÁªìÊûú
            
        Returns:
            ËÅöÂêàÂêéÁöÑÊó∂Èó¥ËåÉÂõ¥Êü•ËØ¢ÂèÇÊï∞
        """
        aggregated_queries = {}
        
        for dataset_name, results in dataset_groups.items():
            if not results:
                continue
            
            # ÊèêÂèñÊó∂Èó¥Êà≥
            timestamps = [r.get('timestamp', 0) for r in results if r.get('timestamp')]
            if not timestamps:
                # Â¶ÇÊûúÊ≤°ÊúâÊó∂Èó¥Êà≥Ôºå‰ΩøÁî®ÂΩìÂâçÊó∂Èó¥
                current_ts = int(datetime.now().timestamp() * 1000)
                timestamps = [current_ts]
            
            # ËÆ°ÁÆóÊó∂Èó¥Á™óÂè£
            min_timestamp = min(timestamps)
            max_timestamp = max(timestamps)
            
            # Êâ©Â±ïÊó∂Èó¥Á™óÂè£ÔºàÈªòËÆ§ÂâçÂêéÂêÑÂª∂‰º∏12Â∞èÊó∂Ôºâ
            window_ms = self.time_window_hours * 60 * 60 * 1000 // 2  # ÂçïËæπÁ™óÂè£
            start_time = min_timestamp - window_ms
            end_time = max_timestamp + window_ms
            
            aggregated_queries[dataset_name] = {
                'start_time': start_time,
                'end_time': end_time,
                'original_timestamps': timestamps,
                'result_count': len(results)
            }
        
        logger.info(f"‚è∞ Êó∂Èó¥Á™óÂè£ËÅöÂêà: ÁîüÊàê{len(aggregated_queries)}‰∏™Êó∂Èó¥ËåÉÂõ¥Êü•ËØ¢")
        return aggregated_queries


class PolygonMerger:
    """PolygonÂêàÂπ∂‰ºòÂåñÂô®
    
    ÂäüËÉΩÔºö
    - ÂêàÂπ∂ÈáçÂè†Â∫¶È´òÁöÑpolygonÔºå‰øùÁïôÊ∫êÊï∞ÊçÆÊò†Â∞Ñ
    - ‰ºòÂåñÊü•ËØ¢ÊïàÁéáÔºåÂáèÂ∞ëÊï∞ÊçÆÂ∫ìËÆøÈóÆ
    """
    
    def __init__(self, overlap_threshold: float = 0.7):
        self.overlap_threshold = overlap_threshold  # ÈáçÂè†Â∫¶ÈòàÂÄº
    
    def merge_overlapping_polygons(self, polygons_with_source: List[Dict]) -> List[Dict]:
        """ÂêàÂπ∂ÈáçÂè†Â∫¶È´òÁöÑpolygonÔºå‰øùÁïôÊ∫êÊï∞ÊçÆÊò†Â∞Ñ
        
        Args:
            polygons_with_source: polygonÂàóË°®ÔºåÊØèÈ°πÂåÖÂê´geometryÂíåproperties
            
        Returns:
            ÂêàÂπ∂ÂêéÁöÑpolygonÂàóË°®Ôºå‰øùÊåÅsourceÊò†Â∞Ñ
        """
        if not polygons_with_source:
            return []
        
        logger.info(f"üîÑ ÂºÄÂßãPolygonÂêàÂπ∂‰ºòÂåñ: {len(polygons_with_source)}‰∏™ÂéüÂßãPolygon")
        
        # ËΩ¨Êç¢‰∏∫Ê†áÂáÜÊ†ºÂºè
        polygons = []
        for i, item in enumerate(polygons_with_source):
            geom = item.get('geometry')
            if isinstance(geom, Polygon) and geom.is_valid:
                polygons.append({
                    'id': item.get('id', f'polygon_{i}'),
                    'geometry': geom,
                    'sources': [item.get('properties', {})],  # ÂàùÂßãsourceÂàóË°®
                    'merged': False
                })
        
        if not polygons:
            return []
        
        # ‰ΩøÁî®ÁÆÄÂçïÁöÑË¥™ÂøÉÂêàÂπ∂ÁÆóÊ≥ï
        merged_polygons = []
        processed = set()
        
        for i, poly1 in enumerate(polygons):
            if i in processed:
                continue
            
            # ÂàùÂßãÂåñÂêàÂπ∂ÁªÑ
            merge_group = [poly1]
            current_geom = poly1['geometry']
            processed.add(i)
            
            # Êü•ÊâæÂèØÂêàÂπ∂ÁöÑpolygon
            for j, poly2 in enumerate(polygons[i+1:], i+1):
                if j in processed:
                    continue
                
                overlap_ratio = self.calculate_overlap_ratio(current_geom, poly2['geometry'])
                if overlap_ratio >= self.overlap_threshold:
                    merge_group.append(poly2)
                    processed.add(j)
                    # Êõ¥Êñ∞ÂêàÂπ∂Âá†‰ΩïÔºàÁÆÄÂçïunionÔºâ
                    try:
                        current_geom = current_geom.union(poly2['geometry'])
                    except Exception as e:
                        logger.warning(f"PolygonÂêàÂπ∂Â§±Ë¥•: {e}")
            
            # ÂàõÂª∫ÂêàÂπ∂ÁªìÊûú
            if len(merge_group) > 1:
                # Â§ö‰∏™polygonÂêàÂπ∂
                all_sources = []
                for poly in merge_group:
                    all_sources.extend(poly['sources'])
                
                merged_polygons.append({
                    'id': f"merged_polygon_{len(merged_polygons)}",
                    'geometry': current_geom,
                    'properties': {
                        'merged_count': len(merge_group),
                        'sources': all_sources,
                        'merge_type': 'overlapping'
                    }
                })
            else:
                # Âçï‰∏™polygon‰øùÊåÅ‰∏çÂèò
                merged_polygons.append({
                    'id': poly1['id'],
                    'geometry': poly1['geometry'],
                    'properties': {
                        'merged_count': 1,
                        'sources': poly1['sources'],
                        'merge_type': 'original'
                    }
                })
        
        optimization_ratio = f"{len(polygons_with_source)} ‚Üí {len(merged_polygons)}"
        logger.info(f"‚úÖ PolygonÂêàÂπ∂ÂÆåÊàê: {optimization_ratio} (ÂéãÁº©Áéá: {1 - len(merged_polygons)/len(polygons_with_source):.1%})")
        
        return merged_polygons
    
    def calculate_overlap_ratio(self, poly1: Polygon, poly2: Polygon) -> float:
        """ËÆ°ÁÆó‰∏§‰∏™polygonÁöÑÈáçÂè†ÊØî‰æã
        
        Args:
            poly1, poly2: ÂæÖÊØîËæÉÁöÑpolygon
            
        Returns:
            ÈáçÂè†ÊØî‰æã (0.0 - 1.0)
        """
        try:
            if not poly1.intersects(poly2):
                return 0.0
            
            intersection_area = poly1.intersection(poly2).area
            union_area = poly1.union(poly2).area
            
            return intersection_area / union_area if union_area > 0 else 0.0
        except Exception:
            return 0.0


class MultimodalTrajectoryWorkflow:
    """Â§öÊ®°ÊÄÅËΩ®ËøπÊ£ÄÁ¥¢Â∑•‰ΩúÊµÅ
    
    ËΩªÈáèÂåñÂçèË∞ÉÂô®Ôºö
    - Êô∫ËÉΩËÅöÂêàÔºödataset_nameÂíåÊó∂Èó¥Á™óÂè£ËÅöÂêà
    - Polygon‰ºòÂåñÔºöÈáçÂè†polygonÂêàÂπ∂
    - ËΩªÈáèËæìÂá∫ÔºöËøîÂõûËΩ®ËøπÁÇπËÄåÈùûÂÆåÊï¥ËΩ®Ëøπ  
    - Êò†Â∞Ñ‰øùÊåÅÔºö‰øùÁïôpolygonÂà∞Ê∫êÊï∞ÊçÆÁöÑÂØπÂ∫îÂÖ≥Á≥ª
    """
    
    def __init__(self, config: MultimodalConfig):
        # ÁªÑÂêàÁé∞ÊúâÂíåÊñ∞Â¢ûÁªÑ‰ª∂
        self.config = config
        self.retriever = MultimodalRetriever(config.api_config)
        self.converter = TrajectoryToPolygonConverter(config.buffer_distance)
        self.aggregator = ResultAggregator(config.time_window_hours)         # Êñ∞Â¢ûÔºöÁªìÊûúËÅöÂêàÂô®
        self.polygon_merger = PolygonMerger(config.overlap_threshold)        # Êñ∞Â¢ûÔºöpolygonÂêàÂπ∂Âô®
        self.polygon_processor = HighPerformancePolygonTrajectoryQuery(config.polygon_config)
    
    def process_text_query(self, text: str, collection: str, count: Optional[int] = None, 
                          start: int = 0, start_time: Optional[int] = None, 
                          end_time: Optional[int] = None, **kwargs) -> Dict:
        """‰ºòÂåñÁöÑÊñáÊú¨Êü•ËØ¢ÊµÅÁ®ã
        
        Args:
            text: Êü•ËØ¢ÊñáÊú¨
            collection: Áõ∏Êú∫collection
            count: Êü•ËØ¢Êï∞ÈáèÔºàÈªòËÆ§‰ΩøÁî®ÈÖçÁΩÆÂÄºÔºâ
            start: Ëµ∑ÂßãÂÅèÁßªÈáèÔºåÈªòËÆ§0
            start_time: ‰∫ã‰ª∂ÂºÄÂßãÊó∂Èó¥Ôºå13‰ΩçÊó∂Èó¥Êà≥ÔºàÂèØÈÄâÔºâ
            end_time: ‰∫ã‰ª∂ÁªìÊùüÊó∂Èó¥Ôºå13‰ΩçÊó∂Èó¥Êà≥ÔºàÂèØÈÄâÔºâ
            
        Returns:
            ËΩªÈáèÂåñÊü•ËØ¢ÁªìÊûú
        """
        if count is None:
            count = self.config.max_search_results
        
        return self._execute_optimized_workflow(
            retrieval_func=lambda: self.retriever.retrieve_by_text(
                text=text,
                collection=collection,
                count=count,
                start=start,
                start_time=start_time,
                end_time=end_time
            ),
            query_type="text",
            query_content=text,
            collection=collection,
            **kwargs
        )
    
    def process_image_query(self, images: List[str], collection: str, count: Optional[int] = None,
                           start: int = 0, start_time: Optional[int] = None,
                           end_time: Optional[int] = None, **kwargs) -> Dict:
        """‰ºòÂåñÁöÑÂõæÁâáÊü•ËØ¢ÊµÅÁ®ã
        
        Args:
            images: ÂõæÁâábase64ÁºñÁ†ÅÂêéÁöÑÂ≠óÁ¨¶‰∏≤ÂàóË°®
            collection: Áõ∏Êú∫collection
            count: Êü•ËØ¢Êï∞ÈáèÔºàÈªòËÆ§‰ΩøÁî®ÈÖçÁΩÆÂÄºÔºâ
            start: Ëµ∑ÂßãÂÅèÁßªÈáèÔºåÈªòËÆ§0
            start_time: ‰∫ã‰ª∂ÂºÄÂßãÊó∂Èó¥Ôºå13‰ΩçÊó∂Èó¥Êà≥ÔºàÂèØÈÄâÔºâ
            end_time: ‰∫ã‰ª∂ÁªìÊùüÊó∂Èó¥Ôºå13‰ΩçÊó∂Èó¥Êà≥ÔºàÂèØÈÄâÔºâ
            
        Returns:
            ËΩªÈáèÂåñÊü•ËØ¢ÁªìÊûú
        """
        if count is None:
            count = self.config.max_search_results
        
        return self._execute_optimized_workflow(
            retrieval_func=lambda: self.retriever.retrieve_by_images(
                images=images,
                collection=collection,
                count=count,
                start=start,
                start_time=start_time,
                end_time=end_time
            ),
            query_type="image",
            query_content=f"{len(images)}Âº†ÂõæÁâá",
            collection=collection,
            **kwargs
        )
    
    def _execute_optimized_workflow(self, retrieval_func, query_type: str, query_content: str, 
                                   collection: str, **kwargs) -> Dict:
        """‰ºòÂåñÁöÑÂ∑•‰ΩúÊµÅÂºïÊìéÔºåÂåÖÂê´Êô∫ËÉΩËÅöÂêàÁ≠ñÁï•"""
        workflow_start = time.time()
        stats = {
            'query_type': query_type,
            'query_content': query_content,
            'collection': collection,
            'start_time': datetime.now(),
            'config': {
                'buffer_distance': self.config.buffer_distance,
                'time_window_days': self.config.time_window_days,
                'overlap_threshold': self.config.overlap_threshold
            }
        }
        
        try:
            # Stage 1: Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢
            logger.info("üîç Stage 1: ÊâßË°åÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢...")
            search_results = retrieval_func()
            stats['search_results_count'] = len(search_results)
            
            if not search_results:
                return self._handle_no_results(stats)
            
            # Stage 2: Êô∫ËÉΩËÅöÂêà (Êñ∞Â¢û‰ºòÂåñÔºÅ)
            logger.info(f"üìä Stage 2: Êô∫ËÉΩËÅöÂêà {len(search_results)} ‰∏™Ê£ÄÁ¥¢ÁªìÊûú...")
            aggregated_datasets = self.aggregator.aggregate_by_dataset(search_results)
            aggregated_queries = self.aggregator.aggregate_by_timewindow(aggregated_datasets)
            stats['aggregated_datasets'] = len(aggregated_datasets)
            stats['aggregated_queries'] = len(aggregated_queries)
            
            # Ê∑ªÂä†Êï∞ÊçÆÈõÜËØ¶ÊÉÖÁî®‰∫éverboseÊ®°ÂºèÊòæÁ§∫
            dataset_details = {}
            for dataset_name, results in aggregated_datasets.items():
                dataset_details[dataset_name] = len(results)
            stats['dataset_details'] = dataset_details
            
            # Stage 3: ËΩ®ËøπÊï∞ÊçÆËé∑Âèñ (‰ºòÂåñÂêéÔºåÂáèÂ∞ëÈáçÂ§çÊü•ËØ¢)
            logger.info(f"üöÄ Stage 3: ÊâπÈáèËé∑Âèñ {len(aggregated_datasets)} ‰∏™Êï∞ÊçÆÈõÜËΩ®Ëøπ...")
            trajectory_data = self._fetch_aggregated_trajectories(aggregated_queries)
            stats['trajectory_data_count'] = len(trajectory_data)
            
            if not trajectory_data:
                return self._handle_no_trajectories(stats)
            
            # Stage 4: PolygonËΩ¨Êç¢ÂíåÂêàÂπ∂ (Êñ∞Â¢ûÂêàÂπ∂‰ºòÂåñÔºÅ)
            logger.info(f"üîÑ Stage 4: ËΩ¨Êç¢ËΩ®Ëøπ‰∏∫PolygonÂπ∂Êô∫ËÉΩÂêàÂπ∂...")
            raw_polygons = self.converter.batch_convert(trajectory_data)
            merged_polygons = self.polygon_merger.merge_overlapping_polygons(raw_polygons)
            stats['raw_polygon_count'] = len(raw_polygons)
            stats['merged_polygon_count'] = len(merged_polygons)
            
            if not merged_polygons:
                return self._handle_no_polygons(stats)
            
            # Stage 5: ËΩªÈáèÂåñPolygonÊü•ËØ¢ (‰ªÖËøîÂõûËΩ®ËøπÁÇπÔºÅ)
            logger.info(f"‚ö° Stage 5: Âü∫‰∫é {len(merged_polygons)} ‰∏™PolygonÊü•ËØ¢ËΩ®ËøπÁÇπ...")
            trajectory_points = self._execute_lightweight_polygon_query(merged_polygons)
            stats['discovered_points_count'] = len(trajectory_points) if trajectory_points is not None else 0
            
            # Stage 6: ËΩªÈáèÂåñÁªìÊûúËæìÂá∫
            logger.info("üíæ Stage 6: ËΩªÈáèÂåñÁªìÊûúËæìÂá∫...")
            final_results = self._finalize_lightweight_results(trajectory_points, merged_polygons, stats)
            
            stats['success'] = True
            stats['total_duration'] = time.time() - workflow_start
            
            return final_results
            
        except Exception as e:
            stats['error'] = str(e)
            stats['success'] = False
            stats['total_duration'] = time.time() - workflow_start
            logger.error(f"‚ùå ‰ºòÂåñÂ∑•‰ΩúÊµÅÊâßË°åÂ§±Ë¥•: {e}")
            return stats
    
    def _handle_no_results(self, stats: Dict) -> Dict:
        """Â§ÑÁêÜÊó†Ê£ÄÁ¥¢ÁªìÊûúÁöÑÊÉÖÂÜµ"""
        logger.warning("‚ö†Ô∏è Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢Êú™ËøîÂõû‰ªª‰ΩïÁªìÊûú")
        stats['message'] = "Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢Êú™ËøîÂõû‰ªª‰ΩïÁªìÊûúÔºåËØ∑Â∞ùËØïË∞ÉÊï¥Êü•ËØ¢Êù°‰ª∂"
        stats['success'] = False
        return stats
    
    def _handle_no_trajectories(self, stats: Dict) -> Dict:
        """Â§ÑÁêÜÊó†ËΩ®ËøπÊï∞ÊçÆÁöÑÊÉÖÂÜµ"""
        logger.warning("‚ö†Ô∏è Êú™ÊâæÂà∞Áõ∏Â∫îÁöÑËΩ®ËøπÊï∞ÊçÆ")
        stats['message'] = "Ê†πÊçÆÊ£ÄÁ¥¢ÁªìÊûúÊú™ÊâæÂà∞Áõ∏Â∫îÁöÑËΩ®ËøπÊï∞ÊçÆ"
        stats['success'] = False
        return stats
    
    def _handle_no_polygons(self, stats: Dict) -> Dict:
        """Â§ÑÁêÜÊó†ÊúâÊïàPolygonÁöÑÊÉÖÂÜµ"""
        logger.warning("‚ö†Ô∏è ËΩ®ËøπËΩ¨Êç¢‰∏∫PolygonÂ§±Ë¥•")
        stats['message'] = "ËΩ®ËøπÊï∞ÊçÆËΩ¨Êç¢‰∏∫PolygonÂ§±Ë¥•ÔºåËØ∑Ê£ÄÊü•Êï∞ÊçÆË¥®Èáè"
        stats['success'] = False
        return stats
    
    def _fetch_aggregated_trajectories(self, aggregated_queries: Dict[str, Dict]) -> List[Dict]:
        """Âü∫‰∫éËÅöÂêàÁªìÊûúËé∑ÂèñËΩ®ËøπÊï∞ÊçÆ - ÂáèÂ∞ëÈáçÂ§çÊü•ËØ¢
        
        Ê≥®ÊÑèÔºöËøôÈáåÂ∫îËØ•Ë∞ÉÁî®Áé∞ÊúâÁöÑËΩ®ËøπÊü•ËØ¢ÊñπÊ≥ïÔºåÊöÇÊó∂ËøîÂõûÊ®°ÊãüÊï∞ÊçÆ
        """
        # TODO: ÈõÜÊàêÁé∞ÊúâÁöÑËΩ®ËøπÊü•ËØ¢ÂäüËÉΩ
        logger.info("üîß ËΩ®ËøπÊï∞ÊçÆËé∑ÂèñÂäüËÉΩÂæÖÈõÜÊàê...")
        
        # Ê®°ÊãüËøîÂõûÊï∞ÊçÆÁªìÊûÑ
        all_trajectory_data = []
        for dataset_name, time_range in aggregated_queries.items():
            # ËøôÈáåÂ∫îËØ•Ë∞ÉÁî®Áé∞ÊúâÁöÑdatasetÊü•ËØ¢ÊñπÊ≥ï
            # ÊöÇÊó∂ÂàõÂª∫Ê®°ÊãüÁöÑLineString
            from shapely.geometry import LineString
            
            # Ê®°ÊãüËΩ®ËøπÁÇπÔºàÂÆûÈôÖÂ∫îËØ•‰ªéÊï∞ÊçÆÂ∫ìÊü•ËØ¢Ôºâ
            mock_coords = [
                (116.3, 39.9), (116.31, 39.91), (116.32, 39.92)  # Âåó‰∫¨ÈôÑËøëÂùêÊ†á
            ]
            trajectory_linestring = LineString(mock_coords)
            
            all_trajectory_data.append({
                'dataset_name': dataset_name,
                'linestring': trajectory_linestring,
                'time_range': time_range,
                'point_count': len(mock_coords)
            })
        
        return all_trajectory_data
    
    def _execute_lightweight_polygon_query(self, merged_polygons: List[Dict]) -> Optional[pd.DataFrame]:
        """ËΩªÈáèÂåñPolygonÊü•ËØ¢ - ‰ªÖËøîÂõûËΩ®ËøπÁÇπÔºå‰∏çÊûÑÂª∫ÂÆåÊï¥ËΩ®Ëøπ
        
        Ê≥®ÊÑèÔºöËøôÈáåÂ∫îËØ•Ë∞ÉÁî®Áé∞ÊúâÁöÑÈ´òÊÄßËÉΩÊü•ËØ¢ÂºïÊìéÔºåÊöÇÊó∂ËøîÂõûÊ®°ÊãüÊï∞ÊçÆ
        """
        if not merged_polygons:
            return None
        
        logger.info("üîß ËΩªÈáèÂåñPolygonÊü•ËØ¢ÂäüËÉΩÂæÖÈõÜÊàê...")
        
        # TODO: Ë∞ÉÁî®Áé∞ÊúâÁöÑHighPerformancePolygonTrajectoryQuery
        # points_df, query_stats = self.polygon_processor.query_intersecting_trajectory_points(merged_polygons)
        
        # Ê®°ÊãüËøîÂõûËΩ®ËøπÁÇπÊï∞ÊçÆ
        mock_data = {
            'dataset_name': ['dataset_1', 'dataset_2'],
            'timestamp': [1739958971349, 1739958971350],
            'longitude': [116.3, 116.31],
            'latitude': [39.9, 39.91],
            'source_polygon_id': ['merged_polygon_0', 'merged_polygon_0']
        }
        
        return pd.DataFrame(mock_data)
    
    def _finalize_lightweight_results(self, trajectory_points: Optional[pd.DataFrame], 
                                     merged_polygons: List[Dict], stats: Dict) -> Dict:
        """ËΩªÈáèÂåñÁªìÊûúÂ§ÑÁêÜ - ËøîÂõûËΩ®ËøπÁÇπÂíåpolygonÊò†Â∞Ñ"""
        if trajectory_points is None or trajectory_points.empty:
            stats['final_results'] = "Êó†ÂèëÁé∞ËΩ®ËøπÁÇπ"
            return stats
        
        # ËΩªÈáèÂåñËæìÂá∫Ê†ºÂºè
        results = {
            'trajectory_points': trajectory_points.to_dict('records') if not trajectory_points.empty else [],
            'source_polygons': [
                {
                    'id': poly['id'],
                    'properties': poly['properties'],
                    'geometry_wkt': poly['geometry'].wkt if poly.get('geometry') else None
                }
                for poly in merged_polygons
            ],
            'summary': {
                'total_points': len(trajectory_points),
                'unique_datasets': trajectory_points['dataset_name'].nunique() if 'dataset_name' in trajectory_points.columns else 0,
                'polygon_sources': len(merged_polygons),
                'optimization_ratio': f"{stats.get('raw_polygon_count', 0)} ‚Üí {stats.get('merged_polygon_count', 0)}"
            },
            'stats': stats
        }
        
        # ÂèØÈÄâÁöÑÊï∞ÊçÆÂ∫ì‰øùÂ≠ò
        if self.config.output_table:
            logger.info(f"üíæ ‰øùÂ≠òÁªìÊûúÂà∞Êï∞ÊçÆÂ∫ìË°®: {self.config.output_table}")
            try:
                # Â∞ÜDataFrameËΩ¨Êç¢‰∏∫Â≠óÂÖ∏ÂàóË°®Áî®‰∫éÊï∞ÊçÆÂ∫ì‰øùÂ≠ò
                trajectory_records = trajectory_points.to_dict('records') if not trajectory_points.empty else []
                save_count = self._save_to_database(trajectory_records, self.config.output_table, stats)
                stats['saved_to_database'] = save_count
                logger.info(f"‚úÖ Êï∞ÊçÆÂ∫ì‰øùÂ≠òÊàêÂäü: {save_count} Êù°ËΩ®ËøπÁÇπ")
            except Exception as e:
                logger.error(f"‚ùå Êï∞ÊçÆÂ∫ì‰øùÂ≠òÂ§±Ë¥•: {e}")
                stats['database_save_error'] = str(e)
        
        # ÂèØÈÄâÁöÑGeoJSON‰øùÂ≠ò
        if self.config.output_geojson:
            logger.info(f"üíæ ÂØºÂá∫GeoJSONÊñá‰ª∂: {self.config.output_geojson}")
            # TODO: ÂÆûÁé∞GeoJSONÂØºÂá∫ÂäüËÉΩ
        
        return results
    
    def _save_to_database(self, trajectories: List[Dict], table_name: str, stats: Dict) -> int:
        """‰øùÂ≠òÂ§öÊ®°ÊÄÅÊ£ÄÁ¥¢ÁªìÊûúÂà∞Êï∞ÊçÆÂ∫ìË°®
        
        Args:
            trajectories: ÂèëÁé∞ÁöÑËΩ®ËøπÁÇπÊï∞ÊçÆ
            table_name: ÁõÆÊ†áË°®Âêç
            stats: ÁªüËÆ°‰ø°ÊÅØ
            
        Returns:
            ‰øùÂ≠òÊàêÂäüÁöÑËÆ∞ÂΩïÊï∞
        """
        from spdatalab.common.io_hive import hive_cursor
        from sqlalchemy import text
        import pandas as pd
        
        if not trajectories:
            logger.warning("üìä Ê≤°ÊúâËΩ®ËøπÊï∞ÊçÆÈúÄË¶Å‰øùÂ≠ò")
            return 0
        
        try:
            # ÂàõÂª∫Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢ÁªìÊûúË°®
            self._create_multimodal_results_table(table_name)
            
            # ÂáÜÂ§áÊèíÂÖ•Êï∞ÊçÆ
            records = []
            for traj in trajectories:
                # Ëé∑ÂèñÊ∫ê‰ø°ÊÅØ
                source_polygons = traj.get('source_polygons', [])
                source_info = source_polygons[0] if source_polygons else {}
                
                record = {
                    'dataset_name': traj.get('dataset_name', ''),
                    'scene_id': traj.get('scene_id', ''),
                    'event_id': traj.get('event_id'),
                    'event_name': traj.get('event_name', ''),
                    'longitude': traj.get('longitude', 0.0),
                    'latitude': traj.get('latitude', 0.0),
                    'timestamp': traj.get('timestamp', 0),
                    'velocity': traj.get('velocity', 0.0),
                    'heading': traj.get('heading', 0.0),
                    'source_dataset': source_info.get('source_dataset', ''),
                    'source_timestamp': source_info.get('source_timestamp', 0),
                    'source_similarity': source_info.get('source_similarity', 0.0),
                    'source_polygon_id': source_info.get('polygon_id', ''),
                    'query_type': stats.get('query_type', 'text'),
                    'query_content': stats.get('query_content', ''),
                    'collection': stats.get('collection', ''),
                    'optimization_ratio': f"{stats.get('raw_polygon_count', 0)}‚Üí{stats.get('merged_polygon_count', 0)}"
                }
                records.append(record)
            
            # ÂàõÂª∫DataFrameÂπ∂‰øùÂ≠ò
            df = pd.DataFrame(records)
            
            # ‰ΩøÁî®hive_cursorËøõË°åÊâπÈáèÊèíÂÖ•
            with hive_cursor() as cursor:
                # ÂàÜÊâπÊèíÂÖ•Êï∞ÊçÆ
                batch_size = 1000
                total_inserted = 0
                
                for i in range(0, len(df), batch_size):
                    batch_df = df.iloc[i:i+batch_size]
                    batch_num = i // batch_size + 1
                    
                    logger.info(f"üíæ ‰øùÂ≠òÁ¨¨ {batch_num} Êâπ: {len(batch_df)} Êù°ËÆ∞ÂΩï")
                    
                    # ÂáÜÂ§áINSERTËØ≠Âè•
                    values_list = []
                    for _, row in batch_df.iterrows():
                        values = f"""(
                            '{row['dataset_name']}', '{row['scene_id']}', {row['event_id'] or 'NULL'},
                            '{row['event_name']}', {row['longitude']}, {row['latitude']}, 
                            {row['timestamp']}, {row['velocity']}, {row['heading']},
                            '{row['source_dataset']}', {row['source_timestamp']}, {row['source_similarity']},
                            '{row['source_polygon_id']}', '{row['query_type']}', '{row['query_content']}',
                            '{row['collection']}', '{row['optimization_ratio']}', CURRENT_TIMESTAMP
                        )"""
                        values_list.append(values)
                    
                    insert_sql = f"""
                        INSERT INTO {table_name} (
                            dataset_name, scene_id, event_id, event_name,
                            longitude, latitude, timestamp, velocity, heading,
                            source_dataset, source_timestamp, source_similarity, source_polygon_id,
                            query_type, query_content, collection, optimization_ratio, created_at
                        ) VALUES {','.join(values_list)}
                    """
                    
                    cursor.execute(insert_sql)
                    total_inserted += len(batch_df)
                    
                    logger.debug(f"üìä Á¨¨ {batch_num} Êâπ‰øùÂ≠òÂÆåÊàê: {len(batch_df)} Êù°")
            
            logger.info(f"‚úÖ Êï∞ÊçÆÂ∫ì‰øùÂ≠òÂÆåÊàê: {total_inserted} Êù°ËÆ∞ÂΩï‰øùÂ≠òÂà∞Ë°® {table_name}")
            return total_inserted
            
        except Exception as e:
            logger.error(f"‚ùå Êï∞ÊçÆÂ∫ì‰øùÂ≠òÂ§±Ë¥•: {e}")
            raise
    
    def _create_multimodal_results_table(self, table_name: str) -> bool:
        """ÂàõÂª∫Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢ÁªìÊûúË°®"""
        from spdatalab.common.io_hive import hive_cursor
        
        try:
            with hive_cursor() as cursor:
                # Ê£ÄÊü•Ë°®ÊòØÂê¶Â≠òÂú®
                check_sql = f"""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = '{table_name}'
                    )
                """
                cursor.execute(check_sql)
                table_exists = cursor.fetchone()[0]
                
                if table_exists:
                    logger.info(f"üìä Ë°® {table_name} Â∑≤Â≠òÂú®ÔºåË∑≥ËøáÂàõÂª∫")
                    return True
                
                # ÂàõÂª∫Ë°®
                create_sql = f"""
                    CREATE TABLE {table_name} (
                        id SERIAL PRIMARY KEY,
                        dataset_name TEXT NOT NULL,
                        scene_id TEXT,
                        event_id INTEGER,
                        event_name VARCHAR(765),
                        longitude DOUBLE PRECISION NOT NULL,
                        latitude DOUBLE PRECISION NOT NULL,
                        timestamp BIGINT NOT NULL,
                        velocity DOUBLE PRECISION DEFAULT 0.0,
                        heading DOUBLE PRECISION DEFAULT 0.0,
                        source_dataset TEXT,
                        source_timestamp BIGINT,
                        source_similarity DOUBLE PRECISION,
                        source_polygon_id TEXT,
                        query_type VARCHAR(50),
                        query_content TEXT,
                        collection VARCHAR(255),
                        optimization_ratio VARCHAR(50),
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """
                cursor.execute(create_sql)
                
                # ÂàõÂª∫Á¥¢Âºï
                indices = [
                    f"CREATE INDEX idx_{table_name}_dataset_name ON {table_name}(dataset_name)",
                    f"CREATE INDEX idx_{table_name}_timestamp ON {table_name}(timestamp)", 
                    f"CREATE INDEX idx_{table_name}_query_type ON {table_name}(query_type)",
                    f"CREATE INDEX idx_{table_name}_collection ON {table_name}(collection)",
                    f"CREATE INDEX idx_{table_name}_created_at ON {table_name}(created_at)"
                ]
                
                for index_sql in indices:
                    cursor.execute(index_sql)
                
                logger.info(f"‚úÖ ÊàêÂäüÂàõÂª∫Â§öÊ®°ÊÄÅÊ£ÄÁ¥¢ÁªìÊûúË°®: {table_name}")
                return True
                
        except Exception as e:
            logger.error(f"‚ùå ÂàõÂª∫Ë°®Â§±Ë¥•: {e}")
            return False


# ÂØºÂá∫‰∏ªË¶ÅÁ±ª
__all__ = [
    'MultimodalConfig',
    'MultimodalTrajectoryWorkflow',
    'ResultAggregator',
    'PolygonMerger'
]


# CLIÊîØÊåÅ
if __name__ == '__main__':
    from .multimodal_cli import main
    main()
