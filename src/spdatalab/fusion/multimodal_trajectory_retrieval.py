"""å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢ç³»ç»Ÿ - ä¸»èåˆåˆ†ææ¨¡å—

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. MultimodalTrajectoryWorkflow - è½»é‡åŒ–åè°ƒå™¨ï¼Œæ™ºèƒ½èšåˆç­–ç•¥
2. ResultAggregator - æ™ºèƒ½èšåˆå™¨ï¼ˆdataset_name + æ—¶é—´çª—å£èšåˆï¼‰
3. PolygonMerger - Polygonåˆå¹¶ä¼˜åŒ–å™¨ï¼ˆé‡å åˆå¹¶ï¼‰
4. è½»é‡åŒ–å·¥ä½œæµï¼šè¿”å›è½¨è¿¹ç‚¹è€Œéå®Œæ•´è½¨è¿¹çº¿
5. æ˜ å°„ä¿æŒï¼šä¿ç•™polygonåˆ°æºæ•°æ®çš„å¯¹åº”å…³ç³»

å¤ç”¨ç°æœ‰æ¨¡å—çš„80%åŠŸèƒ½ï¼Œä¸“æ³¨äºèåˆåˆ†æå’Œæ™ºèƒ½ä¼˜åŒ–ã€‚
"""

import argparse
import json
import logging
import sys
import time
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import warnings

import geopandas as gpd
import pandas as pd
from shapely.geometry import LineString, Point, Polygon, shape
from shapely.ops import unary_union

# å¯¼å…¥åŸºç¡€æ•°æ®å¤„ç†ç»„ä»¶
from spdatalab.dataset.multimodal_data_retriever import (
    APIConfig,
    MultimodalRetriever,
    TrajectoryToPolygonConverter
)

# å¯¼å…¥ç°æœ‰é«˜æ€§èƒ½æŸ¥è¯¢å¼•æ“ï¼ˆå¤ç”¨80%åŠŸèƒ½ï¼‰
from spdatalab.dataset.polygon_trajectory_query import (
    HighPerformancePolygonTrajectoryQuery,
    PolygonTrajectoryConfig
)

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)

# æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)


@dataclass
class MultimodalConfig:
    """è½»é‡åŒ–ç ”å‘å·¥å…·é…ç½®"""
    
    # APIé…ç½®
    api_config: APIConfig
    
    # æ ¸å¿ƒå‚æ•°ï¼ˆç ”å‘åˆ†æä¼˜åŒ–ï¼‰
    max_search_results: int = 5000          # é€‚åˆç ”å‘åˆ†æçš„é»˜è®¤å€¼
    similarity_threshold: float = 0.3
    time_window_days: int = 30              # 30å¤©æ—¶é—´çª—å£
    buffer_distance: float = 10.0           # 10ç±³ç²¾ç¡®ç¼“å†²
    
    # APIé™åˆ¶ï¼ˆç¡¬çº¦æŸï¼‰
    max_single_query: int = 10000           # å•æ¬¡æŸ¥è¯¢ä¸Šé™
    max_total_query: int = 100000           # ç´¯è®¡æŸ¥è¯¢ä¸Šé™
    
    # èšåˆä¼˜åŒ–å‚æ•°
    overlap_threshold: float = 0.7          # Polygoné‡å åº¦é˜ˆå€¼
    time_window_hours: int = 24             # æ—¶é—´çª—å£èšåˆï¼ˆå°æ—¶ï¼‰
    
    # å¤ç”¨ç°æœ‰é…ç½®ï¼ˆç®€åŒ–ï¼‰
    polygon_config: Optional[PolygonTrajectoryConfig] = None
    
    # è¾“å‡ºé…ç½®
    output_table: Optional[str] = None
    output_geojson: Optional[str] = None
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if self.polygon_config is None:
            # ä½¿ç”¨é»˜è®¤çš„é«˜æ€§èƒ½é…ç½®
            self.polygon_config = PolygonTrajectoryConfig(
                batch_threshold=50,
                chunk_size=20,
                limit_per_polygon=15000
            )


class ResultAggregator:
    """å¤šæ¨¡æ€æŸ¥è¯¢ç»“æœèšåˆå™¨
    
    åŠŸèƒ½ï¼š
    - æŒ‰dataset_nameèšåˆï¼Œé¿å…é‡å¤æŸ¥è¯¢
    - æŒ‰æ—¶é—´çª—å£èšåˆï¼Œåˆå¹¶ç›¸è¿‘æ—¶é—´çš„æŸ¥è¯¢
    """
    
    def __init__(self, time_window_hours: int = 24):
        self.time_window_hours = time_window_hours
    
    def aggregate_by_dataset(self, search_results: List[Dict]) -> Dict[str, List[Dict]]:
        """æŒ‰dataset_nameèšåˆï¼Œé¿å…é‡å¤æŸ¥è¯¢
        
        Args:
            search_results: å¤šæ¨¡æ€æ£€ç´¢ç»“æœ
            
        Returns:
            æŒ‰dataset_nameåˆ†ç»„çš„ç»“æœ
        """
        if not search_results:
            return {}
        
        dataset_groups = defaultdict(list)
        for result in search_results:
            dataset_name = result.get('dataset_name', 'unknown')
            dataset_groups[dataset_name].append(result)
        
        logger.info(f"ğŸ“Š Datasetèšåˆ: {len(search_results)}æ¡ç»“æœ â†’ {len(dataset_groups)}ä¸ªæ•°æ®é›†")
        return dict(dataset_groups)
    
    def aggregate_by_timewindow(self, dataset_groups: Dict[str, List[Dict]]) -> Dict[str, Dict]:
        """æŒ‰æ—¶é—´çª—å£èšåˆï¼Œåˆå¹¶ç›¸è¿‘æ—¶é—´çš„æŸ¥è¯¢
        
        Args:
            dataset_groups: æŒ‰datasetåˆ†ç»„çš„ç»“æœ
            
        Returns:
            èšåˆåçš„æ—¶é—´èŒƒå›´æŸ¥è¯¢å‚æ•°
        """
        aggregated_queries = {}
        
        for dataset_name, results in dataset_groups.items():
            if not results:
                continue
            
            # æå–æ—¶é—´æˆ³
            timestamps = [r.get('timestamp', 0) for r in results if r.get('timestamp')]
            if not timestamps:
                # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                current_ts = int(datetime.now().timestamp() * 1000)
                timestamps = [current_ts]
            
            # è®¡ç®—æ—¶é—´çª—å£
            min_timestamp = min(timestamps)
            max_timestamp = max(timestamps)
            
            # æ‰©å±•æ—¶é—´çª—å£ï¼ˆé»˜è®¤å‰åå„å»¶ä¼¸12å°æ—¶ï¼‰
            window_ms = self.time_window_hours * 60 * 60 * 1000 // 2  # å•è¾¹çª—å£
            start_time = min_timestamp - window_ms
            end_time = max_timestamp + window_ms
            
            aggregated_queries[dataset_name] = {
                'start_time': start_time,
                'end_time': end_time,
                'original_timestamps': timestamps,
                'result_count': len(results)
            }
        
        logger.info(f"â° æ—¶é—´çª—å£èšåˆ: ç”Ÿæˆ{len(aggregated_queries)}ä¸ªæ—¶é—´èŒƒå›´æŸ¥è¯¢")
        return aggregated_queries


class PolygonMerger:
    """Polygonåˆå¹¶ä¼˜åŒ–å™¨
    
    åŠŸèƒ½ï¼š
    - åˆå¹¶é‡å åº¦é«˜çš„polygonï¼Œä¿ç•™æºæ•°æ®æ˜ å°„
    - ä¼˜åŒ–æŸ¥è¯¢æ•ˆç‡ï¼Œå‡å°‘æ•°æ®åº“è®¿é—®
    """
    
    def __init__(self, overlap_threshold: float = 0.7):
        self.overlap_threshold = overlap_threshold  # é‡å åº¦é˜ˆå€¼
    
    def merge_overlapping_polygons(self, polygons_with_source: List[Dict]) -> List[Dict]:
        """åˆå¹¶é‡å åº¦é«˜çš„polygonï¼Œä¿ç•™æºæ•°æ®æ˜ å°„
        
        Args:
            polygons_with_source: polygonåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«geometryå’Œproperties
            
        Returns:
            åˆå¹¶åçš„polygonåˆ—è¡¨ï¼Œä¿æŒsourceæ˜ å°„
        """
        if not polygons_with_source:
            return []
        
        logger.info(f"ğŸ”„ å¼€å§‹Polygonåˆå¹¶ä¼˜åŒ–: {len(polygons_with_source)}ä¸ªåŸå§‹Polygon")
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        polygons = []
        for i, item in enumerate(polygons_with_source):
            geom = item.get('geometry')
            if isinstance(geom, Polygon) and geom.is_valid:
                polygons.append({
                    'id': item.get('id', f'polygon_{i}'),
                    'geometry': geom,
                    'sources': [item.get('properties', {})],  # åˆå§‹sourceåˆ—è¡¨
                    'merged': False
                })
        
        if not polygons:
            return []
        
        # ä½¿ç”¨ç®€å•çš„è´ªå¿ƒåˆå¹¶ç®—æ³•
        merged_polygons = []
        processed = set()
        
        for i, poly1 in enumerate(polygons):
            if i in processed:
                continue
            
            # åˆå§‹åŒ–åˆå¹¶ç»„
            merge_group = [poly1]
            current_geom = poly1['geometry']
            processed.add(i)
            
            # æŸ¥æ‰¾å¯åˆå¹¶çš„polygon
            for j, poly2 in enumerate(polygons[i+1:], i+1):
                if j in processed:
                    continue
                
                overlap_ratio = self.calculate_overlap_ratio(current_geom, poly2['geometry'])
                if overlap_ratio >= self.overlap_threshold:
                    merge_group.append(poly2)
                    processed.add(j)
                    # æ›´æ–°åˆå¹¶å‡ ä½•ï¼ˆç®€å•unionï¼‰
                    try:
                        current_geom = current_geom.union(poly2['geometry'])
                    except Exception as e:
                        logger.warning(f"Polygonåˆå¹¶å¤±è´¥: {e}")
            
            # åˆ›å»ºåˆå¹¶ç»“æœ
            if len(merge_group) > 1:
                # å¤šä¸ªpolygonåˆå¹¶
                all_sources = []
                for poly in merge_group:
                    all_sources.extend(poly['sources'])
                
                merged_polygons.append({
                    'id': f"merged_polygon_{len(merged_polygons)}",
                    'geometry': current_geom,
                    'properties': {
                        'merged_count': len(merge_group),
                        'sources': all_sources,
                        'merge_type': 'overlapping'
                    }
                })
            else:
                # å•ä¸ªpolygonä¿æŒä¸å˜
                merged_polygons.append({
                    'id': poly1['id'],
                    'geometry': poly1['geometry'],
                    'properties': {
                        'merged_count': 1,
                        'sources': poly1['sources'],
                        'merge_type': 'original'
                    }
                })
        
        optimization_ratio = f"{len(polygons_with_source)} â†’ {len(merged_polygons)}"
        logger.info(f"âœ… Polygonåˆå¹¶å®Œæˆ: {optimization_ratio} (å‹ç¼©ç‡: {1 - len(merged_polygons)/len(polygons_with_source):.1%})")
        
        return merged_polygons
    
    def calculate_overlap_ratio(self, poly1: Polygon, poly2: Polygon) -> float:
        """è®¡ç®—ä¸¤ä¸ªpolygonçš„é‡å æ¯”ä¾‹
        
        Args:
            poly1, poly2: å¾…æ¯”è¾ƒçš„polygon
            
        Returns:
            é‡å æ¯”ä¾‹ (0.0 - 1.0)
        """
        try:
            if not poly1.intersects(poly2):
                return 0.0
            
            intersection_area = poly1.intersection(poly2).area
            union_area = poly1.union(poly2).area
            
            return intersection_area / union_area if union_area > 0 else 0.0
        except Exception:
            return 0.0


class MultimodalTrajectoryWorkflow:
    """å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢å·¥ä½œæµ
    
    è½»é‡åŒ–åè°ƒå™¨ï¼š
    - æ™ºèƒ½èšåˆï¼šdataset_nameå’Œæ—¶é—´çª—å£èšåˆ
    - Polygonä¼˜åŒ–ï¼šé‡å polygonåˆå¹¶
    - è½»é‡è¾“å‡ºï¼šè¿”å›è½¨è¿¹ç‚¹è€Œéå®Œæ•´è½¨è¿¹  
    - æ˜ å°„ä¿æŒï¼šä¿ç•™polygonåˆ°æºæ•°æ®çš„å¯¹åº”å…³ç³»
    """
    
    def __init__(self, config: MultimodalConfig):
        # ç»„åˆç°æœ‰å’Œæ–°å¢ç»„ä»¶
        self.config = config
        self.retriever = MultimodalRetriever(config.api_config)
        self.converter = TrajectoryToPolygonConverter(config.buffer_distance)
        self.aggregator = ResultAggregator(config.time_window_hours)         # æ–°å¢ï¼šç»“æœèšåˆå™¨
        self.polygon_merger = PolygonMerger(config.overlap_threshold)        # æ–°å¢ï¼špolygonåˆå¹¶å™¨
        self.polygon_processor = HighPerformancePolygonTrajectoryQuery(config.polygon_config)
    
    def process_text_query(self, text: str, collection: str, count: Optional[int] = None, 
                          start_time: Optional[int] = None, end_time: Optional[int] = None,
                          **kwargs) -> Dict:
        """ä¼˜åŒ–çš„æ–‡æœ¬æŸ¥è¯¢æµç¨‹
        
        Args:
            text: æŸ¥è¯¢æ–‡æœ¬
            collection: ç›¸æœºcollection
            count: æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
            start_time: å¼€å§‹æ—¶é—´æˆ³
            end_time: ç»“æŸæ—¶é—´æˆ³
            
        Returns:
            è½»é‡åŒ–æŸ¥è¯¢ç»“æœ
        """
        if count is None:
            count = self.config.max_search_results
        
        return self._execute_optimized_workflow(
            retrieval_func=lambda: self.retriever.retrieve_by_text(
                text=text,
                collection=collection,
                count=count,
                start_time=start_time,
                end_time=end_time,
                similarity_threshold=self.config.similarity_threshold
            ),
            query_type="text",
            query_content=text,
            collection=collection,
            **kwargs
        )
    
    def process_image_query(self, image_paths: List[str], collection: str, **kwargs) -> Dict:
        """å›¾ç‰‡æŸ¥è¯¢æ¥å£ï¼ˆé¢„ç•™ï¼‰"""
        raise NotImplementedError("å›¾ç‰‡æ£€ç´¢åŠŸèƒ½é¢„ç•™ï¼Œæš‚ä¸å¼€å‘")
    
    def _execute_optimized_workflow(self, retrieval_func, query_type: str, query_content: str, 
                                   collection: str, **kwargs) -> Dict:
        """ä¼˜åŒ–çš„å·¥ä½œæµå¼•æ“ï¼ŒåŒ…å«æ™ºèƒ½èšåˆç­–ç•¥"""
        workflow_start = time.time()
        stats = {
            'query_type': query_type,
            'query_content': query_content,
            'collection': collection,
            'start_time': datetime.now(),
            'config': {
                'buffer_distance': self.config.buffer_distance,
                'time_window_days': self.config.time_window_days,
                'overlap_threshold': self.config.overlap_threshold
            }
        }
        
        try:
            # Stage 1: å¤šæ¨¡æ€æ£€ç´¢
            logger.info("ğŸ” Stage 1: æ‰§è¡Œå¤šæ¨¡æ€æ£€ç´¢...")
            search_results = retrieval_func()
            stats['search_results_count'] = len(search_results)
            
            if not search_results:
                return self._handle_no_results(stats)
            
            # Stage 2: æ™ºèƒ½èšåˆ (æ–°å¢ä¼˜åŒ–ï¼)
            logger.info(f"ğŸ“Š Stage 2: æ™ºèƒ½èšåˆ {len(search_results)} ä¸ªæ£€ç´¢ç»“æœ...")
            aggregated_datasets = self.aggregator.aggregate_by_dataset(search_results)
            aggregated_queries = self.aggregator.aggregate_by_timewindow(aggregated_datasets)
            stats['aggregated_datasets'] = len(aggregated_datasets)
            stats['aggregated_queries'] = len(aggregated_queries)
            
            # Stage 3: è½¨è¿¹æ•°æ®è·å– (ä¼˜åŒ–åï¼Œå‡å°‘é‡å¤æŸ¥è¯¢)
            logger.info(f"ğŸš€ Stage 3: æ‰¹é‡è·å– {len(aggregated_datasets)} ä¸ªæ•°æ®é›†è½¨è¿¹...")
            trajectory_data = self._fetch_aggregated_trajectories(aggregated_queries)
            stats['trajectory_data_count'] = len(trajectory_data)
            
            if not trajectory_data:
                return self._handle_no_trajectories(stats)
            
            # Stage 4: Polygonè½¬æ¢å’Œåˆå¹¶ (æ–°å¢åˆå¹¶ä¼˜åŒ–ï¼)
            logger.info(f"ğŸ”„ Stage 4: è½¬æ¢è½¨è¿¹ä¸ºPolygonå¹¶æ™ºèƒ½åˆå¹¶...")
            raw_polygons = self.converter.batch_convert(trajectory_data)
            merged_polygons = self.polygon_merger.merge_overlapping_polygons(raw_polygons)
            stats['raw_polygon_count'] = len(raw_polygons)
            stats['merged_polygon_count'] = len(merged_polygons)
            
            if not merged_polygons:
                return self._handle_no_polygons(stats)
            
            # Stage 5: è½»é‡åŒ–PolygonæŸ¥è¯¢ (ä»…è¿”å›è½¨è¿¹ç‚¹ï¼)
            logger.info(f"âš¡ Stage 5: åŸºäº {len(merged_polygons)} ä¸ªPolygonæŸ¥è¯¢è½¨è¿¹ç‚¹...")
            trajectory_points = self._execute_lightweight_polygon_query(merged_polygons)
            stats['discovered_points_count'] = len(trajectory_points) if trajectory_points is not None else 0
            
            # Stage 6: è½»é‡åŒ–ç»“æœè¾“å‡º
            logger.info("ğŸ’¾ Stage 6: è½»é‡åŒ–ç»“æœè¾“å‡º...")
            final_results = self._finalize_lightweight_results(trajectory_points, merged_polygons, stats)
            
            stats['success'] = True
            stats['total_duration'] = time.time() - workflow_start
            
            return final_results
            
        except Exception as e:
            stats['error'] = str(e)
            stats['success'] = False
            stats['total_duration'] = time.time() - workflow_start
            logger.error(f"âŒ ä¼˜åŒ–å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            return stats
    
    def _handle_no_results(self, stats: Dict) -> Dict:
        """å¤„ç†æ— æ£€ç´¢ç»“æœçš„æƒ…å†µ"""
        logger.warning("âš ï¸ å¤šæ¨¡æ€æ£€ç´¢æœªè¿”å›ä»»ä½•ç»“æœ")
        stats['message'] = "å¤šæ¨¡æ€æ£€ç´¢æœªè¿”å›ä»»ä½•ç»“æœï¼Œè¯·å°è¯•è°ƒæ•´æŸ¥è¯¢æ¡ä»¶"
        stats['success'] = False
        return stats
    
    def _handle_no_trajectories(self, stats: Dict) -> Dict:
        """å¤„ç†æ— è½¨è¿¹æ•°æ®çš„æƒ…å†µ"""
        logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸åº”çš„è½¨è¿¹æ•°æ®")
        stats['message'] = "æ ¹æ®æ£€ç´¢ç»“æœæœªæ‰¾åˆ°ç›¸åº”çš„è½¨è¿¹æ•°æ®"
        stats['success'] = False
        return stats
    
    def _handle_no_polygons(self, stats: Dict) -> Dict:
        """å¤„ç†æ— æœ‰æ•ˆPolygonçš„æƒ…å†µ"""
        logger.warning("âš ï¸ è½¨è¿¹è½¬æ¢ä¸ºPolygonå¤±è´¥")
        stats['message'] = "è½¨è¿¹æ•°æ®è½¬æ¢ä¸ºPolygonå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡"
        stats['success'] = False
        return stats
    
    def _fetch_aggregated_trajectories(self, aggregated_queries: Dict[str, Dict]) -> List[Dict]:
        """åŸºäºèšåˆç»“æœè·å–è½¨è¿¹æ•°æ® - å‡å°‘é‡å¤æŸ¥è¯¢
        
        æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥è°ƒç”¨ç°æœ‰çš„è½¨è¿¹æŸ¥è¯¢æ–¹æ³•ï¼Œæš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
        """
        # TODO: é›†æˆç°æœ‰çš„è½¨è¿¹æŸ¥è¯¢åŠŸèƒ½
        logger.info("ğŸ”§ è½¨è¿¹æ•°æ®è·å–åŠŸèƒ½å¾…é›†æˆ...")
        
        # æ¨¡æ‹Ÿè¿”å›æ•°æ®ç»“æ„
        all_trajectory_data = []
        for dataset_name, time_range in aggregated_queries.items():
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ç°æœ‰çš„datasetæŸ¥è¯¢æ–¹æ³•
            # æš‚æ—¶åˆ›å»ºæ¨¡æ‹Ÿçš„LineString
            from shapely.geometry import LineString
            
            # æ¨¡æ‹Ÿè½¨è¿¹ç‚¹ï¼ˆå®é™…åº”è¯¥ä»æ•°æ®åº“æŸ¥è¯¢ï¼‰
            mock_coords = [
                (116.3, 39.9), (116.31, 39.91), (116.32, 39.92)  # åŒ—äº¬é™„è¿‘åæ ‡
            ]
            trajectory_linestring = LineString(mock_coords)
            
            all_trajectory_data.append({
                'dataset_name': dataset_name,
                'linestring': trajectory_linestring,
                'time_range': time_range,
                'point_count': len(mock_coords)
            })
        
        return all_trajectory_data
    
    def _execute_lightweight_polygon_query(self, merged_polygons: List[Dict]) -> Optional[pd.DataFrame]:
        """è½»é‡åŒ–PolygonæŸ¥è¯¢ - ä»…è¿”å›è½¨è¿¹ç‚¹ï¼Œä¸æ„å»ºå®Œæ•´è½¨è¿¹
        
        æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥è°ƒç”¨ç°æœ‰çš„é«˜æ€§èƒ½æŸ¥è¯¢å¼•æ“ï¼Œæš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
        """
        if not merged_polygons:
            return None
        
        logger.info("ğŸ”§ è½»é‡åŒ–PolygonæŸ¥è¯¢åŠŸèƒ½å¾…é›†æˆ...")
        
        # TODO: è°ƒç”¨ç°æœ‰çš„HighPerformancePolygonTrajectoryQuery
        # points_df, query_stats = self.polygon_processor.query_intersecting_trajectory_points(merged_polygons)
        
        # æ¨¡æ‹Ÿè¿”å›è½¨è¿¹ç‚¹æ•°æ®
        mock_data = {
            'dataset_name': ['dataset_1', 'dataset_2'],
            'timestamp': [1739958971349, 1739958971350],
            'longitude': [116.3, 116.31],
            'latitude': [39.9, 39.91],
            'source_polygon_id': ['merged_polygon_0', 'merged_polygon_0']
        }
        
        return pd.DataFrame(mock_data)
    
    def _finalize_lightweight_results(self, trajectory_points: Optional[pd.DataFrame], 
                                     merged_polygons: List[Dict], stats: Dict) -> Dict:
        """è½»é‡åŒ–ç»“æœå¤„ç† - è¿”å›è½¨è¿¹ç‚¹å’Œpolygonæ˜ å°„"""
        if trajectory_points is None or trajectory_points.empty:
            stats['final_results'] = "æ— å‘ç°è½¨è¿¹ç‚¹"
            return stats
        
        # è½»é‡åŒ–è¾“å‡ºæ ¼å¼
        results = {
            'trajectory_points': trajectory_points.to_dict('records') if not trajectory_points.empty else [],
            'source_polygons': [
                {
                    'id': poly['id'],
                    'properties': poly['properties'],
                    'geometry_wkt': poly['geometry'].wkt if poly.get('geometry') else None
                }
                for poly in merged_polygons
            ],
            'summary': {
                'total_points': len(trajectory_points),
                'unique_datasets': trajectory_points['dataset_name'].nunique() if 'dataset_name' in trajectory_points.columns else 0,
                'polygon_sources': len(merged_polygons),
                'optimization_ratio': f"{stats.get('raw_polygon_count', 0)} â†’ {stats.get('merged_polygon_count', 0)}"
            },
            'stats': stats
        }
        
        # å¯é€‰çš„æ•°æ®åº“ä¿å­˜
        if self.config.output_table:
            logger.info(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°æ•°æ®åº“è¡¨: {self.config.output_table}")
            # TODO: å®ç°æ•°æ®åº“ä¿å­˜åŠŸèƒ½
        
        # å¯é€‰çš„GeoJSONä¿å­˜
        if self.config.output_geojson:
            logger.info(f"ğŸ’¾ å¯¼å‡ºGeoJSONæ–‡ä»¶: {self.config.output_geojson}")
            # TODO: å®ç°GeoJSONå¯¼å‡ºåŠŸèƒ½
        
        return results


# å¯¼å‡ºä¸»è¦ç±»
__all__ = [
    'MultimodalConfig',
    'MultimodalTrajectoryWorkflow',
    'ResultAggregator',
    'PolygonMerger'
]


# CLIæ”¯æŒ
if __name__ == '__main__':
    from .multimodal_cli import main
    main()
