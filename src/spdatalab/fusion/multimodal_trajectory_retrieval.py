"""å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢ç³»ç»Ÿ - ä¸»èåˆåˆ†ææ¨¡å—

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. MultimodalTrajectoryWorkflow - è½»é‡åŒ–åè°ƒå™¨ï¼Œæ™ºèƒ½èšåˆç­–ç•¥
2. ResultAggregator - æ™ºèƒ½èšåˆå™¨ï¼ˆdataset_name + æ—¶é—´çª—å£èšåˆï¼‰
3. PolygonMerger - Polygonåˆå¹¶ä¼˜åŒ–å™¨ï¼ˆé‡å åˆå¹¶ï¼‰
4. è½»é‡åŒ–å·¥ä½œæµï¼šè¿”å›è½¨è¿¹ç‚¹è€Œéå®Œæ•´è½¨è¿¹çº¿
5. æ˜ å°„ä¿æŒï¼šä¿ç•™polygonåˆ°æºæ•°æ®çš„å¯¹åº”å…³ç³»

å¤ç”¨ç°æœ‰æ¨¡å—çš„80%åŠŸèƒ½ï¼Œä¸“æ³¨äºèåˆåˆ†æå’Œæ™ºèƒ½ä¼˜åŒ–ã€‚
"""

import argparse
import json
import logging
import sys
import time
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import warnings

import geopandas as gpd
import pandas as pd
from shapely.geometry import LineString, Point, Polygon, shape
from shapely.ops import unary_union

# å¯¼å…¥ç°æœ‰æ¨¡å—ï¼ˆå¤ç”¨ç°æœ‰åŠŸèƒ½ï¼‰
from spdatalab.dataset.polygon_trajectory_query import HighPerformancePolygonTrajectoryQuery

# å¯¼å…¥åŸºç¡€æ•°æ®å¤„ç†ç»„ä»¶
from spdatalab.dataset.multimodal_data_retriever import (
    APIConfig,
    MultimodalRetriever,
    TrajectoryToPolygonConverter
)

# å¯¼å…¥ç°æœ‰é«˜æ€§èƒ½æŸ¥è¯¢å¼•æ“ï¼ˆå¤ç”¨80%åŠŸèƒ½ï¼‰
from spdatalab.dataset.polygon_trajectory_query import (
    HighPerformancePolygonTrajectoryQuery,
    PolygonTrajectoryConfig
)

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)

# æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)


@dataclass
class MultimodalConfig:
    """è½»é‡åŒ–ç ”å‘å·¥å…·é…ç½®"""
    
    # APIé…ç½®
    api_config: APIConfig
    
    # æ ¸å¿ƒå‚æ•°ï¼ˆç ”å‘åˆ†æä¼˜åŒ–ï¼‰
    max_search_results: int = 5             # é€‚åˆç ”å‘åˆ†æçš„é»˜è®¤å€¼ï¼ˆä¸APIç¤ºä¾‹ä¸€è‡´ï¼‰
    time_window_days: int = 30              # 30å¤©æ—¶é—´çª—å£
    buffer_distance: float = 10.0           # 10ç±³ç²¾ç¡®ç¼“å†²
    
    # APIé™åˆ¶ï¼ˆç¡¬çº¦æŸï¼‰
    max_single_query: int = 10000           # å•æ¬¡æŸ¥è¯¢ä¸Šé™
    max_total_query: int = 100000           # ç´¯è®¡æŸ¥è¯¢ä¸Šé™
    
    # èšåˆä¼˜åŒ–å‚æ•°
    overlap_threshold: float = 0.7          # Polygoné‡å åº¦é˜ˆå€¼
    time_window_hours: int = 24             # æ—¶é—´çª—å£èšåˆï¼ˆå°æ—¶ï¼‰
    
    # å¤ç”¨ç°æœ‰é…ç½®ï¼ˆç®€åŒ–ï¼‰
    polygon_config: Optional[PolygonTrajectoryConfig] = None
    
    # è¾“å‡ºé…ç½®
    output_table: Optional[str] = None
    output_geojson: Optional[str] = None
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if self.polygon_config is None:
            # ä½¿ç”¨é»˜è®¤çš„é«˜æ€§èƒ½é…ç½®
            self.polygon_config = PolygonTrajectoryConfig(
                batch_threshold=50,
                chunk_size=20,
                limit_per_polygon=15000
            )


class ResultAggregator:
    """å¤šæ¨¡æ€æŸ¥è¯¢ç»“æœèšåˆå™¨
    
    åŠŸèƒ½ï¼š
    - æŒ‰dataset_nameèšåˆï¼Œé¿å…é‡å¤æŸ¥è¯¢
    - æŒ‰æ—¶é—´çª—å£èšåˆï¼Œåˆå¹¶ç›¸è¿‘æ—¶é—´çš„æŸ¥è¯¢
    """
    
    def __init__(self, time_window_hours: int = 24):
        self.time_window_hours = time_window_hours
    
    def aggregate_by_dataset(self, search_results: List[Dict]) -> Dict[str, List[Dict]]:
        """æŒ‰dataset_nameèšåˆï¼Œé¿å…é‡å¤æŸ¥è¯¢
        
        Args:
            search_results: å¤šæ¨¡æ€æ£€ç´¢ç»“æœ
            
        Returns:
            æŒ‰dataset_nameåˆ†ç»„çš„ç»“æœ
        """
        if not search_results:
            return {}
        
        dataset_groups = defaultdict(list)
        for result in search_results:
            dataset_name = result.get('dataset_name', 'unknown')
            dataset_groups[dataset_name].append(result)
        
        logger.info(f"ğŸ“Š Datasetèšåˆ: {len(search_results)}æ¡ç»“æœ â†’ {len(dataset_groups)}ä¸ªæ•°æ®é›†")
        
        # åœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†çš„æ•°æ®é›†ä¿¡æ¯
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug("ğŸ“‹ èšåˆçš„æ•°æ®é›†è¯¦æƒ…:")
            for dataset_name, items in dataset_groups.items():
                logger.debug(f"   ğŸ“ {dataset_name}: {len(items)}æ¡ç»“æœ")
                for i, item in enumerate(items[:3]):  # åªæ˜¾ç¤ºå‰3æ¡
                    similarity = item.get('similarity', 0)
                    timestamp = item.get('timestamp', 0)
                    logger.debug(f"      {i+1}. ç›¸ä¼¼åº¦={similarity:.3f}, æ—¶é—´æˆ³={timestamp}")
                if len(items) > 3:
                    logger.debug(f"      ... è¿˜æœ‰{len(items)-3}æ¡ç»“æœ")
        
        return dict(dataset_groups)
    
    def aggregate_by_timewindow(self, dataset_groups: Dict[str, List[Dict]]) -> Dict[str, Dict]:
        """æŒ‰æ—¶é—´çª—å£èšåˆï¼Œåˆå¹¶ç›¸è¿‘æ—¶é—´çš„æŸ¥è¯¢
        
        Args:
            dataset_groups: æŒ‰datasetåˆ†ç»„çš„ç»“æœ
            
        Returns:
            èšåˆåçš„æ—¶é—´èŒƒå›´æŸ¥è¯¢å‚æ•°
        """
        aggregated_queries = {}
        
        for dataset_name, results in dataset_groups.items():
            if not results:
                continue
            
            # æå–æ—¶é—´æˆ³
            timestamps = [r.get('timestamp', 0) for r in results if r.get('timestamp')]
            if not timestamps:
                # å¦‚æœæ²¡æœ‰æ—¶é—´æˆ³ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                current_ts = int(datetime.now().timestamp() * 1000)
                timestamps = [current_ts]
            
            # è®¡ç®—æ—¶é—´çª—å£
            min_timestamp = min(timestamps)
            max_timestamp = max(timestamps)
            
            # æ‰©å±•æ—¶é—´çª—å£ï¼ˆé»˜è®¤å‰åå„å»¶ä¼¸12å°æ—¶ï¼‰
            window_ms = self.time_window_hours * 60 * 60 * 1000 // 2  # å•è¾¹çª—å£
            start_time = min_timestamp - window_ms
            end_time = max_timestamp + window_ms
            
            aggregated_queries[dataset_name] = {
                'start_time': start_time,
                'end_time': end_time,
                'original_timestamps': timestamps,
                'result_count': len(results)
            }
        
        logger.info(f"â° æ—¶é—´çª—å£èšåˆ: ç”Ÿæˆ{len(aggregated_queries)}ä¸ªæ—¶é—´èŒƒå›´æŸ¥è¯¢")
        return aggregated_queries


class PolygonMerger:
    """Polygonåˆå¹¶ä¼˜åŒ–å™¨
    
    åŠŸèƒ½ï¼š
    - åˆå¹¶é‡å åº¦é«˜çš„polygonï¼Œä¿ç•™æºæ•°æ®æ˜ å°„
    - ä¼˜åŒ–æŸ¥è¯¢æ•ˆç‡ï¼Œå‡å°‘æ•°æ®åº“è®¿é—®
    """
    
    def __init__(self, overlap_threshold: float = 0.7):
        self.overlap_threshold = overlap_threshold  # é‡å åº¦é˜ˆå€¼
    
    def merge_overlapping_polygons(self, polygons_with_source: List[Dict]) -> List[Dict]:
        """åˆå¹¶é‡å åº¦é«˜çš„polygonï¼Œä¿ç•™æºæ•°æ®æ˜ å°„
        
        Args:
            polygons_with_source: polygonåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«geometryå’Œproperties
            
        Returns:
            åˆå¹¶åçš„polygonåˆ—è¡¨ï¼Œä¿æŒsourceæ˜ å°„
        """
        if not polygons_with_source:
            return []
        
        logger.info(f"ğŸ”„ å¼€å§‹Polygonåˆå¹¶ä¼˜åŒ–: {len(polygons_with_source)}ä¸ªåŸå§‹Polygon")
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        polygons = []
        for i, item in enumerate(polygons_with_source):
            geom = item.get('geometry')
            if isinstance(geom, Polygon) and geom.is_valid:
                polygons.append({
                    'id': item.get('id', f'polygon_{i}'),
                    'geometry': geom,
                    'sources': [item.get('properties', {})],  # åˆå§‹sourceåˆ—è¡¨
                    'merged': False
                })
        
        if not polygons:
            return []
        
        # ä½¿ç”¨ç®€å•çš„è´ªå¿ƒåˆå¹¶ç®—æ³•
        merged_polygons = []
        processed = set()
        
        for i, poly1 in enumerate(polygons):
            if i in processed:
                continue
            
            # åˆå§‹åŒ–åˆå¹¶ç»„
            merge_group = [poly1]
            current_geom = poly1['geometry']
            processed.add(i)
            
            # æŸ¥æ‰¾å¯åˆå¹¶çš„polygon
            for j, poly2 in enumerate(polygons[i+1:], i+1):
                if j in processed:
                    continue
                
                overlap_ratio = self.calculate_overlap_ratio(current_geom, poly2['geometry'])
                if overlap_ratio >= self.overlap_threshold:
                    merge_group.append(poly2)
                    processed.add(j)
                    # æ›´æ–°åˆå¹¶å‡ ä½•ï¼ˆç®€å•unionï¼‰
                    try:
                        current_geom = current_geom.union(poly2['geometry'])
                    except Exception as e:
                        logger.warning(f"Polygonåˆå¹¶å¤±è´¥: {e}")
            
            # åˆ›å»ºåˆå¹¶ç»“æœ
            if len(merge_group) > 1:
                # å¤šä¸ªpolygonåˆå¹¶
                all_sources = []
                for poly in merge_group:
                    all_sources.extend(poly['sources'])
                
                merged_polygons.append({
                    'id': f"merged_polygon_{len(merged_polygons)}",
                    'geometry': current_geom,
                    'properties': {
                        'merged_count': len(merge_group),
                        'sources': all_sources,
                        'merge_type': 'overlapping'
                    }
                })
            else:
                # å•ä¸ªpolygonä¿æŒä¸å˜
                merged_polygons.append({
                    'id': poly1['id'],
                    'geometry': poly1['geometry'],
                    'properties': {
                        'merged_count': 1,
                        'sources': poly1['sources'],
                        'merge_type': 'original'
                    }
                })
        
        optimization_ratio = f"{len(polygons_with_source)} â†’ {len(merged_polygons)}"
        logger.info(f"âœ… Polygonåˆå¹¶å®Œæˆ: {optimization_ratio} (å‹ç¼©ç‡: {1 - len(merged_polygons)/len(polygons_with_source):.1%})")
        
        return merged_polygons
    
    def calculate_overlap_ratio(self, poly1: Polygon, poly2: Polygon) -> float:
        """è®¡ç®—ä¸¤ä¸ªpolygonçš„é‡å æ¯”ä¾‹
        
        Args:
            poly1, poly2: å¾…æ¯”è¾ƒçš„polygon
            
        Returns:
            é‡å æ¯”ä¾‹ (0.0 - 1.0)
        """
        try:
            if not poly1.intersects(poly2):
                return 0.0
            
            intersection_area = poly1.intersection(poly2).area
            union_area = poly1.union(poly2).area
            
            return intersection_area / union_area if union_area > 0 else 0.0
        except Exception:
            return 0.0


class MultimodalTrajectoryWorkflow:
    """å¤šæ¨¡æ€è½¨è¿¹æ£€ç´¢å·¥ä½œæµ
    
    è½»é‡åŒ–åè°ƒå™¨ï¼š
    - æ™ºèƒ½èšåˆï¼šdataset_nameå’Œæ—¶é—´çª—å£èšåˆ
    - Polygonä¼˜åŒ–ï¼šé‡å polygonåˆå¹¶
    - è½»é‡è¾“å‡ºï¼šè¿”å›è½¨è¿¹ç‚¹è€Œéå®Œæ•´è½¨è¿¹  
    - æ˜ å°„ä¿æŒï¼šä¿ç•™polygonåˆ°æºæ•°æ®çš„å¯¹åº”å…³ç³»
    """
    
    def __init__(self, config: MultimodalConfig):
        # ç»„åˆç°æœ‰å’Œæ–°å¢ç»„ä»¶
        self.config = config
        self.retriever = MultimodalRetriever(config.api_config)
        self.converter = TrajectoryToPolygonConverter(config.buffer_distance)
        self.aggregator = ResultAggregator(config.time_window_hours)         # æ–°å¢ï¼šç»“æœèšåˆå™¨
        self.polygon_merger = PolygonMerger(config.overlap_threshold)        # æ–°å¢ï¼špolygonåˆå¹¶å™¨
        self.polygon_processor = HighPerformancePolygonTrajectoryQuery(config.polygon_config)
    
    def process_text_query(self, text: str, collection: str, count: Optional[int] = None, 
                          start: int = 0, start_time: Optional[int] = None, 
                          end_time: Optional[int] = None, **kwargs) -> Dict:
        """ä¼˜åŒ–çš„æ–‡æœ¬æŸ¥è¯¢æµç¨‹
        
        Args:
            text: æŸ¥è¯¢æ–‡æœ¬
            collection: ç›¸æœºcollection
            count: æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
            start: èµ·å§‹åç§»é‡ï¼Œé»˜è®¤0
            start_time: äº‹ä»¶å¼€å§‹æ—¶é—´ï¼Œ13ä½æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
            end_time: äº‹ä»¶ç»“æŸæ—¶é—´ï¼Œ13ä½æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è½»é‡åŒ–æŸ¥è¯¢ç»“æœ
        """
        if count is None:
            count = self.config.max_search_results
        
        return self._execute_optimized_workflow(
            retrieval_func=lambda: self.retriever.retrieve_by_text(
                text=text,
                collection=collection,
                count=count,
                start=start,
                start_time=start_time,
                end_time=end_time
            ),
            query_type="text",
            query_content=text,
            collection=collection,
            **kwargs
        )
    
    def process_image_query(self, images: List[str], collection: str, count: Optional[int] = None,
                           start: int = 0, start_time: Optional[int] = None,
                           end_time: Optional[int] = None, **kwargs) -> Dict:
        """ä¼˜åŒ–çš„å›¾ç‰‡æŸ¥è¯¢æµç¨‹
        
        Args:
            images: å›¾ç‰‡base64ç¼–ç åçš„å­—ç¬¦ä¸²åˆ—è¡¨
            collection: ç›¸æœºcollection
            count: æŸ¥è¯¢æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®å€¼ï¼‰
            start: èµ·å§‹åç§»é‡ï¼Œé»˜è®¤0
            start_time: äº‹ä»¶å¼€å§‹æ—¶é—´ï¼Œ13ä½æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
            end_time: äº‹ä»¶ç»“æŸæ—¶é—´ï¼Œ13ä½æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è½»é‡åŒ–æŸ¥è¯¢ç»“æœ
        """
        if count is None:
            count = self.config.max_search_results
        
        return self._execute_optimized_workflow(
            retrieval_func=lambda: self.retriever.retrieve_by_images(
                images=images,
                collection=collection,
                count=count,
                start=start,
                start_time=start_time,
                end_time=end_time
            ),
            query_type="image",
            query_content=f"{len(images)}å¼ å›¾ç‰‡",
            collection=collection,
            **kwargs
        )
    
    def _execute_optimized_workflow(self, retrieval_func, query_type: str, query_content: str, 
                                   collection: str, **kwargs) -> Dict:
        """ä¼˜åŒ–çš„å·¥ä½œæµå¼•æ“ï¼ŒåŒ…å«æ™ºèƒ½èšåˆç­–ç•¥"""
        workflow_start = time.time()
        stats = {
            'query_type': query_type,
            'query_content': query_content,
            'collection': collection,
            'start_time': datetime.now(),
            'config': {
                'buffer_distance': self.config.buffer_distance,
                'time_window_days': self.config.time_window_days,
                'overlap_threshold': self.config.overlap_threshold
            }
        }
        
        try:
            # Stage 1: å¤šæ¨¡æ€æ£€ç´¢
            logger.info("ğŸ” Stage 1: æ‰§è¡Œå¤šæ¨¡æ€æ£€ç´¢...")
            search_results = retrieval_func()
            stats['search_results_count'] = len(search_results)
            
            if not search_results:
                return self._handle_no_results(stats)
            
            # Stage 2: æ™ºèƒ½èšåˆ (æ–°å¢ä¼˜åŒ–ï¼)
            aggregation_start = time.time()
            logger.info(f"ğŸ“Š Stage 2: æ™ºèƒ½èšåˆ {len(search_results)} ä¸ªæ£€ç´¢ç»“æœ...")
            aggregated_datasets = self.aggregator.aggregate_by_dataset(search_results)
            aggregated_queries = self.aggregator.aggregate_by_timewindow(aggregated_datasets)
            
            # å¢å¼ºç»Ÿè®¡ä¿¡æ¯æ”¶é›†
            aggregation_time = time.time() - aggregation_start
            stats.update({
                'aggregated_datasets': len(aggregated_datasets),
                'aggregated_queries': len(aggregated_queries),
                'aggregation_time': aggregation_time,
                'aggregation_efficiency': {
                    'original_results': len(search_results),
                    'aggregated_datasets': len(aggregated_datasets),
                    'aggregated_queries': len(aggregated_queries),
                    'query_reduction_ratio': (len(search_results) - len(aggregated_queries)) / len(search_results) if len(search_results) > 0 else 0
                }
            })
            
            # æ·»åŠ æ•°æ®é›†è¯¦æƒ…ç”¨äºverboseæ¨¡å¼æ˜¾ç¤º
            dataset_details = {}
            similarity_stats = {'min': 1.0, 'max': 0.0, 'avg': 0.0}
            timestamps = []
            
            for dataset_name, results in aggregated_datasets.items():
                dataset_details[dataset_name] = len(results)
                # æ”¶é›†ç›¸ä¼¼åº¦å’Œæ—¶é—´æˆ³ç»Ÿè®¡
                for result in results:
                    similarity = result.get('similarity', 0)
                    similarity_stats['min'] = min(similarity_stats['min'], similarity)
                    similarity_stats['max'] = max(similarity_stats['max'], similarity)
                    timestamps.append(result.get('timestamp', 0))
            
            if search_results:
                similarities = [r.get('similarity', 0) for r in search_results]
                similarity_stats['avg'] = sum(similarities) / len(similarities)
            
            stats.update({
                'dataset_details': dataset_details,
                'similarity_stats': similarity_stats,
                'time_range_stats': {
                    'earliest': min(timestamps) if timestamps else 0,
                    'latest': max(timestamps) if timestamps else 0,
                    'span_hours': (max(timestamps) - min(timestamps)) / (1000 * 3600) if len(timestamps) > 1 else 0
                }
            })
            
            # Stage 3: è½¨è¿¹æ•°æ®è·å– (ä¼˜åŒ–åï¼Œå‡å°‘é‡å¤æŸ¥è¯¢)
            logger.info(f"ğŸš€ Stage 3: æ‰¹é‡è·å– {len(aggregated_datasets)} ä¸ªæ•°æ®é›†è½¨è¿¹...")
            trajectory_data = self._fetch_aggregated_trajectories(aggregated_queries)
            stats['trajectory_data_count'] = len(trajectory_data)
            
            if not trajectory_data:
                return self._handle_no_trajectories(stats)
            
            # Stage 4: Polygonè½¬æ¢å’Œåˆå¹¶ (æ–°å¢åˆå¹¶ä¼˜åŒ–ï¼)
            polygon_start = time.time()
            logger.info(f"ğŸ”„ Stage 4: è½¬æ¢è½¨è¿¹ä¸ºPolygonå¹¶æ™ºèƒ½åˆå¹¶...")
            raw_polygons = self.converter.batch_convert(trajectory_data)
            merged_polygons = self.polygon_merger.merge_overlapping_polygons(raw_polygons)
            
            # å¢å¼ºpolygonå¤„ç†ç»Ÿè®¡
            polygon_time = time.time() - polygon_start
            compression_ratio = ((len(raw_polygons) - len(merged_polygons)) / len(raw_polygons) * 100) if len(raw_polygons) > 0 else 0
            
            stats.update({
                'raw_polygon_count': len(raw_polygons),
                'merged_polygon_count': len(merged_polygons),
                'polygon_processing_time': polygon_time,
                'polygon_optimization': {
                    'compression_ratio': compression_ratio,
                    'polygons_eliminated': len(raw_polygons) - len(merged_polygons),
                    'efficiency_gain': compression_ratio / 100 if compression_ratio > 0 else 0
                }
            })
            
            if not merged_polygons:
                return self._handle_no_polygons(stats)
            
            # Stage 5: è½»é‡åŒ–PolygonæŸ¥è¯¢ (ä»…è¿”å›è½¨è¿¹ç‚¹ï¼)
            query_start = time.time()
            logger.info(f"âš¡ Stage 5: åŸºäº {len(merged_polygons)} ä¸ªPolygonæŸ¥è¯¢è½¨è¿¹ç‚¹...")
            trajectory_points = self._execute_lightweight_polygon_query(merged_polygons)
            
            # å¢å¼ºæŸ¥è¯¢ç»“æœç»Ÿè®¡
            query_time = time.time() - query_start
            points_count = len(trajectory_points) if trajectory_points is not None else 0
            
            stats.update({
                'discovered_points_count': points_count,
                'trajectory_query_time': query_time,
                'query_performance': {
                    'points_per_polygon': points_count / len(merged_polygons) if len(merged_polygons) > 0 else 0,
                    'points_per_second': points_count / query_time if query_time > 0 else 0,
                    'unique_datasets_discovered': trajectory_points['dataset_name'].nunique() if trajectory_points is not None and not trajectory_points.empty else 0
                }
            })
            
            # Stage 6: è½»é‡åŒ–ç»“æœè¾“å‡º
            logger.info("ğŸ’¾ Stage 6: è½»é‡åŒ–ç»“æœè¾“å‡º...")
            final_results = self._finalize_lightweight_results(trajectory_points, merged_polygons, stats)
            
            stats['success'] = True
            stats['total_duration'] = time.time() - workflow_start
            
            return final_results
            
        except Exception as e:
            stats['error'] = str(e)
            stats['success'] = False
            stats['total_duration'] = time.time() - workflow_start
            logger.error(f"âŒ ä¼˜åŒ–å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            return stats
    
    def _handle_no_results(self, stats: Dict) -> Dict:
        """å¤„ç†æ— æ£€ç´¢ç»“æœçš„æƒ…å†µ"""
        logger.warning("âš ï¸ å¤šæ¨¡æ€æ£€ç´¢æœªè¿”å›ä»»ä½•ç»“æœ")
        stats['message'] = "å¤šæ¨¡æ€æ£€ç´¢æœªè¿”å›ä»»ä½•ç»“æœï¼Œè¯·å°è¯•è°ƒæ•´æŸ¥è¯¢æ¡ä»¶"
        stats['success'] = False
        return stats
    
    def _handle_no_trajectories(self, stats: Dict) -> Dict:
        """å¤„ç†æ— è½¨è¿¹æ•°æ®çš„æƒ…å†µ"""
        logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸åº”çš„è½¨è¿¹æ•°æ®")
        stats['message'] = "æ ¹æ®æ£€ç´¢ç»“æœæœªæ‰¾åˆ°ç›¸åº”çš„è½¨è¿¹æ•°æ®"
        stats['success'] = False
        return stats
    
    def _handle_no_polygons(self, stats: Dict) -> Dict:
        """å¤„ç†æ— æœ‰æ•ˆPolygonçš„æƒ…å†µ"""
        logger.warning("âš ï¸ è½¨è¿¹è½¬æ¢ä¸ºPolygonå¤±è´¥")
        stats['message'] = "è½¨è¿¹æ•°æ®è½¬æ¢ä¸ºPolygonå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡"
        stats['success'] = False
        return stats
    
    def _fetch_aggregated_trajectories(self, aggregated_queries: Dict[str, Dict]) -> List[Dict]:
        """åŸºäºèšåˆç»“æœè·å–è½¨è¿¹æ•°æ® - å¤ç”¨ç°æœ‰çš„å®Œæ•´è½¨è¿¹æŸ¥è¯¢åŠŸèƒ½
        
        å¤ç”¨HighPerformancePolygonTrajectoryQuery._fetch_complete_trajectoriesæ–¹æ³•
        ç¡®ä¿80%+ä»£ç å¤ç”¨åŸåˆ™
        """
        if not aggregated_queries:
            logger.warning("âš ï¸ æ²¡æœ‰èšåˆæŸ¥è¯¢æ•°æ®")
            return []
        
        logger.info(f"ğŸš€ å¼€å§‹è·å–è½¨è¿¹æ•°æ®: {len(aggregated_queries)} ä¸ªæ•°æ®é›†")
        
        try:
            # æ„å»ºæ¨¡æ‹Ÿçš„intersection_result_dfï¼Œè®©ç°æœ‰æ–¹æ³•èƒ½å¤„ç†
            dataset_names = list(aggregated_queries.keys())
            
            # åˆ›å»ºç®€å•çš„DataFrameæ¥è§¦å‘ç°æœ‰çš„è½¨è¿¹æŸ¥è¯¢åŠŸèƒ½
            import pandas as pd
            intersection_result_df = pd.DataFrame({
                'dataset_name': dataset_names,
                'timestamp': [time_range.get('start_time', 0) for time_range in aggregated_queries.values()]
            })
            
            logger.info(f"ğŸ“‹ å¤ç”¨ç°æœ‰è½¨è¿¹æŸ¥è¯¢æ–¹æ³•è·å– {len(dataset_names)} ä¸ªæ•°æ®é›†è½¨è¿¹...")
            
            # å¤ç”¨ç°æœ‰çš„å®Œæ•´è½¨è¿¹æŸ¥è¯¢åŠŸèƒ½ - 80%å¤ç”¨åŸåˆ™
            complete_trajectory_df, complete_stats = self.polygon_processor._fetch_complete_trajectories(intersection_result_df)
            
            if complete_trajectory_df.empty:
                logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•è½¨è¿¹æ•°æ®")
                return []
            
            logger.info(f"âœ… è½¨è¿¹æ•°æ®è·å–æˆåŠŸ: {len(complete_trajectory_df)} ä¸ªè½¨è¿¹ç‚¹")
            logger.info(f"ğŸ“Š è·å–ç»Ÿè®¡: æ•°æ®é›†æ•°={complete_stats.get('complete_datasets', 0)}, "
                       f"è½¨è¿¹ç‚¹æ•°={complete_stats.get('complete_points', 0)}, "
                       f"ç”¨æ—¶={complete_stats.get('complete_query_time', 0):.2f}s")
            
            # å°†DataFrameè½¬æ¢ä¸ºLineStringåˆ—è¡¨
            all_trajectory_data = self._convert_dataframe_to_linestrings(complete_trajectory_df, aggregated_queries)
            
            return all_trajectory_data
            
        except Exception as e:
            logger.error(f"âŒ è½¨è¿¹æ•°æ®è·å–å¤±è´¥: {e}")
            return []
    
    def _convert_dataframe_to_linestrings(self, trajectory_df: pd.DataFrame, 
                                        aggregated_queries: Dict[str, Dict]) -> List[Dict]:
        """å°†è½¨è¿¹DataFrameè½¬æ¢ä¸ºLineStringåˆ—è¡¨
        
        Args:
            trajectory_df: ä»æ•°æ®åº“æŸ¥è¯¢åˆ°çš„è½¨è¿¹ç‚¹DataFrame
            aggregated_queries: èšåˆæŸ¥è¯¢å‚æ•°
            
        Returns:
            åŒ…å«LineStringå‡ ä½•çš„è½¨è¿¹æ•°æ®åˆ—è¡¨
        """
        if trajectory_df.empty:
            return []
        
        logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢ {len(trajectory_df)} ä¸ªè½¨è¿¹ç‚¹ä¸ºLineString...")
        
        try:
            from shapely.geometry import LineString
            
            all_trajectory_data = []
            
            # æŒ‰dataset_nameåˆ†ç»„å¤„ç†
            grouped = trajectory_df.groupby('dataset_name')
            
            for dataset_name, group in grouped:
                if len(group) < 2:
                    logger.debug(f"âš ï¸ æ•°æ®é›† {dataset_name} ç‚¹æ•°é‡ä¸è¶³({len(group)})ï¼Œè·³è¿‡LineStringæ„å»º")
                    continue
                
                # æŒ‰æ—¶é—´æ’åº
                group = group.sort_values('timestamp')
                
                # æå–åæ ‡ç‚¹
                coordinates = list(zip(group['longitude'], group['latitude']))
                
                # åˆ›å»ºLineString
                trajectory_linestring = LineString(coordinates)
                
                # è·å–æ—¶é—´èŒƒå›´ä¿¡æ¯
                time_range = aggregated_queries.get(dataset_name, {})
                
                # æ„å»ºè½¨è¿¹æ•°æ®
                trajectory_data = {
                    'dataset_name': dataset_name,
                    'linestring': trajectory_linestring,
                    'time_range': time_range,
                    'point_count': len(group),
                    'start_time': int(group['timestamp'].min()),
                    'end_time': int(group['timestamp'].max()),
                    'duration': int(group['timestamp'].max() - group['timestamp'].min())
                }
                
                all_trajectory_data.append(trajectory_data)
                
                logger.debug(f"âœ… è½¬æ¢è½¨è¿¹: {dataset_name}, ç‚¹æ•°: {len(group)}, "
                           f"æ—¶é•¿: {trajectory_data['duration']//1000:.1f}s")
            
            logger.info(f"âœ… LineStringè½¬æ¢å®Œæˆ: {len(all_trajectory_data)} æ¡è½¨è¿¹")
            
            return all_trajectory_data
            
        except Exception as e:
            logger.error(f"âŒ LineStringè½¬æ¢å¤±è´¥: {e}")
            return []
    
    def _execute_lightweight_polygon_query(self, merged_polygons: List[Dict]) -> Optional[pd.DataFrame]:
        """è½»é‡åŒ–PolygonæŸ¥è¯¢ - å¤ç”¨ç°æœ‰é«˜æ€§èƒ½æŸ¥è¯¢å¼•æ“
        
        å¤ç”¨HighPerformancePolygonTrajectoryQuery.query_intersecting_trajectory_pointsæ–¹æ³•
        ç¡®ä¿80%+ä»£ç å¤ç”¨åŸåˆ™
        """
        if not merged_polygons:
            logger.warning("âš ï¸ æ²¡æœ‰polygonæ•°æ®éœ€è¦æŸ¥è¯¢")
            return None
        
        logger.info(f"âš¡ å¼€å§‹è½»é‡åŒ–PolygonæŸ¥è¯¢: {len(merged_polygons)} ä¸ªpolygon")
        
        try:
            # å¤ç”¨ç°æœ‰çš„é«˜æ€§èƒ½æŸ¥è¯¢å¼•æ“ - 80%å¤ç”¨åŸåˆ™
            points_df, query_stats = self.polygon_processor.query_intersecting_trajectory_points(merged_polygons)
            
            if points_df.empty:
                logger.warning("âš ï¸ æœªæŸ¥è¯¢åˆ°ç›¸äº¤çš„è½¨è¿¹ç‚¹")
                return None
            
            logger.info(f"âœ… è½»é‡åŒ–æŸ¥è¯¢æˆåŠŸ: {len(points_df)} ä¸ªè½¨è¿¹ç‚¹")
            logger.info(f"ğŸ“Š æŸ¥è¯¢ç»Ÿè®¡: ç­–ç•¥={query_stats.get('strategy', 'unknown')}, "
                       f"ç”¨æ—¶={query_stats.get('query_time', 0):.2f}s, "
                       f"æ•°æ®é›†æ•°={query_stats.get('unique_datasets', 0)}")
            
            # æ·»åŠ æºpolygonæ˜ å°„ä¿¡æ¯
            points_df = self._add_polygon_mapping(points_df, merged_polygons)
            
            return points_df
            
        except Exception as e:
            logger.error(f"âŒ è½»é‡åŒ–PolygonæŸ¥è¯¢å¤±è´¥: {e}")
            # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›Noneè€Œä¸æ˜¯mockæ•°æ®
            return None
    
    def _add_polygon_mapping(self, points_df: pd.DataFrame, merged_polygons: List[Dict]) -> pd.DataFrame:
        """ä¸ºè½¨è¿¹ç‚¹æ·»åŠ æºpolygonæ˜ å°„ä¿¡æ¯
        
        Args:
            points_df: æŸ¥è¯¢åˆ°çš„è½¨è¿¹ç‚¹DataFrame
            merged_polygons: åˆå¹¶åçš„polygonåˆ—è¡¨
            
        Returns:
            æ·»åŠ äº†source_polygonså­—æ®µçš„DataFrame
        """
        if points_df.empty or not merged_polygons:
            return points_df
        
        logger.info(f"ğŸ”— å¼€å§‹è®¡ç®—è½¨è¿¹ç‚¹åˆ°polygonçš„æ˜ å°„å…³ç³»...")
        
        try:
            from shapely.geometry import Point
            
            # ä¸ºæ¯ä¸ªè½¨è¿¹ç‚¹åˆ›å»ºPointå‡ ä½•
            points_geometry = [Point(row['longitude'], row['latitude']) 
                             for _, row in points_df.iterrows()]
            
            # åˆå§‹åŒ–source_polygonsåˆ—
            source_polygons = []
            
            for i, point_geom in enumerate(points_geometry):
                matched_sources = []
                
                # æ£€æŸ¥ä¸å“ªäº›polygonç›¸äº¤
                for polygon_data in merged_polygons:
                    polygon_geom = polygon_data['geometry']
                    
                    # ç©ºé—´ç›¸äº¤æ£€æŸ¥
                    if point_geom.within(polygon_geom) or point_geom.intersects(polygon_geom):
                        # è·å–æºæ•°æ®ä¿¡æ¯
                        sources = polygon_data.get('sources', [])
                        for source in sources:
                            dataset_name = source.get('dataset_name', 'unknown')
                            timestamp = source.get('timestamp', 0)
                            matched_sources.append(f"{dataset_name}:{timestamp}")
                
                # æ ¼å¼åŒ–æ˜ å°„ä¿¡æ¯
                if matched_sources:
                    source_polygons.append(','.join(matched_sources))
                else:
                    # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨polygon IDä½œä¸ºåå¤‡
                    polygon_id = merged_polygons[0].get('id', 'unknown_polygon')
                    source_polygons.append(f"polygon_{polygon_id}")
            
            # æ·»åŠ åˆ°DataFrame
            points_df = points_df.copy()
            points_df['source_polygons'] = source_polygons
            
            logger.info(f"âœ… æ˜ å°„å…³ç³»è®¡ç®—å®Œæˆ: {len(points_df)} ä¸ªè½¨è¿¹ç‚¹å·²æ·»åŠ polygonæ˜ å°„ä¿¡æ¯")
            
            return points_df
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ polygonæ˜ å°„å¤±è´¥: {e}")
            # æ·»åŠ é»˜è®¤æ˜ å°„ä¿¡æ¯
            points_df = points_df.copy()
            points_df['source_polygons'] = 'mapping_failed'
            return points_df
    
    def _finalize_lightweight_results(self, trajectory_points: Optional[pd.DataFrame], 
                                     merged_polygons: List[Dict], stats: Dict) -> Dict:
        """è½»é‡åŒ–ç»“æœå¤„ç† - è¿”å›è½¨è¿¹ç‚¹å’Œpolygonæ˜ å°„"""
        if trajectory_points is None or trajectory_points.empty:
            stats['final_results'] = "æ— å‘ç°è½¨è¿¹ç‚¹"
            return stats
        
        # è½»é‡åŒ–è¾“å‡ºæ ¼å¼
        results = {
            'trajectory_points': trajectory_points.to_dict('records') if not trajectory_points.empty else [],
            'source_polygons': [
                {
                    'id': poly['id'],
                    'properties': poly['properties'],
                    'geometry_wkt': poly['geometry'].wkt if poly.get('geometry') else None
                }
                for poly in merged_polygons
            ],
            'summary': {
                'total_points': len(trajectory_points),
                'unique_datasets': trajectory_points['dataset_name'].nunique() if 'dataset_name' in trajectory_points.columns else 0,
                'polygon_sources': len(merged_polygons),
                'optimization_ratio': f"{stats.get('raw_polygon_count', 0)} â†’ {stats.get('merged_polygon_count', 0)}"
            },
            'stats': stats
        }
        
        # å¯é€‰çš„æ•°æ®åº“ä¿å­˜ï¼ˆå¤ç”¨ç°æœ‰åŠŸèƒ½ï¼‰
        if self.config.output_table:
            logger.info(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°æ•°æ®åº“è¡¨: {self.config.output_table}")
            try:
                # å°†è½¨è¿¹ç‚¹è½¬æ¢ä¸ºç°æœ‰ä¿å­˜æ–¹æ³•æœŸæœ›çš„æ ¼å¼
                trajectory_data = self._convert_points_to_trajectory_format(trajectory_points, stats)
                if trajectory_data:
                    # å¤ç”¨ç°æœ‰çš„é«˜æ€§èƒ½ä¿å­˜æ–¹æ³•
                    inserted_count, save_stats = self.polygon_processor.save_trajectories_to_table(
                        trajectory_data, self.config.output_table
                    )
                    stats['saved_to_database'] = inserted_count
                    stats['save_stats'] = save_stats
                    logger.info(f"âœ… æ•°æ®åº“ä¿å­˜æˆåŠŸ: {inserted_count} æ¡è½¨è¿¹")
                else:
                    logger.warning("âš ï¸ æ²¡æœ‰è½¨è¿¹æ•°æ®éœ€è¦ä¿å­˜")
                    stats['saved_to_database'] = 0
            except Exception as e:
                logger.error(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
                stats['database_save_error'] = str(e)
        
        # å¯é€‰çš„GeoJSONä¿å­˜
        if self.config.output_geojson:
            logger.info(f"ğŸ’¾ å¯¼å‡ºGeoJSONæ–‡ä»¶: {self.config.output_geojson}")
            # TODO: å®ç°GeoJSONå¯¼å‡ºåŠŸèƒ½
        
        return results
    
    def _convert_points_to_trajectory_format(self, trajectory_points: pd.DataFrame, stats: Dict) -> List[Dict]:
        """å°†è½¨è¿¹ç‚¹è½¬æ¢ä¸ºç°æœ‰ä¿å­˜æ–¹æ³•æœŸæœ›çš„æ ¼å¼ï¼ˆå¤ç”¨ç°æœ‰æ¶æ„ï¼‰
        
        Args:
            trajectory_points: å‘ç°çš„è½¨è¿¹ç‚¹DataFrame
            stats: ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            ç°æœ‰ä¿å­˜æ–¹æ³•æœŸæœ›çš„è½¨è¿¹æ•°æ®æ ¼å¼
        """
        if trajectory_points.empty:
            logger.warning("ğŸ“Š æ²¡æœ‰è½¨è¿¹ç‚¹æ•°æ®éœ€è¦è½¬æ¢")
            return []
        
        trajectories = []
        
        try:
            # æŒ‰dataset_nameåˆ†ç»„å¤„ç†ï¼ˆè½»é‡åŒ–èšåˆï¼‰
            grouped = trajectory_points.groupby('dataset_name')
            logger.info(f"ğŸ”„ è½¬æ¢ {len(grouped)} ä¸ªæ•°æ®é›†çš„è½¨è¿¹ç‚¹ä¸ºæ ‡å‡†æ ¼å¼...")
            
            for dataset_name, group in grouped:
                # è·³è¿‡ç‚¹æ•°è¿‡å°‘çš„æ•°æ®é›†
                if len(group) < 2:
                    logger.debug(f"âš ï¸ æ•°æ®é›† {dataset_name} ç‚¹æ•°é‡ä¸è¶³({len(group)})ï¼Œè·³è¿‡")
                    continue
                
                # æŒ‰æ—¶é—´æ’åº
                group = group.sort_values('timestamp')
                
                # æå–åæ ‡æ„å»ºLineStringå‡ ä½•
                coordinates = list(zip(group['longitude'], group['latitude']))
                from shapely.geometry import LineString
                trajectory_geom = LineString(coordinates)
                
                # æ„å»ºç¬¦åˆç°æœ‰æ ¼å¼çš„è½¨è¿¹æ•°æ®
                trajectory_data = {
                    'dataset_name': dataset_name,
                    'scene_id': group.get('scene_id', pd.Series([''])).iloc[0] if 'scene_id' in group.columns else '',
                    'event_id': group.get('event_id', pd.Series([None])).iloc[0] if 'event_id' in group.columns else None,
                    'event_name': group.get('event_name', pd.Series([''])).iloc[0] if 'event_name' in group.columns else '',
                    'start_time': int(group['timestamp'].min()),
                    'end_time': int(group['timestamp'].max()),
                    'duration': int(group['timestamp'].max() - group['timestamp'].min()),
                    'point_count': len(group),
                    'geometry': trajectory_geom,
                    
                    # å¤šæ¨¡æ€ç‰¹æœ‰å­—æ®µ
                    'query_type': stats.get('query_type', 'text'),
                    'query_content': stats.get('query_content', ''),
                    'collection': stats.get('collection', ''),
                    'source_polygons': group.get('source_polygons', pd.Series([''])).iloc[0] if 'source_polygons' in group.columns else '',
                    'optimization_ratio': f"{stats.get('raw_polygon_count', 0)}â†’{stats.get('merged_polygon_count', 0)}",
                    
                    # å¯é€‰çš„ç»Ÿè®¡å­—æ®µ
                    'polygon_ids': list(group.get('polygon_id', pd.Series(['unknown'])).unique()) if 'polygon_id' in group.columns else ['multimodal_discovery']
                }
                
                # æ·»åŠ é€Ÿåº¦ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if 'velocity' in group.columns:
                    velocity_data = group['velocity'].dropna()
                    if len(velocity_data) > 0:
                        trajectory_data.update({
                            'avg_speed': round(float(velocity_data.mean()), 2),
                            'max_speed': round(float(velocity_data.max()), 2),
                            'min_speed': round(float(velocity_data.min()), 2)
                        })
                
                trajectories.append(trajectory_data)
                
                logger.debug(f"âœ… è½¬æ¢è½¨è¿¹: {dataset_name}, ç‚¹æ•°: {len(group)}, "
                           f"æ—¶é•¿: {trajectory_data['duration']//1000:.1f}s")
            
            logger.info(f"âœ… è½¬æ¢å®Œæˆ: {len(trajectories)} æ¡è½¨è¿¹ï¼ŒåŸºäº {len(trajectory_points)} ä¸ªè½¨è¿¹ç‚¹")
            return trajectories
            
        except Exception as e:
            logger.error(f"âŒ è½¨è¿¹æ ¼å¼è½¬æ¢å¤±è´¥: {e}")
            raise


# å¯¼å‡ºä¸»è¦ç±»
__all__ = [
    'MultimodalConfig',
    'MultimodalTrajectoryWorkflow',
    'ResultAggregator',
    'PolygonMerger'
]


# CLIæ”¯æŒ
if __name__ == '__main__':
    from spdatalab.fusion.cli.multimodal import main

    raise SystemExit(main())
