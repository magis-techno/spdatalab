"""è½¨è¿¹ä¸è½¦é“ç©ºé—´å…³ç³»åˆ†ææ¨¡å—ã€‚

äºŒé˜¶æ®µåˆ†æçš„ç¬¬äºŒé˜¶æ®µï¼šåŸºäºtrajectory_road_analysisæ¨¡å—çš„è¾“å‡ºç»“æœï¼Œ
å¯¹é¢„ç­›é€‰çš„å€™é€‰è½¦é“è¿›è¡Œç²¾ç»†çš„ç©ºé—´å…³ç³»åˆ†æã€‚

ä¾èµ–å…³ç³»ï¼š
1. å¿…é¡»å…ˆè¿è¡Œ trajectory_road_analysis è·å¾—å€™é€‰è½¦é“
2. åŸºäº trajectory_road_lanes è¡¨ä¸­çš„å€™é€‰è½¦é“è¿›è¡Œåˆ†æ
3. é€šè¿‡ road_analysis_id å…³è”ä¸¤ä¸ªé˜¶æ®µçš„åˆ†æç»“æœ

ä¸»è¦åŠŸèƒ½ï¼š
- åŸºäºå€™é€‰è½¦é“è¿›è¡Œè½¨è¿¹åˆ†æ®µã€é‡‡æ ·å’Œç¼“å†²åŒºåˆ†æ
- æ™ºèƒ½è´¨é‡æ£€æŸ¥å’Œè½¨è¿¹é‡æ„
- è¯†åˆ«è½¨è¿¹åœ¨ä¸åŒè½¦é“ä¸Šçš„è¡Œé©¶ç‰¹å¾
"""

from __future__ import annotations
import argparse
import json
import signal
import sys
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Iterator, Tuple, Union, Any
import logging

import geopandas as gpd
import pandas as pd
import numpy as np
from sqlalchemy import text, create_engine
from shapely.geometry import LineString, Point
from shapely.ops import transform
import pyproj
from functools import partial

# å¯¼å…¥ç›¸å…³æ¨¡å—
from spdatalab.common.io_hive import hive_cursor
from spdatalab.dataset.trajectory import (
    load_scene_data_mappings,
    fetch_data_names_from_scene_ids,
    fetch_trajectory_points,
    setup_signal_handlers
)

# æ•°æ®åº“é…ç½®
LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"
POINT_TABLE = "ddi_data_points"  # Hiveä¸­çš„è½¨è¿¹ç‚¹è¡¨åï¼ˆä¸å¸¦schemaå‰ç¼€ï¼‰

# é»˜è®¤é…ç½®
DEFAULT_CONFIG = {
    # è¾“å…¥é…ç½®
    'input_format': 'scene_id_list',
    'polyline_output': True,
    
    # é‡‡æ ·é…ç½®
    'sampling_strategy': 'distance',  # 'distance', 'time', 'uniform'
    'distance_interval': 10.0,        # ç±³
    'time_interval': 5.0,             # ç§’
    'uniform_step': 50,               # ç‚¹æ•°
    
    # æ»‘çª—é…ç½®
    'window_size': 20,                # é‡‡æ ·ç‚¹æ•°
    'window_overlap': 0.5,            # é‡å ç‡
    
    # è½¦é“åˆ†æé…ç½®ï¼ˆåŸºäºtrajectory_road_analysisç»“æœï¼‰
    'road_analysis_lanes_table': 'trajectory_road_lanes',  # æ¥è‡ªtrajectory_road_analysisçš„laneç»“æœ
    'buffer_radius': 15.0,            # ç±³
    'max_lane_distance': 50.0,        # ç±³
    'points_limit_per_lane': 1000,    # æ¯ä¸ªlaneæœ€å¤šæŸ¥è¯¢çš„è½¨è¿¹ç‚¹æ•°
    'enable_time_filter': True,       # å¯ç”¨æ—¶é—´è¿‡æ»¤
    'recent_days': 30,                # åªæŸ¥è¯¢æœ€è¿‘Nå¤©çš„æ•°æ®
    
    # è½¨è¿¹è´¨é‡æ£€æŸ¥é…ç½®
    'min_points_single_lane': 5,      # å•è½¦é“æœ€å°‘ç‚¹æ•°
    'enable_multi_lane_filter': True, # å¯ç”¨å¤šè½¦é“è¿‡æ»¤
    
    # ç®€åŒ–é…ç½®
    'simplify_tolerance': 2.0,        # ç±³
    'enable_simplification': True,
    
    # æ€§èƒ½é…ç½®
    'batch_size': 100,
    'enable_parallel': True,
    'max_workers': 4,
    
    # æ•°æ®åº“è¡¨å
    'trajectory_segments_table': 'trajectory_lane_segments',
    'trajectory_buffer_table': 'trajectory_lane_buffer',
    'quality_check_table': 'trajectory_quality_check',
    
    # è½¦é“åˆ†æç»“æœè¡¨å
    'lane_analysis_main_table': 'trajectory_lane_analysis',
    'lane_hits_table': 'trajectory_lane_hits', 
    'lane_trajectories_table': 'trajectory_lane_complete_trajectories'
}

# å…¨å±€å˜é‡
interrupted = False
logger = logging.getLogger(__name__)

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å‡½æ•°ï¼Œç”¨äºä¼˜é›…é€€å‡º"""
    global interrupted
    print(f"\næ¥æ”¶åˆ°ä¸­æ–­ä¿¡å· ({signum})ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
    print("ç­‰å¾…å½“å‰å¤„ç†å®Œæˆï¼Œè¯·ç¨å€™...")
    interrupted = True

class TrajectoryLaneAnalyzer:
    """è½¨è¿¹è½¦é“åˆ†æå™¨ä¸»ç±»
    
    åŠŸèƒ½ï¼šåŸºäºè¾“å…¥è½¨è¿¹æŸ¥è¯¢å…¶ä»–ç›¸å…³è½¨è¿¹
    1. å¯¹è¾“å…¥è½¨è¿¹æ‰§è¡Œroad_analysisè·å¾—å€™é€‰lanes
    2. åŸºäºè¾“å…¥è½¨è¿¹åˆ†æ®µæ‰¾åˆ°é‚»è¿‘çš„å€™é€‰lanes
    3. ä¸ºlanesåˆ›å»ºbufferï¼Œåœ¨è½¨è¿¹ç‚¹æ•°æ®åº“ä¸­æŸ¥è¯¢å…¶ä»–è½¨è¿¹ç‚¹
    4. æ ¹æ®è¿‡æ»¤è§„åˆ™ä¿ç•™ç¬¦åˆæ¡ä»¶çš„å®Œæ•´è½¨è¿¹
    """
    
    def __init__(self, config: Dict[str, Any] = None, road_analysis_id: str = None):
        """åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            config: é…ç½®å‚æ•°å­—å…¸
            road_analysis_id: trajectory_road_analysisçš„åˆ†æIDï¼Œç”¨äºè·å–å€™é€‰è½¦é“
        """
        self.config = DEFAULT_CONFIG.copy()
        if config:
            self.config.update(config)
        
        self.road_analysis_id = road_analysis_id
        self.engine = create_engine(LOCAL_DSN, future=True)
        
        # **æ–°å¢éªŒè¯**ï¼šå¦‚æœæä¾›äº†road_analysis_idï¼ŒéªŒè¯é…ç½®å’Œæ•°æ®ä¸€è‡´æ€§
        if self.road_analysis_id:
            self._validate_road_analysis_connection()
        
        # ä¸åœ¨åˆå§‹åŒ–æ—¶åˆ›å»ºè¡¨ï¼Œåœ¨ä¿å­˜æ—¶åŠ¨æ€åˆ›å»º
        
        self.stats = {
            'input_trajectories': 0,
            'processed_trajectories': 0,
            'candidate_lanes_found': 0,
            'buffer_queries_executed': 0,
            'trajectory_points_found': 0,
            'unique_data_names_found': 0,
            'trajectories_passed_filter': 0,
            'trajectories_multi_lane': 0,
            'trajectories_sufficient_points': 0,
            'start_time': None,
            'end_time': None
        }
    
    def _generate_dynamic_table_names(self, analysis_id: str) -> Dict[str, str]:
        """æ ¹æ®analysis_idç”ŸæˆåŠ¨æ€è¡¨å
        
        Args:
            analysis_id: åˆ†æID
            
        Returns:
            åŒ…å«å„ç§è¡¨åçš„å­—å…¸
        """
        # ç®€åŒ–è¡¨åç”Ÿæˆé€»è¾‘ï¼Œä½¿ç”¨æ—¶é—´æˆ³è€Œä¸æ˜¯å®Œæ•´çš„analysis_id
        # é¿å…PostgreSQLè¡¨åé•¿åº¦é™åˆ¶ï¼ˆ63å­—ç¬¦ï¼‰
        
        # æå–æ—¶é—´æˆ³éƒ¨åˆ†
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # å¦‚æœroad_analysis_idå­˜åœ¨ï¼Œä»ä¸­æå–åŸºç¡€æ ‡è¯†
        if self.road_analysis_id:
            # ä»road_analysis_idä¸­æå–å‰ç¼€éƒ¨åˆ†
            # ä¾‹å¦‚ï¼šintegrated_20250714_133413_road_f8f65ca59e094aa89f3121fa2510c506
            # æå–ï¼šintegrated_20250714_133413
            if 'integrated_' in self.road_analysis_id:
                # æå– integrated_YYYYMMDD_HHMMSS éƒ¨åˆ†
                parts = self.road_analysis_id.split('_')
                if len(parts) >= 3 and parts[0] == 'integrated':
                    base_name = f"{parts[0]}_{parts[1]}_{parts[2]}"
                else:
                    base_name = f"integrated_{timestamp}"
            else:
                base_name = f"integrated_{timestamp}"
            
            logger.info(f"åŸºäºroad_analysis_idç”Ÿæˆè¡¨åï¼ŒåŸºç¡€å: {base_name}")
        else:
            # å¦‚æœæ²¡æœ‰road_analysis_idï¼Œä½¿ç”¨å½“å‰æ—¶é—´æˆ³
            base_name = f"integrated_{timestamp}"
            logger.info(f"åŸºäºå½“å‰æ—¶é—´ç”Ÿæˆè¡¨åï¼ŒåŸºç¡€å: {base_name}")
        
        # ç”Ÿæˆç®€åŒ–çš„è¡¨å
        table_names = {
            'lane_analysis_main_table': f"{base_name}_lanes",
            'lane_hits_table': f"{base_name}_lane_hits", 
            'lane_trajectories_table': f"{base_name}_lane_trajectories"
        }
        
        # æ£€æŸ¥è¡¨åé•¿åº¦ï¼ŒPostgreSQLé™åˆ¶ä¸º63å­—ç¬¦
        for table_type, table_name in table_names.items():
            if len(table_name) > 63:
                logger.warning(f"è¡¨åè¿‡é•¿ ({len(table_name)} > 63): {table_name}")
                # æˆªæ–­åˆ°å®‰å…¨é•¿åº¦
                table_names[table_type] = table_name[:63]
                logger.warning(f"æˆªæ–­å: {table_names[table_type]}")
        
        logger.info(f"ç”Ÿæˆçš„è½¦é“åˆ†æè¡¨å:")
        logger.info(f"  - ä¸»è¡¨: {table_names['lane_analysis_main_table']} ({len(table_names['lane_analysis_main_table'])} chars)")
        logger.info(f"  - å‘½ä¸­è¡¨: {table_names['lane_hits_table']} ({len(table_names['lane_hits_table'])} chars)")
        logger.info(f"  - è½¨è¿¹è¡¨: {table_names['lane_trajectories_table']} ({len(table_names['lane_trajectories_table'])} chars)")
        
        return table_names
    
    def _validate_road_analysis_connection(self):
        """éªŒè¯ä¸é“è·¯åˆ†æç»“æœçš„è¿æ¥"""
        road_lanes_table = self.config['road_analysis_lanes_table']
        
        logger.info(f"éªŒè¯é“è·¯åˆ†æè¿æ¥: road_analysis_id={self.road_analysis_id}")
        logger.info(f"é¢„æœŸçš„é“è·¯åˆ†æç»“æœè¡¨: {road_lanes_table}")
        
        try:
            with self.engine.connect() as conn:
                # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
                table_check_sql = text(f"""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_schema = 'public' 
                        AND table_name = '{road_lanes_table}'
                    );
                """)
                
                table_exists = conn.execute(table_check_sql).scalar()
                
                if not table_exists:
                    logger.error(f"âŒ é“è·¯åˆ†æç»“æœè¡¨ä¸å­˜åœ¨: {road_lanes_table}")
                    logger.error("å¯èƒ½çš„åŸå› :")
                    logger.error("1. é“è·¯åˆ†æå°šæœªå®Œæˆ")
                    logger.error("2. è¡¨åé…ç½®ä¸æ­£ç¡®")
                    logger.error("3. æ•°æ®åº“è¿æ¥é—®é¢˜")
                    return False
                
                logger.info(f"âœ“ é“è·¯åˆ†æç»“æœè¡¨å­˜åœ¨: {road_lanes_table}")
                
                # æ£€æŸ¥æŒ‡å®šanalysis_idçš„æ•°æ®
                count_sql = text(f"""
                    SELECT COUNT(*) FROM {road_lanes_table}
                    WHERE analysis_id = :road_analysis_id
                """)
                
                lane_count = conn.execute(count_sql, {'road_analysis_id': self.road_analysis_id}).scalar()
                
                if lane_count == 0:
                    logger.error(f"âŒ è¡¨ä¸­æ²¡æœ‰æ‰¾åˆ°analysis_id={self.road_analysis_id}çš„æ•°æ®")
                    
                    # æŸ¥çœ‹è¡¨ä¸­å®é™…æœ‰å“ªäº›analysis_id
                    available_ids_sql = text(f"""
                        SELECT DISTINCT analysis_id, COUNT(*) as lane_count
                        FROM {road_lanes_table}
                        GROUP BY analysis_id
                        ORDER BY lane_count DESC
                        LIMIT 5
                    """)
                    
                    available_results = conn.execute(available_ids_sql).fetchall()
                    if available_results:
                        logger.error("è¡¨ä¸­å¯ç”¨çš„analysis_id:")
                        for row in available_results:
                            logger.error(f"  - {row[0]} ({row[1]} lanes)")
                    else:
                        logger.error("è¡¨ä¸­æ²¡æœ‰ä»»ä½•æ•°æ®")
                    
                    return False
                
                logger.info(f"âœ“ æ‰¾åˆ° {lane_count} ä¸ªå€™é€‰lanes (analysis_id={self.road_analysis_id})")
                
                # è·å–ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
                stats_sql = text(f"""
                    SELECT 
                        lane_type,
                        COUNT(*) as count
                    FROM {road_lanes_table}
                    WHERE analysis_id = :road_analysis_id
                    GROUP BY lane_type
                    ORDER BY count DESC
                """)
                
                type_stats = conn.execute(stats_sql, {'road_analysis_id': self.road_analysis_id}).fetchall()
                if type_stats:
                    logger.info("å€™é€‰lanesç±»å‹åˆ†å¸ƒ:")
                    for row in type_stats:
                        logger.info(f"  - {row[0]}: {row[1]} lanes")
                
                return True
                
        except Exception as e:
            logger.error(f"âŒ éªŒè¯é“è·¯åˆ†æè¿æ¥å¤±è´¥: {e}")
            return False
    

    
    def analyze_trajectory_neighbors(self, input_trajectory_id: str, input_trajectory_geom: str) -> Dict[str, Any]:
        """åˆ†æè¾“å…¥è½¨è¿¹çš„é‚»è¿‘è½¨è¿¹
        
        Args:
            input_trajectory_id: è¾“å…¥è½¨è¿¹ID
            input_trajectory_geom: è¾“å…¥è½¨è¿¹å‡ ä½•WKTå­—ç¬¦ä¸²
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        logger.info(f"å¼€å§‹è½¨è¿¹é‚»è¿‘æ€§åˆ†æ: {input_trajectory_id}")
        
        self.stats['input_trajectories'] += 1
        self.stats['start_time'] = datetime.now()
        
        try:
            # 1. å¯¹è¾“å…¥è½¨è¿¹è¿›è¡Œåˆ†æ®µé‡‡æ ·
            trajectory_segments = self._segment_input_trajectory(input_trajectory_geom)
            if not trajectory_segments:
                logger.warning(f"è½¨è¿¹åˆ†æ®µå¤±è´¥: {input_trajectory_id}")
                return {'error': 'è½¨è¿¹åˆ†æ®µå¤±è´¥'}
            
            logger.info(f"è¾“å…¥è½¨è¿¹åˆ†ä¸º {len(trajectory_segments)} æ®µ")
            
            # 2. ä¸ºæ¯æ®µæ‰¾åˆ°é‚»è¿‘çš„å€™é€‰lanes
            nearby_lanes = self._find_nearby_candidate_lanes(trajectory_segments)
            if not nearby_lanes:
                logger.warning(f"æœªæ‰¾åˆ°é‚»è¿‘çš„å€™é€‰lanes: {input_trajectory_id}")
                return {'error': 'æœªæ‰¾åˆ°é‚»è¿‘çš„å€™é€‰lanes'}
            
            self.stats['candidate_lanes_found'] = len(nearby_lanes)
            logger.info(f"æ‰¾åˆ° {len(nearby_lanes)} ä¸ªé‚»è¿‘å€™é€‰lanes")
            
            # 3. ä¸ºlanesåˆ›å»ºbufferï¼ŒæŸ¥è¯¢è½¨è¿¹ç‚¹æ•°æ®åº“
            trajectory_hits = self._query_trajectory_points_in_buffers(nearby_lanes)
            
            self.stats['buffer_queries_executed'] = len(nearby_lanes)
            self.stats['trajectory_points_found'] = sum(len(hits['points']) for hits in trajectory_hits.values())
            self.stats['unique_data_names_found'] = len(trajectory_hits)
            
            logger.info(f"åœ¨bufferä¸­æ‰¾åˆ° {self.stats['trajectory_points_found']} ä¸ªè½¨è¿¹ç‚¹")
            logger.info(f"æ¶‰åŠ {self.stats['unique_data_names_found']} ä¸ªä¸åŒçš„data_name")
            
            # 4. åº”ç”¨è¿‡æ»¤è§„åˆ™
            filtered_trajectories = self._apply_filtering_rules(trajectory_hits)
            
            self.stats['trajectories_passed_filter'] = len(filtered_trajectories)
            
            # 5. æå–ç¬¦åˆæ¡ä»¶çš„å®Œæ•´è½¨è¿¹
            complete_trajectories = self._extract_complete_trajectories(filtered_trajectories)
            
            self.stats['processed_trajectories'] += 1
            self.stats['end_time'] = datetime.now()
            
            logger.info(f"è½¨è¿¹é‚»è¿‘æ€§åˆ†æå®Œæˆ: {input_trajectory_id}")
            logger.info(f"ç¬¦åˆæ¡ä»¶çš„è½¨è¿¹æ•°: {len(complete_trajectories)}")
            
            # ç”Ÿæˆåˆ†æID
            analysis_id = f"lane_analysis_{input_trajectory_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # è·å–åŠ¨æ€è¡¨å
            dynamic_table_names = self._generate_dynamic_table_names(analysis_id)
            
            # ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
            self._save_lane_analysis_results(analysis_id, input_trajectory_id, trajectory_hits, complete_trajectories, dynamic_table_names)
            
            return {
                'input_trajectory_id': input_trajectory_id,
                'analysis_id': analysis_id,
                'candidate_lanes': nearby_lanes,
                'trajectory_hits': trajectory_hits,
                'filtered_trajectories': filtered_trajectories,
                'complete_trajectories': complete_trajectories,
                'stats': self.stats.copy()
            }
            
        except Exception as e:
            logger.error(f"è½¨è¿¹é‚»è¿‘æ€§åˆ†æå¤±è´¥: {input_trajectory_id}, é”™è¯¯: {e}")
            return {'error': str(e)}
    
    def _segment_input_trajectory(self, trajectory_geom: str) -> List[Dict]:
        """å¯¹è¾“å…¥è½¨è¿¹è¿›è¡Œåˆ†æ®µ
        
        Args:
            trajectory_geom: è½¨è¿¹å‡ ä½•WKTå­—ç¬¦ä¸²
            
        Returns:
            è½¨è¿¹åˆ†æ®µåˆ—è¡¨
        """
        try:
            from shapely import wkt
            trajectory = wkt.loads(trajectory_geom)
            
            if trajectory.geom_type != 'LineString':
                logger.error(f"è½¨è¿¹å‡ ä½•ç±»å‹é”™è¯¯: {trajectory.geom_type}ï¼ŒæœŸæœ›: LineString")
                return []
            
            # è·å–è½¨è¿¹åæ ‡
            coords = list(trajectory.coords)
            if len(coords) < 2:
                logger.error(f"è½¨è¿¹åæ ‡ç‚¹ä¸è¶³: {len(coords)}")
                return []
            
            # æŒ‰è·ç¦»åˆ†æ®µï¼ˆæ¯æ®µçº¦50ç±³ï¼‰
            segment_distance = 50.0  # ç±³
            segment_distance_degrees = segment_distance / 111320.0  # è½¬æ¢ä¸ºåº¦
            
            segments = []
            current_start = 0
            accumulated_distance = 0.0
            
            for i in range(1, len(coords)):
                # è®¡ç®—è·ç¦»
                prev_coord = coords[i-1]
                curr_coord = coords[i]
                dist = ((curr_coord[0] - prev_coord[0])**2 + (curr_coord[1] - prev_coord[1])**2)**0.5
                accumulated_distance += dist
                
                # å½“è·ç¦»è¾¾åˆ°åˆ†æ®µé˜ˆå€¼æ—¶åˆ›å»ºåˆ†æ®µ
                if accumulated_distance >= segment_distance_degrees or i == len(coords) - 1:
                    segment_coords = coords[current_start:i+1]
                    if len(segment_coords) >= 2:
                        segment_geom = LineString(segment_coords)
                        
                        # è®¡ç®—åˆ†æ®µä¸­å¿ƒç‚¹
                        center_coord = segment_geom.interpolate(0.5, normalized=True)
                        
                        segments.append({
                            'segment_id': len(segments),
                            'start_index': current_start,
                            'end_index': i,
                            'geometry': segment_geom,
                            'center_point': (center_coord.x, center_coord.y),
                            'length': accumulated_distance
                        })
                    
                    current_start = i
                    accumulated_distance = 0.0
            
            logger.debug(f"è½¨è¿¹åˆ†æ®µå®Œæˆ: {len(coords)} ä¸ªåæ ‡ç‚¹ â†’ {len(segments)} ä¸ªåˆ†æ®µ")
            return segments
            
        except Exception as e:
            logger.error(f"è½¨è¿¹åˆ†æ®µå¤±è´¥: {e}")
            return []
    
    def _find_nearby_candidate_lanes(self, trajectory_segments: List[Dict]) -> List[Dict]:
        """ä¸ºè½¨è¿¹åˆ†æ®µæ‰¾åˆ°é‚»è¿‘çš„å€™é€‰lanes
        
        Args:
            trajectory_segments: è½¨è¿¹åˆ†æ®µåˆ—è¡¨
            
        Returns:
            é‚»è¿‘å€™é€‰lanesåˆ—è¡¨
        """
        if not self.road_analysis_id:
            logger.error("æœªæŒ‡å®šroad_analysis_idï¼Œæ— æ³•æŸ¥æ‰¾å€™é€‰è½¦é“")
            return []
        
        road_lanes_table = self.config['road_analysis_lanes_table']
        max_distance = self.config['max_lane_distance']
        max_distance_degrees = max_distance / 111320.0
        
        nearby_lanes = []
        
        try:
            with self.engine.connect() as conn:
                for segment in trajectory_segments:
                    center_lng, center_lat = segment['center_point']
                    
                    # æŸ¥è¯¢è¯¥åˆ†æ®µé™„è¿‘çš„å€™é€‰lanes
                    sql = text(f"""
                        SELECT 
                            lane_id, 
                            ST_Distance(geometry, ST_SetSRID(ST_MakePoint(:lng, :lat), 4326)) as distance,
                            lane_type,
                            road_id,
                            ST_AsText(geometry) as geometry_wkt
                        FROM {road_lanes_table}
                        WHERE analysis_id = :road_analysis_id
                        AND geometry IS NOT NULL
                        AND ST_DWithin(geometry, ST_SetSRID(ST_MakePoint(:lng, :lat), 4326), :max_distance)
                        ORDER BY distance
                        LIMIT 5
                    """)
                    
                    result = conn.execute(sql, {
                        'lng': center_lng,
                        'lat': center_lat,
                        'road_analysis_id': self.road_analysis_id,
                        'max_distance': max_distance_degrees
                    })
                    
                    segment_lanes = []
                    for row in result:
                        lane_info = {
                            'lane_id': row[0],
                            'distance': row[1],
                            'lane_type': row[2],
                            'road_id': row[3],
                            'geometry_wkt': row[4],
                            'segment_id': segment['segment_id']
                        }
                        segment_lanes.append(lane_info)
                    
                    if segment_lanes:
                        logger.debug(f"åˆ†æ®µ {segment['segment_id']} æ‰¾åˆ° {len(segment_lanes)} ä¸ªé‚»è¿‘lanes")
                        nearby_lanes.extend(segment_lanes)
            
            # å»é‡ï¼ˆåŒä¸€ä¸ªlaneå¯èƒ½è¢«å¤šä¸ªåˆ†æ®µæ‰¾åˆ°ï¼‰
            unique_lanes = {}
            for lane in nearby_lanes:
                lane_id = lane['lane_id']
                if lane_id not in unique_lanes or lane['distance'] < unique_lanes[lane_id]['distance']:
                    unique_lanes[lane_id] = lane
            
            result_lanes = list(unique_lanes.values())
            logger.info(f"å»é‡åçš„é‚»è¿‘å€™é€‰lanes: {len(result_lanes)} ä¸ª")
            
            return result_lanes
            
        except Exception as e:
            logger.error(f"æŸ¥æ‰¾é‚»è¿‘å€™é€‰laneså¤±è´¥: {e}")
            return []
    
    def _query_trajectory_points_in_buffers(self, nearby_lanes: List[Dict]) -> Dict[str, Dict]:
        """åœ¨lanesçš„bufferä¸­æŸ¥è¯¢è½¨è¿¹ç‚¹æ•°æ®åº“
        
        Args:
            nearby_lanes: é‚»è¿‘å€™é€‰lanesåˆ—è¡¨
            
        Returns:
            æŒ‰data_nameåˆ†ç»„çš„è½¨è¿¹ç‚¹å‘½ä¸­ç»“æœ
        """
        buffer_radius = self.config['buffer_radius']
        trajectory_hits = {}
        
        # é…ç½®å‚æ•°
        points_limit = self.config.get('points_limit_per_lane', 1000)
        enable_time_filter = self.config.get('enable_time_filter', True)
        recent_days = self.config.get('recent_days', 30)  # åªæŸ¥è¯¢æœ€è¿‘30å¤©çš„æ•°æ®
        
        logger.info(f"å¼€å§‹æŸ¥è¯¢{len(nearby_lanes)}ä¸ªlanesçš„bufferå†…è½¨è¿¹ç‚¹")
        logger.info(f"é…ç½®: buffer_radius={buffer_radius}m, points_limit={points_limit}, recent_days={recent_days}")
        
        try:
            # ä½¿ç”¨hive_cursorè¿æ¥dataset_gy1è½¨è¿¹æ•°æ®åº“
            with hive_cursor("dataset_gy1") as cur:
                for i, lane in enumerate(nearby_lanes):
                    logger.info(f"æŸ¥è¯¢lane [{i+1}/{len(nearby_lanes)}]: {lane['lane_id']} (type: {lane['lane_type']})")
                    
                    # æ„å»ºæ—¶é—´è¿‡æ»¤æ¡ä»¶
                    time_filter = ""
                    if enable_time_filter:
                        # æœ€è¿‘Nå¤©çš„æ—¶é—´æˆ³è¿‡æ»¤ï¼ˆå‡è®¾timestampæ˜¯Unixæ—¶é—´æˆ³ï¼‰
                        recent_timestamp = int(time.time()) - (recent_days * 24 * 3600)
                        time_filter = f"AND p.timestamp >= {recent_timestamp}"
                    
                    # ä¸ºlaneåˆ›å»ºbufferå¹¶æŸ¥è¯¢è½¨è¿¹ç‚¹ (ä½¿ç”¨Hive/Trinoå…¼å®¹çš„ç©ºé—´å‡½æ•°)
                    sql = f"""
                        SELECT 
                            p.dataset_name,
                            p.timestamp,
                            ST_X(p.point_lla) as longitude,
                            ST_Y(p.point_lla) as latitude,
                            p.twist_linear,
                            p.avp_flag,
                            p.workstage
                        FROM {POINT_TABLE} p
                        WHERE p.point_lla IS NOT NULL
                        AND ST_DWithin(
                            p.point_lla,
                            ST_Buffer(
                                ST_SetSRID(ST_GeomFromText('{lane['geometry_wkt']}'), 4326),
                                {buffer_radius / 111320.0}
                            ),
                            0
                        )
                        {time_filter}
                        ORDER BY p.timestamp DESC
                        LIMIT {points_limit}
                    """
                    
                    # åœ¨verboseæ¨¡å¼ä¸‹è¾“å‡ºDataGripå¯æ‰§è¡Œçš„SQL
                    if logger.isEnabledFor(logging.DEBUG):
                        datagrip_sql = f"""
-- æŸ¥è¯¢Lane {lane['lane_id']} bufferå†…çš„è½¨è¿¹ç‚¹ (dataset_gy1)
SELECT 
    p.dataset_name,
    p.timestamp,
    ST_X(p.point_lla) as longitude,
    ST_Y(p.point_lla) as latitude,
    p.twist_linear,
    p.avp_flag,
    p.workstage
FROM {POINT_TABLE} p
WHERE p.point_lla IS NOT NULL
AND ST_DWithin(
    p.point_lla,
    ST_Buffer(
        ST_SetSRID(ST_GeomFromText('{lane['geometry_wkt']}'), 4326),
        {buffer_radius / 111320.0}
    ),
    0
)
{time_filter}
ORDER BY p.timestamp DESC
LIMIT {points_limit};
"""
                        logger.debug(f"=== DataGripå¯æ‰§è¡ŒSQL (Lane {lane['lane_id']}) - dataset_gy1 ===")
                        logger.debug(datagrip_sql)
                    
                    # æ‰§è¡ŒæŸ¥è¯¢
                    start_time = time.time()
                    cur.execute(sql)
                    result = cur.fetchall()
                    columns = [d[0] for d in cur.description]
                    query_time = time.time() - start_time
                    
                    points_found = 0
                    unique_data_names = set()
                    
                    # å¤„ç†HiveæŸ¥è¯¢ç»“æœ
                    for row in result:
                        # rowæ˜¯tupleï¼ŒæŒ‰ç…§SQL selecté¡ºåºè§£æ
                        data_name = row[0]
                        timestamp = row[1]
                        longitude = row[2]
                        latitude = row[3]
                        twist_linear = row[4]
                        avp_flag = row[5]
                        workstage = row[6]
                        
                        unique_data_names.add(data_name)
                        
                        point_info = {
                            'timestamp': timestamp,
                            'longitude': longitude,
                            'latitude': latitude,
                            'twist_linear': twist_linear,
                            'avp_flag': avp_flag,
                            'workstage': workstage,
                            'lane_id': lane['lane_id'],
                            'lane_type': lane['lane_type']
                        }
                        
                        if data_name not in trajectory_hits:
                            trajectory_hits[data_name] = {
                                'data_name': data_name,
                                'points': [],
                                'lanes_touched': set(),
                                'lane_details': []
                            }
                        
                        trajectory_hits[data_name]['points'].append(point_info)
                        trajectory_hits[data_name]['lanes_touched'].add(lane['lane_id'])
                        
                        points_found += 1
                    
                    # è¾“å‡ºæŸ¥è¯¢ç»“æœç»Ÿè®¡
                    if points_found > 0:
                        logger.info(f"âœ“ Lane {lane['lane_id']} bufferå†…æ‰¾åˆ° {points_found} ä¸ªè½¨è¿¹ç‚¹")
                        logger.info(f"  - æ¶‰åŠ {len(unique_data_names)} ä¸ªä¸åŒdata_name")
                        logger.info(f"  - æŸ¥è¯¢è€—æ—¶: {query_time:.2f}ç§’")
                        
                        if points_found >= points_limit:
                            logger.warning(f"  - âš ï¸ è¾¾åˆ°é™åˆ¶({points_limit})ï¼Œå¯èƒ½æœ‰æ›´å¤šæ•°æ®æœªæŸ¥è¯¢")
                    else:
                        logger.info(f"âœ— Lane {lane['lane_id']} bufferå†…æœªæ‰¾åˆ°è½¨è¿¹ç‚¹")
                        logger.info(f"  - æŸ¥è¯¢è€—æ—¶: {query_time:.2f}ç§’")
            
            # è½¬æ¢setä¸ºlistä»¥ä¾¿åºåˆ—åŒ–
            for data_name in trajectory_hits:
                trajectory_hits[data_name]['lanes_touched'] = list(trajectory_hits[data_name]['lanes_touched'])
                trajectory_hits[data_name]['total_points'] = len(trajectory_hits[data_name]['points'])
                trajectory_hits[data_name]['total_lanes'] = len(trajectory_hits[data_name]['lanes_touched'])
            
            logger.info(f"ğŸ¯ bufferæŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(trajectory_hits)} ä¸ªdata_nameçš„è½¨è¿¹ç‚¹")
            
            # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡
            if trajectory_hits:
                total_points = sum(hit['total_points'] for hit in trajectory_hits.values())
                multi_lane_count = sum(1 for hit in trajectory_hits.values() if hit['total_lanes'] > 1)
                
                logger.info(f"ğŸ“Š æŸ¥è¯¢ç»Ÿè®¡:")
                logger.info(f"  - æ€»è½¨è¿¹ç‚¹æ•°: {total_points}")
                logger.info(f"  - å¤šè½¦é“è½¨è¿¹æ•°: {multi_lane_count}")
                logger.info(f"  - å•è½¦é“è½¨è¿¹æ•°: {len(trajectory_hits) - multi_lane_count}")
                
                # è¾“å‡ºå‰å‡ ä¸ªdata_nameç¤ºä¾‹
                logger.info(f"  - å‰5ä¸ªdata_nameç¤ºä¾‹:")
                for i, (data_name, hit) in enumerate(list(trajectory_hits.items())[:5]):
                    logger.info(f"    {i+1}. {data_name}: {hit['total_points']}ç‚¹, {hit['total_lanes']}lanes")
            
            return trajectory_hits
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢bufferå†…è½¨è¿¹ç‚¹å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {}
    
    def _apply_filtering_rules(self, trajectory_hits: Dict[str, Dict]) -> Dict[str, Dict]:
        """åº”ç”¨è¿‡æ»¤è§„åˆ™
        
        Args:
            trajectory_hits: è½¨è¿¹ç‚¹å‘½ä¸­ç»“æœ
            
        Returns:
            ç¬¦åˆæ¡ä»¶çš„è½¨è¿¹
        """
        min_points_threshold = self.config.get('min_points_single_lane', 5)
        filtered = {}
        
        for data_name, hit_info in trajectory_hits.items():
            total_lanes = hit_info['total_lanes']
            total_points = hit_info['total_points']
            
            # è¿‡æ»¤è§„åˆ™1ï¼šä¸2ä¸ªåŠä»¥ä¸Šlaneç›¸é‚»
            if total_lanes >= 2:
                filtered[data_name] = hit_info.copy()
                filtered[data_name]['filter_reason'] = f'multi_lane_{total_lanes}_lanes'
                self.stats['trajectories_multi_lane'] += 1
                logger.debug(f"ä¿ç•™è½¨è¿¹ {data_name}: å¤šè½¦é“ ({total_lanes} lanes)")
                continue
            
            # è¿‡æ»¤è§„åˆ™2ï¼šå‘½ä¸­ç‚¹æ•°è¶…è¿‡é˜ˆå€¼
            if total_points >= min_points_threshold:
                filtered[data_name] = hit_info.copy()
                filtered[data_name]['filter_reason'] = f'sufficient_points_{total_points}_points'
                self.stats['trajectories_sufficient_points'] += 1
                logger.debug(f"ä¿ç•™è½¨è¿¹ {data_name}: è¶³å¤Ÿç‚¹æ•° ({total_points} points)")
                continue
            
            logger.debug(f"è¿‡æ»¤æ‰è½¨è¿¹ {data_name}: å•è½¦é“({total_lanes})ä¸”ç‚¹æ•°ä¸è¶³({total_points}<{min_points_threshold})")
        
        logger.info(f"è¿‡æ»¤è§„åˆ™åº”ç”¨å®Œæˆ: {len(trajectory_hits)} â†’ {len(filtered)}")
        logger.info(f"  - å¤šè½¦é“ä¿ç•™: {self.stats['trajectories_multi_lane']}")
        logger.info(f"  - è¶³å¤Ÿç‚¹æ•°ä¿ç•™: {self.stats['trajectories_sufficient_points']}")
        
        return filtered
    
    def _fetch_complete_trajectory_from_hive(self, data_name: str) -> pd.DataFrame:
        """ä»Hiveæ•°æ®åº“è·å–å®Œæ•´è½¨è¿¹æ•°æ®
        
        Args:
            data_name: æ•°æ®åç§°
            
        Returns:
            åŒ…å«è½¨è¿¹ç‚¹ä¿¡æ¯çš„DataFrame
        """
        try:
            with hive_cursor("dataset_gy1") as cur:
                # æŸ¥è¯¢å®Œæ•´è½¨è¿¹æ•°æ®
                sql = f"""
                    SELECT 
                        dataset_name,
                        timestamp,
                        ST_X(point_lla) as longitude,
                        ST_Y(point_lla) as latitude,
                        twist_linear,
                        avp_flag,
                        workstage
                    FROM {POINT_TABLE}
                    WHERE dataset_name = '{data_name}'
                    AND point_lla IS NOT NULL
                    ORDER BY timestamp ASC
                """
                
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug(f"=== è·å–å®Œæ•´è½¨è¿¹SQL (dataset_gy1) ===")
                    logger.debug(f"data_name: {data_name}")
                    logger.debug(sql)
                
                cur.execute(sql)
                rows = cur.fetchall()
                
                if not rows:
                    logger.warning(f"æœªæ‰¾åˆ°data_nameçš„è½¨è¿¹æ•°æ®: {data_name}")
                    return pd.DataFrame()
                
                # æ„å»ºDataFrame
                columns = ['dataset_name', 'timestamp', 'longitude', 'latitude', 
                          'twist_linear', 'avp_flag', 'workstage']
                df = pd.DataFrame(rows, columns=columns)
                
                logger.debug(f"æŸ¥è¯¢åˆ° {len(df)} ä¸ªè½¨è¿¹ç‚¹: {data_name}")
                return df
                
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å®Œæ•´è½¨è¿¹å¤±è´¥: {data_name}, é”™è¯¯: {str(e)}")
            return pd.DataFrame()
    
    def _extract_complete_trajectories(self, filtered_trajectories: Dict[str, Dict]) -> Dict[str, Dict]:
        """æå–ç¬¦åˆæ¡ä»¶çš„å®Œæ•´è½¨è¿¹
        
        Args:
            filtered_trajectories: ç¬¦åˆæ¡ä»¶çš„è½¨è¿¹ä¿¡æ¯
            
        Returns:
            å®Œæ•´è½¨è¿¹æ•°æ®
        """
        complete_trajectories = {}
        
        for data_name, trajectory_info in filtered_trajectories.items():
            try:
                # æŸ¥è¯¢å®Œæ•´è½¨è¿¹æ•°æ® (ä½¿ç”¨Hiveè¿æ¥)
                points_df = self._fetch_complete_trajectory_from_hive(data_name)
                
                if points_df.empty:
                    logger.warning(f"æ— æ³•è·å–å®Œæ•´è½¨è¿¹æ•°æ®: {data_name}")
                    continue
                
                # æ„å»ºå®Œæ•´è½¨è¿¹
                points_df = points_df.sort_values('timestamp')
                coordinates = []
                for _, row in points_df.iterrows():
                    if pd.notna(row['longitude']) and pd.notna(row['latitude']):
                        coordinates.append((float(row['longitude']), float(row['latitude'])))
                
                if len(coordinates) < 2:
                    logger.warning(f"è½¨è¿¹åæ ‡ç‚¹ä¸è¶³: {data_name}")
                    continue
                
                trajectory_geom = LineString(coordinates)
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                speed_data = points_df['twist_linear'].dropna()
                avp_data = points_df['avp_flag'].dropna()
                
                complete_trajectory = {
                    'data_name': data_name,
                    'geometry': trajectory_geom,
                    'geometry_wkt': trajectory_geom.wkt,
                    'start_time': int(points_df['timestamp'].min()),
                    'end_time': int(points_df['timestamp'].max()),
                    'duration': int(points_df['timestamp'].max() - points_df['timestamp'].min()),
                    'total_points': len(points_df),
                    'valid_coordinates': len(coordinates),
                    'trajectory_length': trajectory_geom.length,
                    
                    # ä»è¿‡æ»¤ä¿¡æ¯ç»§æ‰¿
                    'lanes_touched': trajectory_info['lanes_touched'],
                    'total_lanes': trajectory_info['total_lanes'],
                    'hit_points_count': trajectory_info['total_points'],
                    'filter_reason': trajectory_info['filter_reason'],
                    
                    # é€Ÿåº¦ç»Ÿè®¡
                    'avg_speed': round(float(speed_data.mean()), 2) if len(speed_data) > 0 else 0.0,
                    'max_speed': round(float(speed_data.max()), 2) if len(speed_data) > 0 else 0.0,
                    'min_speed': round(float(speed_data.min()), 2) if len(speed_data) > 0 else 0.0,
                    
                    # AVPç»Ÿè®¡
                    'avp_ratio': round(float((avp_data == 1).mean()), 3) if len(avp_data) > 0 else 0.0
                }
                
                complete_trajectories[data_name] = complete_trajectory
                logger.debug(f"æå–å®Œæ•´è½¨è¿¹: {data_name}, ç‚¹æ•°: {len(coordinates)}")
                
            except Exception as e:
                logger.error(f"æå–å®Œæ•´è½¨è¿¹å¤±è´¥: {data_name}, é”™è¯¯: {e}")
                continue
        
        logger.info(f"æå–å®Œæ•´è½¨è¿¹å®Œæˆ: {len(complete_trajectories)} ä¸ª")
        return complete_trajectories

    def _save_lane_analysis_results(self, analysis_id: str, input_trajectory_id: str, 
                                   trajectory_hits: Dict, complete_trajectories: Dict,
                                   dynamic_table_names: Dict[str, str]):
        """ä¿å­˜è½¦é“åˆ†æç»“æœåˆ°æ•°æ®åº“"""
        try:
            # 1. ä¿å­˜ä¸»åˆ†æè®°å½•
            self._save_main_analysis_record(analysis_id, input_trajectory_id, complete_trajectories, dynamic_table_names)
            
            # 2. ä¿å­˜è½¨è¿¹å‘½ä¸­è®°å½•
            self._save_trajectory_hits_records(analysis_id, trajectory_hits, dynamic_table_names)
            
            # 3. ä¿å­˜å®Œæ•´è½¨è¿¹è®°å½•
            self._save_complete_trajectories_records(analysis_id, complete_trajectories, dynamic_table_names)
            
            logger.info(f"âœ“ è½¦é“åˆ†æç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“: {analysis_id}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è½¦é“åˆ†æç»“æœå¤±è´¥: {analysis_id}, é”™è¯¯: {e}")

    def _save_main_analysis_record(self, analysis_id: str, input_trajectory_id: str, complete_trajectories: Dict, dynamic_table_names: Dict[str, str]):
        """ä¿å­˜ä¸»åˆ†æè®°å½•"""
        table_name = dynamic_table_names['lane_analysis_main_table']
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ä¸”ç»“æ„æ­£ç¡®
        check_table_sql = text(f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = '{table_name}'
            );
        """)
        
        # æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦å­˜åœ¨
        check_columns_sql = text(f"""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_schema = 'public' 
            AND table_name = '{table_name}'
            AND column_name IN ('analysis_id', 'input_trajectory_id', 'road_analysis_id', 'candidate_lanes_found')
        """)
        
        with self.engine.connect() as conn:
            table_exists = conn.execute(check_table_sql).scalar()
            
            if table_exists:
                # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                existing_columns = conn.execute(check_columns_sql).fetchall()
                existing_column_names = {row[0] for row in existing_columns}
                required_columns = {'analysis_id', 'input_trajectory_id', 'road_analysis_id', 'candidate_lanes_found'}
                
                if not required_columns.issubset(existing_column_names):
                    logger.warning(f"è¡¨ {table_name} ç»“æ„ä¸æ­£ç¡®ï¼Œç¼ºå°‘å­—æ®µ: {required_columns - existing_column_names}")
                    logger.warning(f"åˆ é™¤å¹¶é‡æ–°åˆ›å»ºè¡¨: {table_name}")
                    
                    # åˆ é™¤æ—§è¡¨
                    drop_table_sql = text(f"DROP TABLE IF EXISTS {table_name}")
                    conn.execute(drop_table_sql)
                    conn.commit()
                    table_exists = False
            
            if not table_exists:
                # åˆ›å»ºæ–°è¡¨
                create_table_sql = text(f"""
                    CREATE TABLE {table_name} (
                        id SERIAL PRIMARY KEY,
                        analysis_id VARCHAR(100) NOT NULL,
                        input_trajectory_id VARCHAR(100) NOT NULL,
                        road_analysis_id VARCHAR(100),
                        candidate_lanes_found INTEGER DEFAULT 0,
                        trajectory_points_found INTEGER DEFAULT 0,
                        unique_data_names_found INTEGER DEFAULT 0,
                        trajectories_passed_filter INTEGER DEFAULT 0,
                        trajectories_multi_lane INTEGER DEFAULT 0,
                        trajectories_sufficient_points INTEGER DEFAULT 0,
                        complete_trajectories_count INTEGER DEFAULT 0,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                conn.execute(create_table_sql)
                
                # åˆ›å»ºç´¢å¼•
                create_indexes_sql = text(f"""
                    CREATE INDEX idx_{table_name.replace('-', '_')}_analysis_id ON {table_name}(analysis_id);
                    CREATE INDEX idx_{table_name.replace('-', '_')}_input_trajectory_id ON {table_name}(input_trajectory_id);
                """)
                
                conn.execute(create_indexes_sql)
                conn.commit()
                
                logger.info(f"âœ“ åˆ›å»ºè¡¨: {table_name}")
        
        # å…ˆåˆ é™¤å¯èƒ½å­˜åœ¨çš„è®°å½•ï¼ˆé¿å…é‡å¤ï¼‰
        delete_sql = text(f"""
            DELETE FROM {table_name} 
            WHERE analysis_id = :analysis_id AND input_trajectory_id = :input_trajectory_id
        """)
        
        insert_sql = text(f"""
            INSERT INTO {table_name} 
            (analysis_id, input_trajectory_id, road_analysis_id, candidate_lanes_found,
             trajectory_points_found, unique_data_names_found, trajectories_passed_filter,
             trajectories_multi_lane, trajectories_sufficient_points, complete_trajectories_count)
            VALUES (
                :analysis_id, :input_trajectory_id, :road_analysis_id, :candidate_lanes_found,
                :trajectory_points_found, :unique_data_names_found, :trajectories_passed_filter,
                :trajectories_multi_lane, :trajectories_sufficient_points, :complete_trajectories_count
            )
        """)
        
        with self.engine.connect() as conn:
            # åˆ é™¤ç°æœ‰è®°å½•
            conn.execute(delete_sql, {
                'analysis_id': analysis_id,
                'input_trajectory_id': input_trajectory_id
            })
            
            # æ’å…¥æ–°è®°å½•
            conn.execute(insert_sql, {
                'analysis_id': analysis_id,
                'input_trajectory_id': input_trajectory_id,
                'road_analysis_id': self.road_analysis_id,
                'candidate_lanes_found': self.stats.get('candidate_lanes_found', 0),
                'trajectory_points_found': self.stats.get('trajectory_points_found', 0),
                'unique_data_names_found': self.stats.get('unique_data_names_found', 0),
                'trajectories_passed_filter': self.stats.get('trajectories_passed_filter', 0),
                'trajectories_multi_lane': self.stats.get('trajectories_multi_lane', 0),
                'trajectories_sufficient_points': self.stats.get('trajectories_sufficient_points', 0),
                'complete_trajectories_count': len(complete_trajectories)
            })
            conn.commit()
            
        logger.info(f"âœ“ ä¸»åˆ†æè®°å½•å·²ä¿å­˜åˆ°è¡¨: {table_name}")

    def _save_trajectory_hits_records(self, analysis_id: str, trajectory_hits: Dict, dynamic_table_names: Dict[str, str]):
        """ä¿å­˜è½¨è¿¹å‘½ä¸­è®°å½•"""
        table_name = dynamic_table_names['lane_hits_table']
        
        if not trajectory_hits:
            return
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ä¸”ç»“æ„æ­£ç¡®
        check_table_sql = text(f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = '{table_name}'
            );
        """)
        
        # æ£€æŸ¥è¡¨ç»“æ„æ˜¯å¦æ­£ç¡®
        check_columns_sql = text(f"""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_schema = 'public' 
            AND table_name = '{table_name}'
            AND column_name IN ('data_name', 'total_points', 'total_lanes', 'filter_reason')
        """)
        
        with self.engine.connect() as conn:
            table_exists = conn.execute(check_table_sql).scalar()
            
            if table_exists:
                # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                existing_columns = conn.execute(check_columns_sql).fetchall()
                existing_column_names = {row[0] for row in existing_columns}
                required_columns = {'data_name', 'total_points', 'total_lanes', 'filter_reason'}
                
                if not required_columns.issubset(existing_column_names):
                    logger.warning(f"è¡¨ {table_name} ç»“æ„ä¸æ­£ç¡®ï¼Œç¼ºå°‘å­—æ®µ: {required_columns - existing_column_names}")
                    logger.warning(f"åˆ é™¤å¹¶é‡æ–°åˆ›å»ºè¡¨: {table_name}")
                    
                    # åˆ é™¤æ—§è¡¨
                    drop_table_sql = text(f"DROP TABLE IF EXISTS {table_name}")
                    conn.execute(drop_table_sql)
                    conn.commit()
                    table_exists = False
            
            if not table_exists:
                # åˆ›å»ºæ–°è¡¨
                create_table_sql = text(f"""
                    CREATE TABLE {table_name} (
                        id SERIAL PRIMARY KEY,
                        analysis_id VARCHAR(100) NOT NULL,
                        data_name VARCHAR(255) NOT NULL,
                        total_points INTEGER DEFAULT 0,
                        total_lanes INTEGER DEFAULT 0,
                        filter_reason VARCHAR(100),
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        INDEX idx_{table_name.replace('-', '_')}_analysis_id (analysis_id),
                        INDEX idx_{table_name.replace('-', '_')}_data_name (data_name)
                    )
                """)
                
                # ä¿®æ­£ç´¢å¼•è¯­æ³•ä¸ºPostgreSQLå…¼å®¹
                create_table_sql = text(f"""
                    CREATE TABLE {table_name} (
                        id SERIAL PRIMARY KEY,
                        analysis_id VARCHAR(100) NOT NULL,
                        data_name VARCHAR(255) NOT NULL,
                        total_points INTEGER DEFAULT 0,
                        total_lanes INTEGER DEFAULT 0,
                        filter_reason VARCHAR(100),
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                conn.execute(create_table_sql)
                
                # åˆ›å»ºç´¢å¼•
                create_indexes_sql = text(f"""
                    CREATE INDEX idx_{table_name.replace('-', '_')}_analysis_id ON {table_name}(analysis_id);
                    CREATE INDEX idx_{table_name.replace('-', '_')}_data_name ON {table_name}(data_name);
                """)
                
                conn.execute(create_indexes_sql)
                conn.commit()
                
                logger.info(f"âœ“ åˆ›å»ºè¡¨: {table_name}")
        
        # å…ˆåˆ é™¤å¯èƒ½å­˜åœ¨çš„è®°å½•ï¼ˆé¿å…é‡å¤ï¼‰
        delete_sql = text(f"""
            DELETE FROM {table_name} 
            WHERE analysis_id = :analysis_id
        """)
        
        insert_sql = text(f"""
            INSERT INTO {table_name} 
            (analysis_id, data_name, total_points, total_lanes, filter_reason)
            VALUES (:analysis_id, :data_name, :total_points, :total_lanes, :filter_reason)
        """)
        
        with self.engine.connect() as conn:
            # åˆ é™¤ç°æœ‰è®°å½•
            conn.execute(delete_sql, {'analysis_id': analysis_id})
            
            # æ’å…¥æ–°è®°å½•
            for data_name, hit_info in trajectory_hits.items():
                conn.execute(insert_sql, {
                    'analysis_id': analysis_id,
                    'data_name': data_name,
                    'total_points': hit_info.get('total_points', 0),
                    'total_lanes': hit_info.get('total_lanes', 0),
                    'filter_reason': hit_info.get('filter_reason', 'hit_found')
                })
            conn.commit()
            
        logger.info(f"âœ“ è½¨è¿¹å‘½ä¸­è®°å½•å·²ä¿å­˜åˆ°è¡¨: {table_name} ({len(trajectory_hits)} æ¡è®°å½•)")

    def _save_complete_trajectories_records(self, analysis_id: str, complete_trajectories: Dict, dynamic_table_names: Dict[str, str]):
        """ä¿å­˜å®Œæ•´è½¨è¿¹è®°å½•"""
        table_name = dynamic_table_names['lane_trajectories_table']
        
        if not complete_trajectories:
            return
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ä¸”ç»“æ„æ­£ç¡®
        check_table_sql = text(f"""
            SELECT EXISTS (
                SELECT FROM information_schema.tables 
                WHERE table_schema = 'public' 
                AND table_name = '{table_name}'
            );
        """)
        
        # æ£€æŸ¥å…³é”®å­—æ®µæ˜¯å¦å­˜åœ¨
        check_columns_sql = text(f"""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_schema = 'public' 
            AND table_name = '{table_name}'
            AND column_name IN ('analysis_id', 'data_name', 'filter_reason', 'lanes_touched_count')
        """)
        
        # æ£€æŸ¥å‡ ä½•åˆ—æ˜¯å¦å­˜åœ¨
        check_geometry_sql = text(f"""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_schema = 'public' 
            AND table_name = '{table_name}'
            AND column_name = 'geometry'
        """)
        
        with self.engine.connect() as conn:
            table_exists = conn.execute(check_table_sql).scalar()
            
            if table_exists:
                # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
                existing_columns = conn.execute(check_columns_sql).fetchall()
                existing_column_names = {row[0] for row in existing_columns}
                required_columns = {'analysis_id', 'data_name', 'filter_reason', 'lanes_touched_count'}
                
                # æ£€æŸ¥å‡ ä½•åˆ—
                geometry_columns = conn.execute(check_geometry_sql).fetchall()
                has_geometry = len(geometry_columns) > 0
                
                if not required_columns.issubset(existing_column_names) or not has_geometry:
                    logger.warning(f"è¡¨ {table_name} ç»“æ„ä¸æ­£ç¡®")
                    logger.warning(f"ç¼ºå°‘å­—æ®µ: {required_columns - existing_column_names}")
                    logger.warning(f"å‡ ä½•åˆ—å­˜åœ¨: {has_geometry}")
                    logger.warning(f"åˆ é™¤å¹¶é‡æ–°åˆ›å»ºè¡¨: {table_name}")
                    
                    # åˆ é™¤æ—§è¡¨
                    drop_table_sql = text(f"DROP TABLE IF EXISTS {table_name}")
                    conn.execute(drop_table_sql)
                    conn.commit()
                    table_exists = False
            
            if not table_exists:
                # åˆ›å»ºæ–°è¡¨
                create_table_sql = text(f"""
                    CREATE TABLE {table_name} (
                        id SERIAL PRIMARY KEY,
                        analysis_id VARCHAR(100) NOT NULL,
                        data_name VARCHAR(255) NOT NULL,
                        filter_reason VARCHAR(100),
                        lanes_touched_count INTEGER DEFAULT 0,
                        hit_points_count INTEGER DEFAULT 0,
                        total_points INTEGER DEFAULT 0,
                        valid_coordinates INTEGER DEFAULT 0,
                        trajectory_length FLOAT,
                        avg_speed FLOAT,
                        max_speed FLOAT,
                        min_speed FLOAT,
                        avp_ratio FLOAT,
                        start_time BIGINT,
                        end_time BIGINT,
                        duration BIGINT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                conn.execute(create_table_sql)
                
                # æ·»åŠ å‡ ä½•åˆ—
                add_geometry_sql = text(f"""
                    SELECT AddGeometryColumn('public', '{table_name}', 'geometry', 4326, 'LINESTRING', 2)
                """)
                
                try:
                    conn.execute(add_geometry_sql)
                    logger.info(f"âœ“ æ·»åŠ å‡ ä½•åˆ—åˆ°è¡¨: {table_name}")
                except Exception as e:
                    logger.warning(f"æ·»åŠ å‡ ä½•åˆ—å¤±è´¥: {e}")
                
                # åˆ›å»ºç´¢å¼•
                create_indexes_sql = text(f"""
                    CREATE INDEX idx_{table_name.replace('-', '_')}_analysis_id ON {table_name}(analysis_id);
                    CREATE INDEX idx_{table_name.replace('-', '_')}_data_name ON {table_name}(data_name);
                """)
                
                try:
                    conn.execute(create_indexes_sql)
                    # åˆ›å»ºå‡ ä½•ç´¢å¼•
                    create_geom_index_sql = text(f"""
                        CREATE INDEX idx_{table_name.replace('-', '_')}_geometry ON {table_name} USING GIST(geometry);
                    """)
                    conn.execute(create_geom_index_sql)
                    logger.info(f"âœ“ åˆ›å»ºç´¢å¼•: {table_name}")
                except Exception as e:
                    logger.warning(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}")
                
                conn.commit()
                logger.info(f"âœ“ åˆ›å»ºè¡¨: {table_name}")
        
        # å…ˆåˆ é™¤å¯èƒ½å­˜åœ¨çš„è®°å½•ï¼ˆé¿å…é‡å¤ï¼‰
        delete_sql = text(f"""
            DELETE FROM {table_name} 
            WHERE analysis_id = :analysis_id
        """)
        
        insert_sql = text(f"""
            INSERT INTO {table_name} 
            (analysis_id, data_name, filter_reason, lanes_touched_count, hit_points_count,
             total_points, valid_coordinates, trajectory_length, avg_speed, max_speed,
             min_speed, avp_ratio, start_time, end_time, duration, geometry)
            VALUES (
                :analysis_id, :data_name, :filter_reason, :lanes_touched_count, :hit_points_count,
                :total_points, :valid_coordinates, :trajectory_length, :avg_speed, :max_speed,
                :min_speed, :avp_ratio, :start_time, :end_time, :duration,
                ST_SetSRID(ST_GeomFromText(:geometry_wkt), 4326)
            )
        """)
        
        with self.engine.connect() as conn:
            # åˆ é™¤ç°æœ‰è®°å½•
            conn.execute(delete_sql, {'analysis_id': analysis_id})
            
            # æ’å…¥æ–°è®°å½•
            for data_name, trajectory in complete_trajectories.items():
                conn.execute(insert_sql, {
                    'analysis_id': analysis_id,
                    'data_name': data_name,
                    'filter_reason': trajectory.get('filter_reason', ''),
                    'lanes_touched_count': len(trajectory.get('lanes_touched', [])),
                    'hit_points_count': trajectory.get('hit_points_count', 0),
                    'total_points': trajectory.get('total_points', 0),
                    'valid_coordinates': trajectory.get('valid_coordinates', 0),
                    'trajectory_length': trajectory.get('trajectory_length', 0.0),
                    'avg_speed': trajectory.get('avg_speed', 0.0),
                    'max_speed': trajectory.get('max_speed', 0.0),
                    'min_speed': trajectory.get('min_speed', 0.0),
                    'avp_ratio': trajectory.get('avp_ratio', 0.0),
                    'start_time': trajectory.get('start_time', 0),
                    'end_time': trajectory.get('end_time', 0),
                    'duration': trajectory.get('duration', 0),
                    'geometry_wkt': trajectory.get('geometry_wkt', '')
                })
            conn.commit()
            
        logger.info(f"âœ“ å®Œæ•´è½¨è¿¹è®°å½•å·²ä¿å­˜åˆ°è¡¨: {table_name} ({len(complete_trajectories)} æ¡è®°å½•)")


def batch_analyze_lanes_from_road_results(
    road_analysis_results: List[Tuple[str, str, Dict[str, Any]]],
    batch_analysis_id: Optional[str] = None,
    config: Optional[Dict[str, Any]] = None
) -> List[Tuple[str, str, Dict[str, Any]]]:
    """
    åŸºäºé“è·¯åˆ†æç»“æœæ‰¹é‡è¿›è¡Œè½¦é“åˆ†æï¼ˆè½¨è¿¹é‚»è¿‘æ€§æŸ¥è¯¢ï¼‰
    
    Args:
        road_analysis_results: é“è·¯åˆ†æç»“æœåˆ—è¡¨ [(trajectory_id, road_analysis_id, summary), ...]
        batch_analysis_id: æ‰¹é‡åˆ†æIDï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰
        config: è½¨è¿¹è½¦é“åˆ†æé…ç½®
        
    Returns:
        è½¦é“åˆ†æç»“æœåˆ—è¡¨ [(trajectory_id, lane_analysis_id, summary), ...]
    """
    if not batch_analysis_id:
        batch_analysis_id = f"batch_lane_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    logger.info(f"å¼€å§‹æ‰¹é‡è½¨è¿¹é‚»è¿‘æ€§åˆ†æ: {batch_analysis_id}")
    logger.info(f"é“è·¯åˆ†æç»“æœæ•°: {len(road_analysis_results)}")
    
    # è¿‡æ»¤å‡ºæˆåŠŸçš„é“è·¯åˆ†æç»“æœ
    successful_road_results = [r for r in road_analysis_results if r[1] is not None]
    
    if not successful_road_results:
        logger.warning("æ²¡æœ‰æˆåŠŸçš„é“è·¯åˆ†æç»“æœï¼Œè·³è¿‡è½¦é“åˆ†æ")
        return []
    
    logger.info(f"æœ‰æ•ˆçš„é“è·¯åˆ†æç»“æœ: {len(successful_road_results)}")
    
    # æ‰¹é‡è½¦é“åˆ†æ
    lane_results = []
    
    for i, (trajectory_id, road_analysis_id, road_summary) in enumerate(successful_road_results):
        try:
            logger.info(f"åˆ†æè½¨è¿¹é‚»è¿‘æ€§ [{i+1}/{len(successful_road_results)}]: {trajectory_id}")
            
            # è·å–è½¨è¿¹å‡ ä½•ä¿¡æ¯
            input_trajectory_geom = road_summary.get('input_trajectory_geom')
            if not input_trajectory_geom:
                logger.error(f"é“è·¯åˆ†æç»“æœä¸­ç¼ºå°‘è½¨è¿¹å‡ ä½•ä¿¡æ¯: {trajectory_id}")
                lane_results.append((trajectory_id, None, {
                    'error': 'é“è·¯åˆ†æç»“æœä¸­ç¼ºå°‘è½¨è¿¹å‡ ä½•ä¿¡æ¯',
                    'road_analysis_id': road_analysis_id
                }))
                continue
            
            # **å…³é”®ä¿®å¤**ï¼šç¡®ä¿é…ç½®ä¸­åŒ…å«æ­£ç¡®çš„é“è·¯åˆ†æç»“æœè¡¨å
            analyzer_config = (config or {}).copy()
            # ä»road_analysis_idä¸­æå–æ‰¹é‡åˆ†æIDï¼Œæ„é€ æ­£ç¡®çš„è¡¨å
            # road_analysis_idæ ¼å¼: integrated_YYYYMMDD_HHMMSS_road_trajectory_id
            # é“è·¯åˆ†æç»“æœè¡¨åæ ¼å¼: integrated_YYYYMMDD_HHMMSS_road_lanes
            if '_road_' in road_analysis_id:
                batch_part = road_analysis_id.split('_road_')[0]
                lanes_table_name = f"{batch_part}_road_lanes"
            else:
                # å…œåº•é€»è¾‘ï¼šç›´æ¥ä½¿ç”¨road_analysis_idæ„é€ è¡¨å
                lanes_table_name = f"{road_analysis_id}_lanes"
            
            analyzer_config['road_analysis_lanes_table'] = lanes_table_name
            
            logger.info(f"è½¦é“åˆ†æå™¨é…ç½®: road_analysis_id={road_analysis_id}, lanes_table={lanes_table_name}")
            
            # åˆ›å»ºè½¦é“åˆ†æå™¨
            analyzer = TrajectoryLaneAnalyzer(config=analyzer_config, road_analysis_id=road_analysis_id)
            
            # æ‰§è¡Œé‚»è¿‘æ€§åˆ†æ
            analysis_result = analyzer.analyze_trajectory_neighbors(trajectory_id, input_trajectory_geom)
            
            if 'error' in analysis_result:
                lane_results.append((trajectory_id, None, {
                    'error': analysis_result['error'],
                    'road_analysis_id': road_analysis_id
                }))
                continue
            
            # ç”Ÿæˆè½¦é“åˆ†æID
            lane_analysis_id = f"{batch_analysis_id}_{trajectory_id}"
            
            # è·å–åŠ¨æ€è¡¨å
            dynamic_table_names = analyzer._generate_dynamic_table_names(lane_analysis_id)
            
            # æ„å»ºè½¦é“åˆ†ææ±‡æ€»
            lane_summary = {
                'lane_analysis_id': lane_analysis_id,
                'road_analysis_id': road_analysis_id,
                'trajectory_id': trajectory_id,
                'input_trajectory_geom': input_trajectory_geom,
                'candidate_lanes_found': analysis_result['stats']['candidate_lanes_found'],
                'trajectory_points_found': analysis_result['stats']['trajectory_points_found'],
                'unique_data_names_found': analysis_result['stats']['unique_data_names_found'],
                'trajectories_passed_filter': analysis_result['stats']['trajectories_passed_filter'],
                'trajectories_multi_lane': analysis_result['stats']['trajectories_multi_lane'],
                'trajectories_sufficient_points': analysis_result['stats']['trajectories_sufficient_points'],
                'complete_trajectories_count': len(analysis_result['complete_trajectories']),
                'complete_trajectories': analysis_result['complete_trajectories'],
                'duration': analysis_result['stats'].get('end_time', datetime.now()) - analysis_result['stats'].get('start_time', datetime.now()),
                'properties': road_summary.get('properties', {})
            }
            
            lane_results.append((trajectory_id, lane_analysis_id, lane_summary))
            
            logger.info(f"âœ“ å®Œæˆè½¨è¿¹é‚»è¿‘æ€§åˆ†æ: {trajectory_id}")
            logger.info(f"  - æ‰¾åˆ° {lane_summary['candidate_lanes_found']} ä¸ªå€™é€‰lanes")
            logger.info(f"  - æ‰¾åˆ° {lane_summary['trajectory_points_found']} ä¸ªè½¨è¿¹ç‚¹")
            logger.info(f"  - ç¬¦åˆæ¡ä»¶çš„è½¨è¿¹: {lane_summary['complete_trajectories_count']} ä¸ª")
            
        except Exception as e:
            logger.error(f"åˆ†æè½¨è¿¹é‚»è¿‘æ€§å¤±è´¥: {trajectory_id}, é”™è¯¯: {e}")
            lane_results.append((trajectory_id, None, {
                'error': str(e),
                'road_analysis_id': road_analysis_id,
                'properties': road_summary.get('properties', {})
            }))
    
    # ç»Ÿè®¡ç»“æœ
    successful_count = len([r for r in lane_results if r[1] is not None])
    failed_count = len(lane_results) - successful_count
    
    logger.info(f"æ‰¹é‡è½¨è¿¹é‚»è¿‘æ€§åˆ†æå®Œæˆ: {batch_analysis_id}")
    logger.info(f"  - æˆåŠŸ: {successful_count}")
    logger.info(f"  - å¤±è´¥: {failed_count}")
    logger.info(f"  - æ€»è®¡: {len(lane_results)}")
    
    return lane_results


def batch_analyze_lanes_from_trajectory_records(
    trajectory_records: List,
    road_analysis_results: List[Tuple[str, str, Dict[str, Any]]],
    batch_analysis_id: Optional[str] = None,
    config: Optional[Dict[str, Any]] = None
) -> List[Tuple[str, str, Dict[str, Any]]]:
    """
    åŸºäºè½¨è¿¹è®°å½•å’Œé“è·¯åˆ†æç»“æœæ‰¹é‡è¿›è¡Œè½¦é“åˆ†æï¼ˆè½¨è¿¹é‚»è¿‘æ€§æŸ¥è¯¢ï¼‰
    
    Args:
        trajectory_records: è½¨è¿¹è®°å½•åˆ—è¡¨ (TrajectoryRecordå¯¹è±¡)
        road_analysis_results: é“è·¯åˆ†æç»“æœåˆ—è¡¨ [(trajectory_id, road_analysis_id, summary), ...]
        batch_analysis_id: æ‰¹é‡åˆ†æIDï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰
        config: è½¨è¿¹è½¦é“åˆ†æé…ç½®
        
    Returns:
        è½¦é“åˆ†æç»“æœåˆ—è¡¨ [(trajectory_id, lane_analysis_id, summary), ...]
    """
    if not batch_analysis_id:
        batch_analysis_id = f"batch_lane_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    logger.info(f"å¼€å§‹æ‰¹é‡è½¨è¿¹é‚»è¿‘æ€§åˆ†æ: {batch_analysis_id}")
    logger.info(f"è½¨è¿¹è®°å½•æ•°: {len(trajectory_records)}")
    logger.info(f"é“è·¯åˆ†æç»“æœæ•°: {len(road_analysis_results)}")
    
    # åˆ›å»ºtrajectory_idåˆ°road_analysisç»“æœçš„æ˜ å°„
    road_analysis_map = {r[0]: (r[1], r[2]) for r in road_analysis_results if r[1] is not None}
    
    # åˆ›å»ºtrajectory_idåˆ°è½¨è¿¹å‡ ä½•çš„æ˜ å°„
    trajectory_geom_map = {t.scene_id: t.geometry_wkt for t in trajectory_records}
    
    # è¿‡æ»¤å‡ºæœ‰å¯¹åº”é“è·¯åˆ†æç»“æœçš„è½¨è¿¹è®°å½•
    valid_trajectories = [t for t in trajectory_records if t.scene_id in road_analysis_map]
    
    if not valid_trajectories:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è½¨è¿¹è®°å½•ä¸é“è·¯åˆ†æç»“æœçš„åŒ¹é…")
        return []
    
    logger.info(f"æœ‰æ•ˆçš„è½¨è¿¹è®°å½•: {len(valid_trajectories)}")
    
    # æ‰¹é‡è½¦é“åˆ†æ
    lane_results = []
    
    for i, trajectory in enumerate(valid_trajectories):
        try:
            trajectory_id = trajectory.scene_id
            road_analysis_id, road_summary = road_analysis_map[trajectory_id]
            input_trajectory_geom = trajectory_geom_map[trajectory_id]
            
            logger.info(f"åˆ†æè½¨è¿¹é‚»è¿‘æ€§ [{i+1}/{len(valid_trajectories)}]: {trajectory_id}")
            
            # **å…³é”®ä¿®å¤**ï¼šç¡®ä¿é…ç½®ä¸­åŒ…å«æ­£ç¡®çš„é“è·¯åˆ†æç»“æœè¡¨å
            analyzer_config = (config or {}).copy()
            # ä»road_analysis_idä¸­æå–æ‰¹é‡åˆ†æIDï¼Œæ„é€ æ­£ç¡®çš„è¡¨å
            # road_analysis_idæ ¼å¼: integrated_YYYYMMDD_HHMMSS_road_trajectory_id
            # é“è·¯åˆ†æç»“æœè¡¨åæ ¼å¼: integrated_YYYYMMDD_HHMMSS_road_lanes
            if '_road_' in road_analysis_id:
                batch_part = road_analysis_id.split('_road_')[0]
                lanes_table_name = f"{batch_part}_road_lanes"
            else:
                # å…œåº•é€»è¾‘ï¼šç›´æ¥ä½¿ç”¨road_analysis_idæ„é€ è¡¨å
                lanes_table_name = f"{road_analysis_id}_lanes"
            
            analyzer_config['road_analysis_lanes_table'] = lanes_table_name
            
            logger.info(f"è½¦é“åˆ†æå™¨é…ç½®: road_analysis_id={road_analysis_id}, lanes_table={lanes_table_name}")
            
            # åˆ›å»ºè½¦é“åˆ†æå™¨
            analyzer = TrajectoryLaneAnalyzer(config=analyzer_config, road_analysis_id=road_analysis_id)
            
            # æ‰§è¡Œé‚»è¿‘æ€§åˆ†æ
            analysis_result = analyzer.analyze_trajectory_neighbors(trajectory_id, input_trajectory_geom)
            
            if 'error' in analysis_result:
                lane_results.append((trajectory_id, None, {
                    'error': analysis_result['error'],
                    'road_analysis_id': road_analysis_id,
                    'data_name': trajectory.data_name
                }))
                continue
            
            # ç”Ÿæˆè½¦é“åˆ†æID
            lane_analysis_id = f"{batch_analysis_id}_{trajectory_id}"
            
            # è·å–åŠ¨æ€è¡¨å
            dynamic_table_names = analyzer._generate_dynamic_table_names(lane_analysis_id)
            
            # æ„å»ºè½¦é“åˆ†ææ±‡æ€»
            lane_summary = {
                'lane_analysis_id': lane_analysis_id,
                'road_analysis_id': road_analysis_id,
                'trajectory_id': trajectory_id,
                'data_name': trajectory.data_name,
                'input_trajectory_geom': input_trajectory_geom,
                'candidate_lanes_found': analysis_result['stats']['candidate_lanes_found'],
                'trajectory_points_found': analysis_result['stats']['trajectory_points_found'],
                'unique_data_names_found': analysis_result['stats']['unique_data_names_found'],
                'trajectories_passed_filter': analysis_result['stats']['trajectories_passed_filter'],
                'trajectories_multi_lane': analysis_result['stats']['trajectories_multi_lane'],
                'trajectories_sufficient_points': analysis_result['stats']['trajectories_sufficient_points'],
                'complete_trajectories_count': len(analysis_result['complete_trajectories']),
                'complete_trajectories': analysis_result['complete_trajectories'],
                'duration': analysis_result['stats'].get('end_time', datetime.now()) - analysis_result['stats'].get('start_time', datetime.now()),
                'properties': trajectory.properties
            }
            
            lane_results.append((trajectory_id, lane_analysis_id, lane_summary))
            
            logger.info(f"âœ“ å®Œæˆè½¨è¿¹é‚»è¿‘æ€§åˆ†æ: {trajectory_id}")
            logger.info(f"  - æ‰¾åˆ° {lane_summary['candidate_lanes_found']} ä¸ªå€™é€‰lanes")
            logger.info(f"  - æ‰¾åˆ° {lane_summary['trajectory_points_found']} ä¸ªè½¨è¿¹ç‚¹")
            logger.info(f"  - ç¬¦åˆæ¡ä»¶çš„è½¨è¿¹: {lane_summary['complete_trajectories_count']} ä¸ª")
            
        except Exception as e:
            logger.error(f"åˆ†æè½¨è¿¹é‚»è¿‘æ€§å¤±è´¥: {trajectory_id}, é”™è¯¯: {e}")
            lane_results.append((trajectory_id, None, {
                'error': str(e),
                'road_analysis_id': road_analysis_map.get(trajectory_id, (None, {}))[0],
                'data_name': trajectory.data_name,
                'properties': trajectory.properties
            }))
    
    # ç»Ÿè®¡ç»“æœ
    successful_count = len([r for r in lane_results if r[1] is not None])
    failed_count = len(lane_results) - successful_count
    
    logger.info(f"æ‰¹é‡è½¨è¿¹é‚»è¿‘æ€§åˆ†æå®Œæˆ: {batch_analysis_id}")
    logger.info(f"  - æˆåŠŸ: {successful_count}")
    logger.info(f"  - å¤±è´¥: {failed_count}")
    logger.info(f"  - æ€»è®¡: {len(lane_results)}")
    
    return lane_results


def create_batch_lane_analysis_report(
    lane_results: List[Tuple[str, str, Dict[str, Any]]],
    batch_analysis_id: str
) -> str:
    """
    åˆ›å»ºæ‰¹é‡è½¨è¿¹é‚»è¿‘æ€§åˆ†ææŠ¥å‘Š
    
    Args:
        lane_results: æ‰¹é‡é‚»è¿‘æ€§åˆ†æç»“æœ
        batch_analysis_id: æ‰¹é‡åˆ†æID
        
    Returns:
        æŠ¥å‘Šæ–‡æœ¬
    """
    successful_results = [r for r in lane_results if r[1] is not None]
    failed_results = [r for r in lane_results if r[1] is None]
    
    report_lines = [
        f"# æ‰¹é‡è½¨è¿¹é‚»è¿‘æ€§åˆ†ææŠ¥å‘Š",
        f"",
        f"**æ‰¹é‡åˆ†æID**: {batch_analysis_id}",
        f"**åˆ†ææ—¶é—´**: {datetime.now().isoformat()}",
        f"",
        f"## æ€»ä½“ç»Ÿè®¡",
        f"",
        f"- **æ€»è½¨è¿¹æ•°**: {len(lane_results)}",
        f"- **æˆåŠŸåˆ†æ**: {len(successful_results)}",
        f"- **å¤±è´¥åˆ†æ**: {len(failed_results)}",
        f"- **æˆåŠŸç‡**: {len(successful_results)/len(lane_results)*100:.1f}%",
        f"",
    ]
    
    if successful_results:
        # æˆåŠŸåˆ†æç»Ÿè®¡
        total_candidate_lanes = sum(r[2].get('candidate_lanes_found', 0) for r in successful_results)
        total_trajectory_points = sum(r[2].get('trajectory_points_found', 0) for r in successful_results)
        total_unique_data_names = sum(r[2].get('unique_data_names_found', 0) for r in successful_results)
        total_complete_trajectories = sum(r[2].get('complete_trajectories_count', 0) for r in successful_results)
        
        report_lines.extend([
            f"## æˆåŠŸåˆ†ææ±‡æ€»",
            f"",
            f"- **æ€»å€™é€‰Laneæ•°**: {total_candidate_lanes}",
            f"- **æ€»è½¨è¿¹ç‚¹æ•°**: {total_trajectory_points}",
            f"- **æ€»data_nameæ•°**: {total_unique_data_names}",
            f"- **æ€»ç¬¦åˆæ¡ä»¶è½¨è¿¹æ•°**: {total_complete_trajectories}",
            f"- **å¹³å‡å€™é€‰Laneæ•°/è¾“å…¥è½¨è¿¹**: {total_candidate_lanes/len(successful_results):.1f}",
            f"- **å¹³å‡ç¬¦åˆæ¡ä»¶è½¨è¿¹æ•°/è¾“å…¥è½¨è¿¹**: {total_complete_trajectories/len(successful_results):.1f}",
            f"",
        ])
        
        # æˆåŠŸåˆ†æè¯¦æƒ…
        report_lines.extend([
            f"## æˆåŠŸåˆ†æè¯¦æƒ…",
            f"",
            f"| è½¨è¿¹ID | åˆ†æID | å€™é€‰Lanes | è½¨è¿¹ç‚¹æ•° | ç¬¦åˆæ¡ä»¶è½¨è¿¹æ•° |",
            f"|--------|--------|-----------|----------|---------------|",
        ])
        
        for trajectory_id, lane_analysis_id, summary in successful_results[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            candidate_lanes = summary.get('candidate_lanes_found', 0)
            trajectory_points = summary.get('trajectory_points_found', 0)
            complete_trajectories = summary.get('complete_trajectories_count', 0)
            report_lines.append(f"| {trajectory_id} | {lane_analysis_id} | {candidate_lanes} | {trajectory_points} | {complete_trajectories} |")
        
        if len(successful_results) > 10:
            report_lines.append(f"| ... | ... | ... | ... | ... |")
            report_lines.append(f"| (å…±{len(successful_results)}ä¸ªæˆåŠŸåˆ†æ) | | | | |")
    
    if failed_results:
        # å¤±è´¥åˆ†æè¯¦æƒ…
        report_lines.extend([
            f"",
            f"## å¤±è´¥åˆ†æè¯¦æƒ…",
            f"",
        ])
        
        for trajectory_id, _, summary in failed_results:
            error_msg = summary.get('error', 'æœªçŸ¥é”™è¯¯')
            report_lines.append(f"- **{trajectory_id}**: {error_msg}")
    
    return "\n".join(report_lines)


def main():
    """ä¸»å‡½æ•°ï¼ŒCLIå…¥å£ç‚¹"""
    parser = argparse.ArgumentParser(
        description='è½¨è¿¹é‚»è¿‘æ€§åˆ†ææ¨¡å— - åŸºäºè¾“å…¥è½¨è¿¹æŸ¥è¯¢ç›¸å…³è½¨è¿¹',
        epilog="""
å‰ææ¡ä»¶:
  å¿…é¡»å…ˆè¿è¡Œ trajectory_road_analysis è·å¾—é“è·¯åˆ†æç»“æœï¼Œæœ¬æ¨¡å—åŸºäºå…¶è¾“å‡ºçš„å€™é€‰è½¦é“è¿›è¡Œé‚»è¿‘æ€§æŸ¥è¯¢
  
æ”¯æŒçš„è¾“å…¥æ ¼å¼:
  éœ€è¦æä¾›è¾“å…¥è½¨è¿¹çš„å‡ ä½•ä¿¡æ¯ï¼ˆWKTæ ¼å¼ï¼‰
  
åˆ†ææµç¨‹:
  è¾“å…¥è½¨è¿¹ â†’ åˆ†æ®µ â†’ æ‰¾é‚»è¿‘å€™é€‰lanes â†’ bufferæŸ¥è¯¢è½¨è¿¹ç‚¹æ•°æ®åº“ â†’ è¿‡æ»¤è§„åˆ™ â†’ æå–ç¬¦åˆæ¡ä»¶çš„å®Œæ•´è½¨è¿¹
  
ç¤ºä¾‹:
  # éœ€è¦å…ˆè¿è¡Œé“è·¯åˆ†æï¼ˆè·å¾—road_analysis_idï¼‰
  python -m spdatalab.fusion.trajectory_road_analysis --trajectory-id my_traj --trajectory-geom "LINESTRING(...)"
  
  # ç„¶åè¿è¡Œé‚»è¿‘æ€§åˆ†æ
  python -m spdatalab.fusion.trajectory_lane_analysis --trajectory-id my_traj --trajectory-geom "LINESTRING(...)" --road-analysis-id trajectory_road_20241201_123456
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # è¾“å…¥å‚æ•°
    parser.add_argument('--trajectory-id', required=True, 
                       help='è¾“å…¥è½¨è¿¹ID')
    parser.add_argument('--trajectory-geom', required=True,
                       help='è¾“å…¥è½¨è¿¹å‡ ä½•ï¼ˆWKTæ ¼å¼ï¼‰')
    parser.add_argument('--road-analysis-id', required=True,
                       help='trajectory_road_analysisçš„åˆ†æIDï¼ˆå¿…éœ€ï¼Œç”¨äºè·å–å€™é€‰è½¦é“ï¼‰')
    
    # è½¦é“åˆ†æå‚æ•°
    parser.add_argument('--road-lanes-table', default='trajectory_road_lanes',
                       help='é“è·¯åˆ†æç»“æœè½¦é“è¡¨å')
    parser.add_argument('--buffer-radius', type=float, default=15.0,
                       help='è½¦é“ç¼“å†²åŒºåŠå¾„ï¼ˆç±³ï¼‰')
    parser.add_argument('--max-lane-distance', type=float, default=50.0,
                       help='æœ€å¤§è½¦é“æœç´¢è·ç¦»ï¼ˆç±³ï¼‰')
    
    # è¿‡æ»¤å‚æ•°
    parser.add_argument('--min-points-single-lane', type=int, default=5,
                       help='å•è½¦é“æœ€å°‘ç‚¹æ•°é˜ˆå€¼')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output-format', choices=['summary', 'detailed', 'geojson'], 
                       default='summary', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--output-file', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†æ—¥å¿—')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # æ„å»ºé…ç½®
        config = {
            'road_analysis_lanes_table': args.road_lanes_table,
            'buffer_radius': args.buffer_radius,
            'max_lane_distance': args.max_lane_distance,
            'min_points_single_lane': args.min_points_single_lane
        }
        
        # è¾“å‡ºé…ç½®ä¿¡æ¯
        logger.info(f"è¾“å…¥è½¨è¿¹ID: {args.trajectory_id}")
        logger.info(f"é“è·¯åˆ†æID: {args.road_analysis_id}")
        logger.info(f"å€™é€‰è½¦é“è¡¨: {config['road_analysis_lanes_table']}")
        logger.info(f"ç¼“å†²åŒºåŠå¾„: {config['buffer_radius']}ç±³")
        logger.info(f"æœ€å¤§è½¦é“è·ç¦»: {config['max_lane_distance']}ç±³")
        logger.info(f"å•è½¦é“æœ€å°‘ç‚¹æ•°: {config['min_points_single_lane']}")
        
        # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
        analyzer = TrajectoryLaneAnalyzer(config, road_analysis_id=args.road_analysis_id)
        analysis_result = analyzer.analyze_trajectory_neighbors(args.trajectory_id, args.trajectory_geom)
        
        if 'error' in analysis_result:
            logger.error(f"åˆ†æå¤±è´¥: {analysis_result['error']}")
            return 1
        
        # è¾“å‡ºç»“æœ
        logger.info("=== åˆ†æå®Œæˆ ===")
        stats = analysis_result['stats']
        logger.info(f"å€™é€‰lanes: {stats['candidate_lanes_found']} ä¸ª")
        logger.info(f"è½¨è¿¹ç‚¹æ•°: {stats['trajectory_points_found']} ä¸ª")
        logger.info(f"data_nameæ•°: {stats['unique_data_names_found']} ä¸ª")
        logger.info(f"ç¬¦åˆæ¡ä»¶çš„è½¨è¿¹: {stats['trajectories_passed_filter']} ä¸ª")
        logger.info(f"  - å¤šè½¦é“: {stats['trajectories_multi_lane']} ä¸ª")
        logger.info(f"  - è¶³å¤Ÿç‚¹æ•°: {stats['trajectories_sufficient_points']} ä¸ª")
        logger.info(f"å®Œæ•´è½¨è¿¹æ•°: {len(analysis_result['complete_trajectories'])} ä¸ª")
        
        # è¾“å‡ºè¯¦ç»†ç»“æœ
        if args.output_format == 'detailed' or args.verbose:
            logger.info("\n=== ç¬¦åˆæ¡ä»¶çš„è½¨è¿¹è¯¦æƒ… ===")
            for data_name, trajectory in analysis_result['complete_trajectories'].items():
                logger.info(f"è½¨è¿¹: {data_name}")
                logger.info(f"  - è¿‡æ»¤åŸå› : {trajectory['filter_reason']}")
                logger.info(f"  - æ¶‰åŠlanes: {len(trajectory['lanes_touched'])} ä¸ª")
                logger.info(f"  - å‘½ä¸­ç‚¹æ•°: {trajectory['hit_points_count']} ä¸ª")
                logger.info(f"  - æ€»ç‚¹æ•°: {trajectory['total_points']} ä¸ª")
                logger.info(f"  - è½¨è¿¹é•¿åº¦: {trajectory['trajectory_length']:.6f} åº¦")
                logger.info(f"  - å¹³å‡é€Ÿåº¦: {trajectory['avg_speed']} km/h")
        
        # ä¿å­˜è¾“å‡ºæ–‡ä»¶
        if args.output_file:
            output_data = {
                'input_trajectory_id': args.trajectory_id,
                'road_analysis_id': args.road_analysis_id,
                'analysis_result': analysis_result,
                'config': config,
                'timestamp': datetime.now().isoformat()
            }
            
            if args.output_format == 'geojson':
                # è¾“å‡ºGeoJSONæ ¼å¼
                geojson_features = []
                for data_name, trajectory in analysis_result['complete_trajectories'].items():
                    from shapely.geometry import mapping
                    feature = {
                        'type': 'Feature',
                        'properties': {
                            'data_name': data_name,
                            'filter_reason': trajectory['filter_reason'],
                            'lanes_touched_count': len(trajectory['lanes_touched']),
                            'lanes_touched': trajectory['lanes_touched'],
                            'hit_points_count': trajectory['hit_points_count'],
                            'total_points': trajectory['total_points'],
                            'avg_speed': trajectory['avg_speed'],
                            'max_speed': trajectory['max_speed'],
                            'avp_ratio': trajectory['avp_ratio']
                        },
                        'geometry': mapping(trajectory['geometry'])
                    }
                    geojson_features.append(feature)
                
                geojson_output = {
                    'type': 'FeatureCollection',
                    'features': geojson_features,
                    'metadata': {
                        'input_trajectory_id': args.trajectory_id,
                        'road_analysis_id': args.road_analysis_id,
                        'total_trajectories': len(geojson_features),
                        'analysis_stats': stats
                    }
                }
                
                with open(args.output_file, 'w', encoding='utf-8') as f:
                    json.dump(geojson_output, f, indent=2, ensure_ascii=False)
                
                logger.info(f"GeoJSONç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")
            else:
                # è¾“å‡ºJSONæ ¼å¼
                with open(args.output_file, 'w', encoding='utf-8') as f:
                    # åºåˆ—åŒ–å‡ ä½•å¯¹è±¡
                    def serialize_geometry(obj):
                        if hasattr(obj, 'wkt'):
                            return obj.wkt
                        elif hasattr(obj, 'isoformat'):
                            return obj.isoformat()
                        return str(obj)
                    
                    json.dump(output_data, f, indent=2, ensure_ascii=False, default=serialize_geometry)
                
                logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")
        
        return 0 if len(analysis_result['complete_trajectories']) > 0 else 1
        
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        return 1

if __name__ == '__main__':
    sys.exit(main()) 