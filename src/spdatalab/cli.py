"""å‘½ä»¤è¡Œæ¥å£æ¨¡å—ã€‚"""

import click
import logging
from pathlib import Path

from .dataset.scene_list_generator import SceneListGenerator
from .dataset.dataset_manager import DatasetManager

logger = logging.getLogger(__name__)

@click.group()
def cli():
    """Spatial-Data-Lab å·¥å…·é›†ã€‚"""
    pass


@cli.command()
@click.option('--index-file', required=True, help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
def generate_scene_list(index_file: str, output: str):
    """ç”Ÿæˆåœºæ™¯æ•°æ®åˆ—è¡¨ã€‚
    
    ä»ç´¢å¼•æ–‡ä»¶è¯»å–æ•°æ®æºä¿¡æ¯ï¼Œç”Ÿæˆåœºæ™¯æ•°æ®åˆ—è¡¨å¹¶ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶ã€‚
    
    Args:
        index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œæ ¼å¼ä¸º obs_path@duplicateN
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¿å­˜ä¸ºJSONæ ¼å¼
    """
    setup_logging()
    
    try:
        generator = SceneListGenerator()
        generator.generate_scene_list(index_file, output)
    except Exception as e:
        logger.error(f"ç”Ÿæˆåœºæ™¯æ•°æ®åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--index-file', help='ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆtxtæ ¼å¼ï¼‰')
@click.option('--training-dataset-json', help='è®­ç»ƒæ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„')
@click.option('--dataset-name', help='æ•°æ®é›†åç§°ï¼ˆä½¿ç”¨JSONè¾“å…¥æ—¶å¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨JSONä¸­çš„åç§°ï¼‰')
@click.option('--description', default='', help='æ•°æ®é›†æè¿°ï¼ˆä½¿ç”¨JSONè¾“å…¥æ—¶å¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨JSONä¸­çš„æè¿°ï¼‰')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
@click.option('--format', type=click.Choice(['json', 'parquet']), default='json', help='è¾“å‡ºæ ¼å¼')
@click.option('--defect-mode', is_flag=True, help='å¯ç”¨é—®é¢˜å•æ¨¡å¼ï¼ˆå¤„ç†é—®é¢˜å•URLï¼‰')
def build_dataset(index_file: str, training_dataset_json: str, dataset_name: str, description: str, output: str, format: str, defect_mode: bool):
    """æ„å»ºæ•°æ®é›†ç»“æ„ã€‚
    
    æ”¯æŒä¸¤ç§è¾“å…¥æ ¼å¼ï¼š
    1. ç´¢å¼•æ–‡ä»¶è¾“å…¥ï¼ˆtxtæ ¼å¼ï¼‰ï¼šä»ç´¢å¼•æ–‡ä»¶è¯»å–æ•°æ®æºä¿¡æ¯
    2. è®­ç»ƒæ•°æ®é›†JSONè¾“å…¥ï¼šä»ç»“æ„åŒ–JSONæ–‡ä»¶è¯»å–æ•°æ®é›†ä¿¡æ¯ï¼ˆæ¨èï¼‰
    
    æ”¯æŒä¸¤ç§å¤„ç†æ¨¡å¼ï¼š
    1. æ ‡å‡†æ¨¡å¼ï¼šå¤„ç†OBSè·¯å¾„æ ¼å¼çš„è®­ç»ƒæ•°æ®ï¼ˆé»˜è®¤ï¼‰
    2. é—®é¢˜å•æ¨¡å¼ï¼šå¤„ç†é—®é¢˜å•URLæ•°æ®ï¼ˆä½¿ç”¨--defect-modeï¼‰
    
    Args:
        index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼ˆtxtæ ¼å¼ï¼‰
                   æ ‡å‡†æ¨¡å¼ï¼šæ¯è¡Œæ ¼å¼ä¸º obs_path@duplicateN
                   é—®é¢˜å•æ¨¡å¼ï¼šæ¯è¡Œä¸ºé—®é¢˜å•URLæˆ–URL|å±æ€§
        training_dataset_json: è®­ç»ƒæ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«å®Œæ•´çš„æ•°æ®é›†å…ƒä¿¡æ¯
        dataset_name: æ•°æ®é›†åç§°ï¼ˆä½¿ç”¨JSONè¾“å…¥æ—¶å¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨JSONä¸­çš„åç§°ï¼‰
        description: æ•°æ®é›†æè¿°ï¼ˆä½¿ç”¨JSONè¾“å…¥æ—¶å¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨JSONä¸­çš„æè¿°ï¼‰
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¿å­˜ä¸ºæŒ‡å®šæ ¼å¼
        format: è¾“å‡ºæ ¼å¼ï¼Œjson æˆ– parquet
        defect_mode: æ˜¯å¦å¯ç”¨é—®é¢˜å•å¤„ç†æ¨¡å¼
        
    Examples:
        # ä½¿ç”¨JSONæ ¼å¼è¾“å…¥ï¼ˆæ¨èï¼‰
        python -m spdatalab.cli build-dataset \\
            --training-dataset-json training_dataset.json \\
            --output datasets/dataset.json
            
        # ä½¿ç”¨ä¼ ç»Ÿtxtæ ¼å¼è¾“å…¥
        python -m spdatalab.cli build-dataset \\
            --index-file data/index.txt \\
            --dataset-name "Dataset" \\
            --description "GOD E2E training dataset" \\
            --output datasets/dataset.json
    """
    setup_logging()
    
    # éªŒè¯è¾“å…¥å‚æ•°
    if not index_file and not training_dataset_json:
        raise click.ClickException("å¿…é¡»æä¾› --index-file æˆ– --training-dataset-json å…¶ä¸­ä¹‹ä¸€")
    
    if index_file and training_dataset_json:
        raise click.ClickException("--index-file å’Œ --training-dataset-json ä¸èƒ½åŒæ—¶ä½¿ç”¨")
    
    # ä½¿ç”¨ txt æ ¼å¼è¾“å…¥æ—¶ï¼Œdataset_name æ˜¯å¿…éœ€çš„
    if index_file and not dataset_name:
        raise click.ClickException("ä½¿ç”¨ --index-file æ—¶ï¼Œ--dataset-name æ˜¯å¿…éœ€çš„")
    
    try:
        manager = DatasetManager(defect_mode=defect_mode)
        
        if training_dataset_json:
            # ä½¿ç”¨ JSON æ ¼å¼è¾“å…¥
            dataset = manager.build_dataset_from_training_json(
                training_dataset_json, dataset_name, description
            )
            input_type = "JSONæ ¼å¼"
        else:
            # ä½¿ç”¨ä¼ ç»Ÿ txt æ ¼å¼è¾“å…¥
            dataset = manager.build_dataset_from_index(index_file, dataset_name, description)
            input_type = "txtæ ¼å¼"
        
        manager.save_dataset(dataset, output, format=format)
        
        click.echo(f"âœ… æ•°æ®é›†æ„å»ºå®Œæˆ: {dataset.name}")
        click.echo(f"   - è¾“å…¥æ ¼å¼: {input_type}")
        click.echo(f"   - å¤„ç†æ¨¡å¼: {'é—®é¢˜å•æ¨¡å¼' if defect_mode else 'æ ‡å‡†æ¨¡å¼'}")
        click.echo(f"   - å­æ•°æ®é›†æ•°é‡: {len(dataset.subdatasets)}")
        click.echo(f"   - æ€»å”¯ä¸€åœºæ™¯æ•°: {dataset.total_unique_scenes}")
        click.echo(f"   - æ€»åœºæ™¯æ•°(å«å€å¢): {dataset.total_scenes}")
        click.echo(f"   - å·²ä¿å­˜åˆ°: {output} ({format}æ ¼å¼)")
        
        if defect_mode:
            # æ˜¾ç¤ºé—®é¢˜å•å¤„ç†ç»Ÿè®¡
            stats = manager.stats
            click.echo(f"   - æˆåŠŸå¤„ç†: {stats['processed_files']} ä¸ªURL")
            click.echo(f"   - å¤±è´¥å¤„ç†: {stats['failed_files']} ä¸ªURL")
            if stats['defect_query_failed'] > 0:
                click.echo(f"   - æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {stats['defect_query_failed']} ä¸ª")
            if stats['defect_no_scene'] > 0:
                click.echo(f"   - æ— scene_id: {stats['defect_no_scene']} ä¸ª")
        
        if format == 'parquet':
            click.echo(f"   - æ¨èå®‰è£…: pip install pandas pyarrow")
            
    except Exception as e:
        logger.error(f"æ„å»ºæ•°æ®é›†å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--input', required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒJSON/Parquet/æ–‡æœ¬æ ¼å¼ï¼‰')
@click.option('--batch', type=int, default=1000, help='å¤„ç†æ‰¹æ¬¡å¤§å°')
@click.option('--insert-batch', type=int, default=1000, help='æ’å…¥æ‰¹æ¬¡å¤§å°')
@click.option('--work-dir', help='å·¥ä½œç›®å½•ï¼Œç”¨äºå­˜å‚¨è¿›åº¦æ–‡ä»¶')
@click.option('--retry-failed', is_flag=True, help='åªé‡è¯•å¤±è´¥çš„æ•°æ®')
@click.option('--show-stats', is_flag=True, help='æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡ä¿¡æ¯å¹¶é€€å‡º')
@click.option('--create-table', is_flag=True, default=True, help='æ˜¯å¦åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰')
@click.option('--use-partitioning', is_flag=True, help='ä½¿ç”¨åˆ†è¡¨æ¨¡å¼å¤„ç†ï¼ˆæŒ‰å­æ•°æ®é›†åˆ†è¡¨å­˜å‚¨ï¼‰')
@click.option('--create-unified-view', is_flag=True, default=True, help='åˆ†è¡¨æ¨¡å¼ä¸‹æ˜¯å¦åˆ›å»ºç»Ÿä¸€è§†å›¾')
@click.option('--maintain-view-only', is_flag=True, help='ä»…ç»´æŠ¤ç»Ÿä¸€è§†å›¾ï¼Œä¸å¤„ç†æ•°æ®')
def process_bbox(input: str, batch: int, insert_batch: int, work_dir: str, retry_failed: bool, show_stats: bool, create_table: bool, use_partitioning: bool, create_unified_view: bool, maintain_view_only: bool):
    """å¤„ç†è¾¹ç•Œæ¡†æ•°æ®ã€‚
    
    ä»æ•°æ®é›†æ–‡ä»¶ä¸­åŠ è½½åœºæ™¯IDï¼Œè·å–è¾¹ç•Œæ¡†ä¿¡æ¯å¹¶æ’å…¥åˆ°PostGISæ•°æ®åº“ä¸­ã€‚
    æ”¯æŒJSONã€Parquetå’Œæ–‡æœ¬æ ¼å¼çš„æ•°æ®é›†æ–‡ä»¶ã€‚
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. ä¼ ç»Ÿæ¨¡å¼ï¼šæ’å…¥åˆ°å•è¡¨clips_bboxï¼ˆé»˜è®¤ï¼‰
    2. åˆ†è¡¨æ¨¡å¼ï¼šæŒ‰å­æ•°æ®é›†åˆ†è¡¨å­˜å‚¨ï¼Œå¯é€‰æ‹©åˆ›å»ºç»Ÿä¸€è§†å›¾
    
    Args:
        input: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒJSON/Parquet/æ–‡æœ¬æ ¼å¼
        batch: å¤„ç†æ‰¹æ¬¡å¤§å°ï¼Œæ¯æ‰¹ä»æ•°æ®åº“è·å–å¤šå°‘ä¸ªåœºæ™¯çš„ä¿¡æ¯
        insert_batch: æ’å…¥æ‰¹æ¬¡å¤§å°ï¼Œæ¯æ‰¹å‘æ•°æ®åº“æ’å…¥å¤šå°‘æ¡è®°å½•
        work_dir: å·¥ä½œç›®å½•ï¼Œç”¨äºå­˜å‚¨è¿›åº¦æ–‡ä»¶
        retry_failed: æ˜¯å¦åªé‡è¯•å¤±è´¥çš„æ•°æ®
        show_stats: æ˜¯å¦æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯å¹¶é€€å‡º
        create_table: æ˜¯å¦åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        use_partitioning: æ˜¯å¦ä½¿ç”¨åˆ†è¡¨æ¨¡å¼å¤„ç†
        create_unified_view: åˆ†è¡¨æ¨¡å¼ä¸‹æ˜¯å¦åˆ›å»ºç»Ÿä¸€è§†å›¾
        maintain_view_only: æ˜¯å¦ä»…ç»´æŠ¤ç»Ÿä¸€è§†å›¾ï¼Œä¸å¤„ç†æ•°æ®
    """
    setup_logging()
    
    try:
        # æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„è¿è¡Œå‡½æ•°
        if use_partitioning:
            from .dataset.bbox import run_with_partitioning
            
            click.echo(f"ğŸ¯ å¼€å§‹åˆ†è¡¨æ¨¡å¼å¤„ç†è¾¹ç•Œæ¡†æ•°æ®:")
            click.echo(f"  - è¾“å…¥æ–‡ä»¶: {input}")
            click.echo(f"  - å¤„ç†æ‰¹æ¬¡: {batch}")
            click.echo(f"  - æ’å…¥æ‰¹æ¬¡: {insert_batch}")
            click.echo(f"  - å·¥ä½œç›®å½•: {work_dir or './bbox_import_logs'}")
            click.echo(f"  - åˆ›å»ºç»Ÿä¸€è§†å›¾: {'æ˜¯' if create_unified_view else 'å¦'}")
            click.echo(f"  - ä»…ç»´æŠ¤è§†å›¾: {'æ˜¯' if maintain_view_only else 'å¦'}")
            
            if show_stats:
                click.echo("åˆ†è¡¨æ¨¡å¼ä¸‹æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯åŠŸèƒ½æš‚æœªå®ç°")
                return
            
            run_with_partitioning(
                input_path=input,
                batch=batch,
                insert_batch=insert_batch,
                work_dir=work_dir or "./bbox_import_logs",
                create_unified_view_flag=create_unified_view,
                maintain_view_only=maintain_view_only
            )
            
            click.echo("âœ… åˆ†è¡¨æ¨¡å¼è¾¹ç•Œæ¡†å¤„ç†å®Œæˆ")
            
        else:
            from .dataset.bbox import run as bbox_run
            
            if show_stats:
                click.echo("æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡ä¿¡æ¯åŠŸèƒ½æš‚æœªå®ç°")
                return
            
            click.echo(f"ğŸ“ å¼€å§‹ä¼ ç»Ÿæ¨¡å¼å¤„ç†è¾¹ç•Œæ¡†æ•°æ®:")
            click.echo(f"  - è¾“å…¥æ–‡ä»¶: {input}")
            click.echo(f"  - å¤„ç†æ‰¹æ¬¡: {batch}")
            click.echo(f"  - æ’å…¥æ‰¹æ¬¡: {insert_batch}")
            if work_dir:
                click.echo(f"  - å·¥ä½œç›®å½•: {work_dir}")
            if retry_failed:
                click.echo(f"  - é‡è¯•æ¨¡å¼: ä»…å¤„ç†å¤±è´¥çš„æ•°æ®")
            
            bbox_run(
                input_path=input,
                batch=batch,
                insert_batch=insert_batch,
                work_dir=work_dir or "./bbox_import_logs",
                retry_failed=retry_failed,
                create_table=create_table
            )
            
            click.echo("âœ… ä¼ ç»Ÿæ¨¡å¼è¾¹ç•Œæ¡†å¤„ç†å®Œæˆ")
        
    except Exception as e:
        logger.error(f"å¤„ç†è¾¹ç•Œæ¡†å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--view-name', default='clips_bbox_unified', help='ç»Ÿä¸€è§†å›¾åç§°')
def create_unified_view(view_name: str):
    """åˆ›å»ºæˆ–æ›´æ–°bboxåˆ†è¡¨çš„ç»Ÿä¸€è§†å›¾ã€‚
    
    è‡ªåŠ¨å‘ç°æ‰€æœ‰bboxåˆ†è¡¨å¹¶åˆ›å»ºç»Ÿä¸€è§†å›¾ï¼Œä¾¿äºè·¨è¡¨æŸ¥è¯¢ã€‚
    
    Args:
        view_name: ç»Ÿä¸€è§†å›¾åç§°
    """
    setup_logging()
    
    try:
        from .dataset.bbox import create_unified_view as create_view_func
        from sqlalchemy import create_engine
        
        # è¿™é‡Œéœ€è¦å¯¼å…¥æ•°æ®åº“è¿æ¥é…ç½®
        LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"
        eng = create_engine(LOCAL_DSN, future=True)
        
        click.echo(f"ğŸ”§ åˆ›å»ºç»Ÿä¸€è§†å›¾: {view_name}")
        
        success = create_view_func(eng, view_name)
        
        if success:
            click.echo(f"âœ… ç»Ÿä¸€è§†å›¾ {view_name} åˆ›å»ºæˆåŠŸ")
        else:
            click.echo(f"âŒ ç»Ÿä¸€è§†å›¾ {view_name} åˆ›å»ºå¤±è´¥")
            
    except Exception as e:
        logger.error(f"åˆ›å»ºç»Ÿä¸€è§†å›¾å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--view-name', default='clips_bbox_unified', help='ç»Ÿä¸€è§†å›¾åç§°')
def maintain_unified_view(view_name: str):
    """ç»´æŠ¤bboxåˆ†è¡¨çš„ç»Ÿä¸€è§†å›¾ã€‚
    
    æ£€æŸ¥å¹¶æ›´æ–°ç»Ÿä¸€è§†å›¾ä»¥åŒ…å«æ‰€æœ‰å½“å‰çš„åˆ†è¡¨ã€‚
    
    Args:
        view_name: ç»Ÿä¸€è§†å›¾åç§°
    """
    setup_logging()
    
    try:
        from .dataset.bbox import maintain_unified_view as maintain_view_func
        from sqlalchemy import create_engine
        
        # è¿™é‡Œéœ€è¦å¯¼å…¥æ•°æ®åº“è¿æ¥é…ç½®
        LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"
        eng = create_engine(LOCAL_DSN, future=True)
        
        click.echo(f"ğŸ”§ ç»´æŠ¤ç»Ÿä¸€è§†å›¾: {view_name}")
        
        success = maintain_view_func(eng, view_name)
        
        if success:
            click.echo(f"âœ… ç»Ÿä¸€è§†å›¾ {view_name} ç»´æŠ¤æˆåŠŸ")
        else:
            click.echo(f"âŒ ç»Ÿä¸€è§†å›¾ {view_name} ç»´æŠ¤å¤±è´¥")
            
    except Exception as e:
        logger.error(f"ç»´æŠ¤ç»Ÿä¸€è§†å›¾å¤±è´¥: {str(e)}")
        raise

@cli.command()
def list_bbox_tables():
    """åˆ—å‡ºæ‰€æœ‰bboxç›¸å…³çš„æ•°æ®è¡¨ã€‚
    
    æ˜¾ç¤ºæ•°æ®åº“ä¸­æ‰€æœ‰bboxåˆ†è¡¨çš„ä¿¡æ¯ã€‚
    """
    setup_logging()
    
    try:
        from .dataset.bbox import list_bbox_tables as list_tables_func
        from sqlalchemy import create_engine
        
        # è¿™é‡Œéœ€è¦å¯¼å…¥æ•°æ®åº“è¿æ¥é…ç½®
        LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"
        eng = create_engine(LOCAL_DSN, future=True)
        
        click.echo("ğŸ“‹ æŸ¥è¯¢bboxæ•°æ®è¡¨...")
        
        tables = list_tables_func(eng)
        
        if tables:
            click.echo(f"æ‰¾åˆ° {len(tables)} ä¸ªbboxè¡¨:")
            for i, table in enumerate(tables, 1):
                click.echo(f"  {i:2d}. {table}")
        else:
            click.echo("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•bboxè¡¨")
            
    except Exception as e:
        logger.error(f"åˆ—å‡ºbboxè¡¨å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--index-file', required=True, help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
@click.option('--dataset-name', required=True, help='æ•°æ®é›†åç§°')
@click.option('--description', default='', help='æ•°æ®é›†æè¿°')
@click.option('--output', required=True, help='è¾“å‡ºæ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--format', type=click.Choice(['json', 'parquet']), default='json', help='æ•°æ®é›†ä¿å­˜æ ¼å¼')
@click.option('--batch', type=int, default=1000, help='è¾¹ç•Œæ¡†å¤„ç†æ‰¹æ¬¡å¤§å°')
@click.option('--insert-batch', type=int, default=1000, help='è¾¹ç•Œæ¡†æ’å…¥æ‰¹æ¬¡å¤§å°')
@click.option('--buffer-meters', type=int, default=50, help='ç¼“å†²åŒºå¤§å°ï¼ˆç±³ï¼‰')
@click.option('--precise-buffer', is_flag=True, help='ä½¿ç”¨ç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒº')
@click.option('--skip-bbox', is_flag=True, help='è·³è¿‡è¾¹ç•Œæ¡†å¤„ç†')
@click.option('--defect-mode', is_flag=True, help='å¯ç”¨é—®é¢˜å•æ¨¡å¼ï¼ˆå¤„ç†é—®é¢˜å•URLï¼‰')
def build_dataset_with_bbox(index_file: str, dataset_name: str, description: str, output: str, 
                           format: str, batch: int, insert_batch: int, buffer_meters: int, 
                           precise_buffer: bool, skip_bbox: bool, defect_mode: bool):
    """æ„å»ºæ•°æ®é›†å¹¶å¤„ç†è¾¹ç•Œæ¡†ï¼ˆå®Œæ•´å·¥ä½œæµç¨‹ï¼‰ã€‚
    
    ä»ç´¢å¼•æ–‡ä»¶æ„å»ºæ•°æ®é›†ï¼Œä¿å­˜åè‡ªåŠ¨å¤„ç†è¾¹ç•Œæ¡†æ•°æ®ï¼Œæä¾›ä¸€é”®å¼å®Œæ•´å·¥ä½œæµç¨‹ã€‚
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. æ ‡å‡†æ¨¡å¼ï¼šå¤„ç†OBSè·¯å¾„æ ¼å¼çš„è®­ç»ƒæ•°æ®ï¼ˆé»˜è®¤ï¼‰
    2. é—®é¢˜å•æ¨¡å¼ï¼šå¤„ç†é—®é¢˜å•URLæ•°æ®ï¼ˆä½¿ç”¨--defect-modeï¼‰
    
    Args:
        index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„
                   æ ‡å‡†æ¨¡å¼ï¼šæ¯è¡Œæ ¼å¼ä¸º obs_path@duplicateN
                   é—®é¢˜å•æ¨¡å¼ï¼šæ¯è¡Œä¸ºé—®é¢˜å•URLæˆ–URL|å±æ€§
        dataset_name: æ•°æ®é›†åç§°
        description: æ•°æ®é›†æè¿°
        output: è¾“å‡ºæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        format: æ•°æ®é›†ä¿å­˜æ ¼å¼ï¼Œjson æˆ– parquet
        batch: è¾¹ç•Œæ¡†å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch: è¾¹ç•Œæ¡†æ’å…¥æ‰¹æ¬¡å¤§å°
        buffer_meters: ç¼“å†²åŒºå¤§å°ï¼ˆç±³ï¼‰
        precise_buffer: æ˜¯å¦ä½¿ç”¨ç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒº
        skip_bbox: æ˜¯å¦è·³è¿‡è¾¹ç•Œæ¡†å¤„ç†
        defect_mode: æ˜¯å¦å¯ç”¨é—®é¢˜å•å¤„ç†æ¨¡å¼
    """
    setup_logging()
    
    try:
        # æ­¥éª¤1ï¼šæ„å»ºæ•°æ®é›†
        click.echo("=== æ­¥éª¤1: æ„å»ºæ•°æ®é›† ===")
        click.echo(f"   - å¤„ç†æ¨¡å¼: {'é—®é¢˜å•æ¨¡å¼' if defect_mode else 'æ ‡å‡†æ¨¡å¼'}")
        manager = DatasetManager(defect_mode=defect_mode)
        dataset = manager.build_dataset_from_index(index_file, dataset_name, description)
        
        # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_dataset_stats(dataset)
        click.echo(f"æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
        click.echo(f"  - æ•°æ®é›†åç§°: {stats['dataset_name']}")
        click.echo(f"  - å­æ•°æ®é›†æ•°é‡: {stats['subdataset_count']}")
        click.echo(f"  - å”¯ä¸€åœºæ™¯æ•°: {stats['total_unique_scenes']}")
        click.echo(f"  - æ€»åœºæ™¯æ•°(å«å€å¢): {stats['total_scenes_with_duplicates']}")
        
        # æ­¥éª¤2ï¼šä¿å­˜æ•°æ®é›†
        click.echo("=== æ­¥éª¤2: ä¿å­˜æ•°æ®é›† ===")
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(output).parent.mkdir(parents=True, exist_ok=True)
        manager.save_dataset(dataset, output, format=format)
        click.echo(f"âœ… æ•°æ®é›†å·²ä¿å­˜åˆ°: {output} ({format}æ ¼å¼)")
        
        # æ­¥éª¤3ï¼šå¤„ç†è¾¹ç•Œæ¡†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not skip_bbox:
            click.echo("=== æ­¥éª¤3: å¤„ç†è¾¹ç•Œæ¡† ===")
            
            from .dataset.bbox import run as bbox_run
            
            click.echo(f"è¾¹ç•Œæ¡†å¤„ç†é…ç½®:")
            click.echo(f"  - å¤„ç†æ‰¹æ¬¡: {batch}")
            click.echo(f"  - æ’å…¥æ‰¹æ¬¡: {insert_batch}")
            click.echo(f"  - ç¼“å†²åŒº: {buffer_meters}ç±³")
            click.echo(f"  - ç²¾ç¡®æ¨¡å¼: {'æ˜¯' if precise_buffer else 'å¦'}")
            
            bbox_run(
                input_path=output,
                batch=batch,
                insert_batch=insert_batch,
                buffer_meters=buffer_meters,
                use_precise_buffer=precise_buffer
            )
            
            click.echo("âœ… è¾¹ç•Œæ¡†å¤„ç†å®Œæˆ")
        else:
            click.echo("=== æ­¥éª¤3: è·³è¿‡è¾¹ç•Œæ¡†å¤„ç† ===")
        
        click.echo("ğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"å®Œæ•´å·¥ä½œæµç¨‹å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--subdataset', default=None, help='å­æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰')
@click.option('--output', default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
def list_scenes(dataset_file: str, subdataset: str, output: str):
    """åˆ—å‡ºæ•°æ®é›†ä¸­çš„åœºæ™¯IDã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        subdataset: å­æ•°æ®é›†åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™åˆ—å‡ºæ‰€æœ‰åœºæ™¯
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è¾“å‡ºåˆ°æ§åˆ¶å°
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        scene_ids = manager.list_scene_ids(dataset, subdataset)
        
        if output:
            with open(output, 'w', encoding='utf-8') as f:
                for scene_id in scene_ids:
                    f.write(f"{scene_id}\n")
            click.echo(f"âœ… åœºæ™¯IDåˆ—è¡¨å·²ä¿å­˜åˆ°: {output}")
        else:
            click.echo(f"åœºæ™¯IDåˆ—è¡¨ ({'å­æ•°æ®é›†: ' + subdataset if subdataset else 'å…¨éƒ¨'}):")
            for scene_id in scene_ids:
                click.echo(f"  {scene_id}")
        
        click.echo(f"æ€»å…± {len(scene_ids)} ä¸ªåœºæ™¯")
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºåœºæ™¯å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--subdataset', default=None, help='å­æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
def generate_scene_ids(dataset_file: str, subdataset: str, output: str):
    """ç”ŸæˆåŒ…å«å€å¢çš„åœºæ™¯IDåˆ—è¡¨ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        subdataset: å­æ•°æ®é›†åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™å¤„ç†æ‰€æœ‰å­æ•°æ®é›†
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        
        count = 0
        with open(output, 'w', encoding='utf-8') as f:
            for scene_id in manager.generate_scene_list_with_duplication(dataset, subdataset):
                f.write(f"{scene_id}\n")
                count += 1
        
        click.echo(f"âœ… åœºæ™¯IDåˆ—è¡¨ï¼ˆå«å€å¢ï¼‰å·²ä¿å­˜åˆ°: {output}")
        click.echo(f"æ€»å…± {count} ä¸ªåœºæ™¯ï¼ˆå«å€å¢ï¼‰")
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆåœºæ™¯IDåˆ—è¡¨å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
def dataset_info(dataset_file: str):
    """æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        
        click.echo(f"æ•°æ®é›†ä¿¡æ¯:")
        click.echo(f"  åç§°: {dataset.name}")
        click.echo(f"  æè¿°: {dataset.description}")
        click.echo(f"  åˆ›å»ºæ—¶é—´: {dataset.created_at}")
        click.echo(f"  å­æ•°æ®é›†æ•°é‡: {len(dataset.subdatasets)}")
        click.echo(f"  æ€»å”¯ä¸€åœºæ™¯æ•°: {dataset.total_unique_scenes}")
        click.echo(f"  æ€»åœºæ™¯æ•°(å«å€å¢): {dataset.total_scenes}")
        
        if dataset.subdatasets:
            click.echo(f"\nå­æ•°æ®é›†è¯¦æƒ…:")
            for i, subdataset in enumerate(dataset.subdatasets, 1):
                click.echo(f"  {i}. {subdataset.name}")
                click.echo(f"     - OBSè·¯å¾„: {subdataset.obs_path}")
                click.echo(f"     - åœºæ™¯æ•°: {subdataset.scene_count}")
                click.echo(f"     - å€å¢å› å­: {subdataset.duplication_factor}")
                click.echo(f"     - å€å¢ååœºæ™¯æ•°: {subdataset.scene_count * subdataset.duplication_factor}")
        
    except Exception as e:
        logger.error(f"æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
@click.option('--include-duplicates', is_flag=True, help='æ˜¯å¦åŒ…å«å€å¢çš„åœºæ™¯ID')
def export_scene_ids(dataset_file: str, output: str, include_duplicates: bool):
    """å¯¼å‡ºåœºæ™¯IDä¸ºParquetæ ¼å¼ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆparquetæ ¼å¼ï¼‰
        include_duplicates: æ˜¯å¦åŒ…å«å€å¢çš„åœºæ™¯ID
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        manager.export_scene_ids_parquet(dataset, output, include_duplicates)
        
        click.echo(f"âœ… åœºæ™¯IDå·²å¯¼å‡ºåˆ°: {output}")
        if include_duplicates:
            click.echo(f"   - åŒ…å«å€å¢ï¼Œæ€»è®°å½•æ•°: {dataset.total_scenes}")
        else:
            click.echo(f"   - ä¸å«å€å¢ï¼Œæ€»è®°å½•æ•°: {dataset.total_unique_scenes}")
            
    except Exception as e:
        logger.error(f"å¯¼å‡ºåœºæ™¯IDå¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--parquet-file', required=True, help='Parquetæ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--subdataset', default=None, help='æŒ‰å­æ•°æ®é›†è¿‡æ»¤')
@click.option('--duplication-factor', type=int, default=None, help='æŒ‰å€å¢å› å­è¿‡æ»¤')
@click.option('--output', default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
def query_parquet(parquet_file: str, subdataset: str, duplication_factor: int, output: str):
    """æŸ¥è¯¢Parquetæ ¼å¼æ•°æ®é›†ã€‚
    
    Args:
        parquet_file: Parquetæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        subdataset: æŒ‰å­æ•°æ®é›†åç§°è¿‡æ»¤
        duplication_factor: æŒ‰å€å¢å› å­è¿‡æ»¤
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filters = {}
        if subdataset:
            filters['subdataset_name'] = subdataset
        if duplication_factor:
            filters['duplication_factor'] = duplication_factor
            
        df = manager.query_scenes_parquet(parquet_file, **filters)
        
        if output:
            df.to_parquet(output, index=False)
            click.echo(f"âœ… æŸ¥è¯¢ç»“æœå·²ä¿å­˜åˆ°: {output}")
        else:
            click.echo(f"æŸ¥è¯¢ç»“æœ:")
            click.echo(f"  - åŒ¹é…è®°å½•æ•°: {len(df)}")
            if len(df) > 0:
                click.echo(f"  - å”¯ä¸€å­æ•°æ®é›†: {df['subdataset_name'].nunique()}")
                click.echo(f"  - å­æ•°æ®é›†åˆ—è¡¨:")
                for name in df['subdataset_name'].unique():
                    count = len(df[df['subdataset_name'] == name])
                    click.echo(f"    * {name}: {count} ä¸ªåœºæ™¯")
        
        click.echo(f"æ€»è®°å½•æ•°: {len(df)}")
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢Parquetæ•°æ®é›†å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--output-format', type=click.Choice(['json', 'parquet']), default='json', help='è¾“å‡ºæ ¼å¼')
def dataset_stats(dataset_file: str, output_format: str):
    """æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        output_format: è¾“å‡ºæ ¼å¼
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        stats = manager.get_dataset_stats(dataset)
        
        if output_format == 'json':
            click.echo(json.dumps(stats, ensure_ascii=False, indent=2))
        else:
            click.echo(f"æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
            click.echo(f"  æ•°æ®é›†åç§°: {stats['dataset_name']}")
            click.echo(f"  å­æ•°æ®é›†æ•°é‡: {stats['subdataset_count']}")
            click.echo(f"  æ€»å”¯ä¸€åœºæ™¯æ•°: {stats['total_unique_scenes']}")
            click.echo(f"  æ€»åœºæ™¯æ•°(å«å€å¢): {stats['total_scenes_with_duplicates']}")
            
            click.echo(f"\nå­æ•°æ®é›†è¯¦æƒ…:")
            for i, sub_stats in enumerate(stats['subdatasets'], 1):
                click.echo(f"  {i}. {sub_stats['name']}")
                click.echo(f"     - åœºæ™¯æ•°: {sub_stats['scene_count']}")
                click.echo(f"     - å€å¢å› å­: {sub_stats['duplication_factor']}")
                click.echo(f"     - å€å¢ååœºæ™¯æ•°: {sub_stats['total_scenes_with_duplicates']}")
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--left-table', default='clips_bbox', help='å·¦è¡¨åï¼ˆå¦‚clips_bboxï¼‰')
@click.option('--right-table', required=True, help='å³è¡¨åï¼ˆå¦‚intersectionsï¼‰')
@click.option('--num-bbox', type=int, default=1000, help='è¦å¤„ç†çš„bboxæ•°é‡')
@click.option('--city-filter', help='åŸå¸‚è¿‡æ»¤æ¡ä»¶')
@click.option('--chunk-size', type=int, help='åˆ†å—å¤§å°ï¼ˆç”¨äºå¤§è§„æ¨¡æ•°æ®å¤„ç†ï¼‰')
@click.option('--spatial-relation', type=click.Choice([
    'intersects', 'within', 'contains', 'touches', 'crosses', 'overlaps', 'dwithin'
]), default='intersects', help='ç©ºé—´å…³ç³»')
@click.option('--distance-meters', type=float, help='è·ç¦»é˜ˆå€¼ï¼ˆä»…ç”¨äºdwithinå…³ç³»ï¼‰')
@click.option('--buffer-meters', type=float, default=0.0, help='ç¼“å†²åŒºåŠå¾„ï¼ˆç±³ï¼‰')
@click.option('--output-table', help='è¾“å‡ºæ•°æ®åº“è¡¨å')
@click.option('--output-file', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆCSVæ ¼å¼ï¼‰')
@click.option('--fields-to-add', help='è¦æ·»åŠ çš„å­—æ®µåˆ—è¡¨ï¼Œé€—å·åˆ†éš”ï¼ˆå¦‚: field1,field2ï¼‰')
@click.option('--discard-nonmatching', is_flag=True, help='ä¸¢å¼ƒæœªåŒ¹é…çš„è®°å½•ï¼ˆINNER JOINï¼‰')
@click.option('--summarize', is_flag=True, help='å¼€å¯ç»Ÿè®¡æ±‡æ€»æ¨¡å¼')
@click.option('--summary-fields', help='ç»Ÿè®¡å­—æ®µï¼Œæ ¼å¼: field1:method1,field2:method2')
def spatial_join(left_table: str, right_table: str, num_bbox: int, city_filter: str, chunk_size: int,
                spatial_relation: str, distance_meters: float, buffer_meters: float, output_table: str,
                output_file: str, fields_to_add: str, discard_nonmatching: bool,
                summarize: bool, summary_fields: str):
    """æ‰§è¡Œç©ºé—´è¿æ¥åˆ†æ - ç±»ä¼¼QGISçš„join attributes by locationã€‚
    
    Examples:
        # åŸºç¡€ç›¸äº¤åˆ†æ
        spdatalab spatial-join --right-table intersections
        
        # è·ç¦»èŒƒå›´å†…è¿æ¥
        spdatalab spatial-join --right-table intersections --spatial-relation dwithin --distance-meters 50
        
        # é€‰æ‹©ç‰¹å®šå­—æ®µ
        spdatalab spatial-join --right-table intersections --fields-to-add "intersection_id,intersection_type"
        
        # ç»Ÿè®¡æ±‡æ€»
        spdatalab spatial-join --right-table intersections --buffer-meters 50 --summarize --summary-fields "count:count,distance:distance"
    """
    setup_logging()
    
    try:
        from .fusion import SpatialJoin
        
        click.echo(f"æ‰§è¡Œç©ºé—´è¿æ¥:")
        click.echo(f"  - å·¦è¡¨: {left_table}")
        click.echo(f"  - å³è¡¨: {right_table}")
        click.echo(f"  - å¤„ç†æ•°é‡: {num_bbox} ä¸ªbbox")
        if city_filter:
            click.echo(f"  - åŸå¸‚è¿‡æ»¤: {city_filter}")
        if chunk_size:
            click.echo(f"  - åˆ†å—å¤§å°: {chunk_size}")
        click.echo(f"  - ç©ºé—´å…³ç³»: {spatial_relation}")
        if distance_meters:
            click.echo(f"  - è·ç¦»é˜ˆå€¼: {distance_meters}ç±³")
        if buffer_meters > 0:
            click.echo(f"  - ç¼“å†²åŒº: {buffer_meters}ç±³")
        if discard_nonmatching:
            click.echo(f"  - è¿æ¥ç±»å‹: INNER JOIN (ä¸¢å¼ƒæœªåŒ¹é…)")
        else:
            click.echo(f"  - è¿æ¥ç±»å‹: LEFT JOIN (ä¿ç•™æ‰€æœ‰å·¦è¡¨è®°å½•)")
        
        # è§£æå­—æ®µé€‰æ‹©
        parsed_fields_to_add = None
        if fields_to_add:
            parsed_fields_to_add = [f.strip() for f in fields_to_add.split(',')]
        
        # è§£æç»Ÿè®¡å­—æ®µ
        parsed_summary_fields = None
        if summary_fields:
            parsed_summary_fields = {}
            for field_spec in summary_fields.split(','):
                if ':' in field_spec:
                    field, method = field_spec.split(':', 1)
                    parsed_summary_fields[field.strip()] = method.strip()
                else:
                    parsed_summary_fields[field_spec.strip()] = "count"
        
        # æ³¨æ„ï¼šå½“å‰ç‰ˆæœ¬ä¸»è¦æ”¯æŒpolygonç›¸äº¤æŸ¥è¯¢
        # å…¶ä»–ç©ºé—´å…³ç³»å’Œå­—æ®µé€‰æ‹©åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­
        
        click.echo("âš ï¸  å½“å‰ç‰ˆæœ¬ä¸“æ³¨äºé«˜æ€§èƒ½polygonç›¸äº¤æŸ¥è¯¢")
        click.echo("   å¤æ‚çš„ç©ºé—´å…³ç³»å’Œå­—æ®µé€‰æ‹©åŠŸèƒ½å°†åœ¨åç»­ç‰ˆæœ¬æä¾›")
        
        # ä½¿ç”¨ç”Ÿäº§çº§ç©ºé—´è¿æ¥
        spatial_joiner = SpatialJoin()
        
        try:
            # å½“å‰ç‰ˆæœ¬åªæ”¯æŒåŸºæœ¬çš„polygonç›¸äº¤
            result, stats = spatial_joiner.polygon_intersect(
                num_bbox=num_bbox,
                city_filter=city_filter,
                chunk_size=chunk_size
            )
            
            # å°†ç»“æœè½¬æ¢ä¸ºDataFrameæ ¼å¼ï¼ˆå…¼å®¹æ€§ï¼‰
            if len(result) > 0:
                # ä¸ºäº†å…¼å®¹åç»­å¤„ç†ï¼Œç¡®ä¿æœ‰scene_tokenåˆ—
                result = result.rename(columns={'scene_token': 'scene_token'})
            else:
                import pandas as pd
                result = pd.DataFrame()
        except Exception as join_error:
            # æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯å’Œå»ºè®®
            error_msg = str(join_error)
            if "does not exist" in error_msg and "relation" in error_msg:
                click.echo(f"âŒ è¡¨ '{right_table}' ä¸å­˜åœ¨")
                click.echo("\nğŸ’¡ è¯·æ£€æŸ¥ä»¥ä¸‹äº‹é¡¹:")
                click.echo("1. ç¡®ä¿æ•°æ®åº“è¿æ¥æ­£å¸¸:")
                click.echo("   make psql")
                click.echo("\n2. æ£€æŸ¥clips_bboxè¡¨æ˜¯å¦å­˜åœ¨:")
                click.echo("   SELECT count(*) FROM clips_bbox;")
                click.echo("\n3. æ£€æŸ¥è¿œç«¯æ•°æ®åº“è¿æ¥:")
                click.echo("   å½“å‰ç‰ˆæœ¬ä½¿ç”¨å†…ç½®çš„è¿œç«¯è¿æ¥é…ç½®")
                click.echo("\n4. å¦‚éœ€å¤„ç†å…¶ä»–è¡¨ï¼Œè¯·ä½¿ç”¨å®Œæ•´çš„APIæ¥å£:")
            raise
        
        click.echo(f"âœ… ç©ºé—´è¿æ¥å®Œæˆï¼Œå…± {len(result)} æ¡è®°å½•")
        click.echo(f"  - ä½¿ç”¨ç­–ç•¥: {stats['strategy']}")
        click.echo(f"  - å¤„ç†é€Ÿåº¦: {stats['speed_bbox_per_sec']:.1f} bbox/ç§’")
        click.echo(f"  - æ€»è€—æ—¶: {stats['total_time']:.2f}ç§’")
        
        # æ˜¾ç¤ºåŸºç¡€ç»Ÿè®¡
        if len(result) > 0:
            if 'scene_token' in result.columns:
                click.echo(f"  - å”¯ä¸€åœºæ™¯æ•°: {result['scene_token'].nunique()}")
        
        # å¯¼å‡ºæ–‡ä»¶
        if output_file and len(result) > 0:
            # ç§»é™¤å‡ ä½•åˆ—å¹¶ä¿å­˜ä¸ºCSV
            df = result.drop(columns=['geometry']) if 'geometry' in result.columns else result
            df.to_csv(output_file, index=False, encoding='utf-8')
            click.echo(f"  - ç»“æœå·²å¯¼å‡ºåˆ°: {output_file}")
        
    except Exception as e:
        logger.error(f"ç©ºé—´è¿æ¥å¤±è´¥: {str(e)}")
        raise

@cli.command()
def list_layers():
    """æŸ¥çœ‹å¯ç”¨çš„æ ‡å‡†åŒ–å›¾å±‚ä¿¡æ¯ã€‚
    
    æ˜¾ç¤ºå½“å‰FDWé…ç½®ä¸­å¯ç”¨çš„æ ‡å‡†åŒ–å›¾å±‚åˆ—è¡¨å’ŒåŸºæœ¬ä¿¡æ¯ã€‚
    """
    setup_logging()
    
    try:
        click.echo("âš ï¸  å½“å‰ç‰ˆæœ¬ä¸»è¦ä¸“æ³¨äºpolygonç›¸äº¤æŸ¥è¯¢")
        click.echo("å›¾å±‚ç®¡ç†åŠŸèƒ½å°†åœ¨åç»­ç‰ˆæœ¬æä¾›")
        click.echo()
        
        click.echo("ğŸ“‹ å½“å‰æ”¯æŒçš„åŠŸèƒ½:")
        click.echo("  - clips_bbox ä¸ full_intersection çš„é«˜æ€§èƒ½ç›¸äº¤æŸ¥è¯¢")
        click.echo("  - è‡ªåŠ¨ç­–ç•¥é€‰æ‹©ï¼ˆæ‰¹é‡æŸ¥è¯¢ vs åˆ†å—æŸ¥è¯¢ï¼‰")
        click.echo("  - è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡")
        click.echo()
        
        click.echo("ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
        click.echo("  # Python API")
        click.echo("  from spdatalab.fusion import quick_spatial_join")
        click.echo("  result, stats = quick_spatial_join(num_bbox=100)")
        click.echo()
        click.echo("  # å‘½ä»¤è¡Œï¼ˆåŸºç¡€åŠŸèƒ½ï¼‰")
        click.echo("  spdatalab spatial-join --right-table intersections")
        
        return
        
        # ä»¥ä¸‹ä»£ç ä¿ç•™ä»¥ä¾¿å°†æ¥å¯ç”¨
        # from .fusion import SpatialJoin
        # spatial_joiner = SpatialJoin()
        
        # æŸ¥è¯¢å¯ç”¨å›¾å±‚ä¿¡æ¯
        try:
            import pandas as pd
            # layers_df = pd.read_sql(
            #     "SELECT * FROM available_layers ORDER BY layer_name",
            #     spatial_joiner.engine
            # )
            
            if len(layers_df) == 0:
                click.echo("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„å›¾å±‚")
                click.echo("è¯·ç¡®ä¿å·²æ­£ç¡®é…ç½®FDWï¼š")
                click.echo("  psql -h local_pg -U postgres -f sql/01_fdw_remote.sql")
                return
            
            click.echo("ğŸ“‹ å¯ç”¨çš„æ ‡å‡†åŒ–å›¾å±‚:")
            click.echo("=" * 80)
            
            for _, layer in layers_df.iterrows():
                click.echo(f"ğŸ—‚ï¸  {layer['layer_name']}")
                click.echo(f"   æè¿°: {layer['description']}")
                click.echo(f"   æºè¡¨: {layer['source_table']}")
                click.echo(f"   å‡ ä½•ç±»å‹: {layer['geometry_type']}")
                click.echo(f"   è®°å½•æ•°: {layer['record_count']:,}")
                click.echo()
            
            click.echo("ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
            for _, layer in layers_df.iterrows():
                click.echo(f"  spdatalab spatial-join --right-table {layer['layer_name']} --buffer-meters 50")
            
        except Exception as e:
            if "available_layers" in str(e):
                click.echo("âŒ available_layers è§†å›¾ä¸å­˜åœ¨")
                click.echo("è¯·é‡æ–°é…ç½®FDWä»¥åˆ›å»ºæ ‡å‡†åŒ–è§†å›¾ï¼š")
                click.echo("  psql -h local_pg -U postgres -f sql/01_fdw_remote.sql")
            else:
                click.echo(f"âŒ æŸ¥è¯¢å›¾å±‚ä¿¡æ¯å¤±è´¥: {str(e)}")
            
    except Exception as e:
        logger.error(f"åˆ—å‡ºå›¾å±‚å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--view-name', default='clips_bbox_unified_qgis', help='QGISå…¼å®¹è§†å›¾åç§°')
def create_qgis_view(view_name: str):
    """åˆ›å»ºQGISå…¼å®¹çš„ç»Ÿä¸€è§†å›¾ã€‚
    
    åˆ›å»ºå¸¦æœ‰å…¨å±€å”¯ä¸€IDçš„ç»Ÿä¸€è§†å›¾ï¼Œè§£å†³QGISåŠ è½½PostgreSQLè§†å›¾çš„å…¼å®¹æ€§é—®é¢˜ã€‚
    
    Args:
        view_name: QGISå…¼å®¹è§†å›¾åç§°
    """
    setup_logging()
    
    try:
        from .dataset.bbox import create_qgis_compatible_unified_view
        from .db import get_psql_engine
        
        click.echo(f"ğŸ”§ åˆ›å»ºQGISå…¼å®¹ç»Ÿä¸€è§†å›¾: {view_name}")
        
        eng = get_psql_engine()
        success = create_qgis_compatible_unified_view(eng, view_name)
        
        if success:
            click.echo(f"âœ… QGISå…¼å®¹è§†å›¾ {view_name} åˆ›å»ºæˆåŠŸ")
            click.echo(f"ğŸ“ åœ¨QGISä¸­è¿æ¥PostgreSQLæ•°æ®åº“æ—¶ï¼š")
            click.echo(f"   1. é€‰æ‹©è§†å›¾: {view_name}")
            click.echo(f"   2. ä¸»é”®åˆ—é€‰æ‹©: qgis_id")
            click.echo(f"   3. å‡ ä½•åˆ—é€‰æ‹©: geometry")
        else:
            click.echo(f"âŒ QGISå…¼å®¹è§†å›¾ {view_name} åˆ›å»ºå¤±è´¥")
            
    except Exception as e:
        logger.error(f"åˆ›å»ºQGISå…¼å®¹è§†å›¾å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--view-name', default='clips_bbox_unified_mat', help='ç‰©åŒ–è§†å›¾åç§°')
def create_materialized_view(view_name: str):
    """åˆ›å»ºç‰©åŒ–ç»Ÿä¸€è§†å›¾ã€‚
    
    åˆ›å»ºç‰©åŒ–è§†å›¾ä»¥æä¾›æ›´å¥½çš„QGISæ€§èƒ½ï¼Œé€‚åˆå¤§æ•°æ®é‡åœºæ™¯ã€‚
    ç‰©åŒ–è§†å›¾å°†æŸ¥è¯¢ç»“æœç‰©ç†å­˜å‚¨ï¼ŒæŸ¥è¯¢é€Ÿåº¦å¿«ä½†éœ€è¦æ‰‹åŠ¨åˆ·æ–°ã€‚
    
    Args:
        view_name: ç‰©åŒ–è§†å›¾åç§°
    """
    setup_logging()
    
    try:
        from .dataset.bbox import create_materialized_unified_view
        from .db import get_psql_engine
        
        click.echo(f"ğŸ”§ åˆ›å»ºç‰©åŒ–ç»Ÿä¸€è§†å›¾: {view_name}")
        
        eng = get_psql_engine()
        success = create_materialized_unified_view(eng, view_name)
        
        if success:
            click.echo(f"âœ… ç‰©åŒ–è§†å›¾ {view_name} åˆ›å»ºæˆåŠŸ")
            click.echo(f"ğŸ“ åœ¨QGISä¸­è¿æ¥PostgreSQLæ•°æ®åº“æ—¶ï¼š")
            click.echo(f"   1. é€‰æ‹©ç‰©åŒ–è§†å›¾: {view_name}")
            click.echo(f"   2. ä¸»é”®åˆ—é€‰æ‹©: qgis_id")
            click.echo(f"   3. å‡ ä½•åˆ—é€‰æ‹©: geometry")
            click.echo(f"âš ï¸  æé†’ï¼šæ•°æ®æ›´æ–°åè®°å¾—åˆ·æ–°ç‰©åŒ–è§†å›¾")
        else:
            click.echo(f"âŒ ç‰©åŒ–è§†å›¾ {view_name} åˆ›å»ºå¤±è´¥")
            
    except Exception as e:
        logger.error(f"åˆ›å»ºç‰©åŒ–è§†å›¾å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--view-name', default='clips_bbox_unified_mat', help='ç‰©åŒ–è§†å›¾åç§°')
def refresh_materialized_view(view_name: str):
    """åˆ·æ–°ç‰©åŒ–è§†å›¾ã€‚
    
    æ›´æ–°ç‰©åŒ–è§†å›¾çš„æ•°æ®ï¼Œä½¿å…¶åŒ…å«æœ€æ–°çš„åˆ†è¡¨æ•°æ®ã€‚
    åœ¨åˆ†è¡¨æ•°æ®æœ‰æ›´æ–°æ—¶éœ€è¦è¿è¡Œæ­¤å‘½ä»¤ã€‚
    
    Args:
        view_name: è¦åˆ·æ–°çš„ç‰©åŒ–è§†å›¾åç§°
    """
    setup_logging()
    
    try:
        from .dataset.bbox import refresh_materialized_view as refresh_func
        from .db import get_psql_engine
        
        click.echo(f"ğŸ”„ åˆ·æ–°ç‰©åŒ–è§†å›¾: {view_name}")
        
        eng = get_psql_engine()
        success = refresh_func(eng, view_name)
        
        if success:
            click.echo(f"âœ… ç‰©åŒ–è§†å›¾ {view_name} åˆ·æ–°å®Œæˆ")
            click.echo(f"ğŸ¯ æ–°æ•°æ®å·²å¯åœ¨QGISä¸­ä½¿ç”¨")
        else:
            click.echo(f"âŒ ç‰©åŒ–è§†å›¾ {view_name} åˆ·æ–°å¤±è´¥")
            
    except Exception as e:
        logger.error(f"åˆ·æ–°ç‰©åŒ–è§†å›¾å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--limit', type=int, help='é™åˆ¶åˆ†æçš„æ”¶è´¹ç«™æ•°é‡ï¼ˆå¯é€‰ï¼‰')
@click.option('--analysis-id', help='è‡ªå®šä¹‰åˆ†æID')
@click.option('--export-qgis', is_flag=True, help='å¯¼å‡ºQGISå¯è§†åŒ–è§†å›¾')
@click.option('--max-trajectory-records', type=int, default=10000, help='æœ€å¤§è½¨è¿¹è®°å½•æ•°')
def analyze_toll_stations(limit: int, analysis_id: str, export_qgis: bool,
                         max_trajectory_records: int):
    """
    åˆ†ææ”¶è´¹ç«™ï¼ˆintersectiontype=2ï¼‰åŠå…¶èŒƒå›´å†…çš„è½¨è¿¹æ•°æ®
    
    åŠŸèƒ½ï¼š
    1. ç›´æ¥æŸ¥æ‰¾intersectiontype=2çš„æ”¶è´¹ç«™æ•°æ®ï¼ˆä¸ä¾èµ–bboxï¼‰
    2. ä½¿ç”¨æ”¶è´¹ç«™åŸå§‹å‡ ä½•ä¸è½¨è¿¹æ•°æ®è¿›è¡Œç©ºé—´ç›¸äº¤åˆ†æ
    3. æŒ‰dataset_nameèšåˆè½¨è¿¹ç»Ÿè®¡
    4. å¯é€‰å¯¼å‡ºQGISå¯è§†åŒ–è§†å›¾
    
    ç¤ºä¾‹ï¼š
        # åŸºç¡€åˆ†æ
        spdatalab analyze-toll-stations
        
        # é™åˆ¶åˆ†ææ•°é‡
        spdatalab analyze-toll-stations --limit 100
        
        # å¯¼å‡ºQGISè§†å›¾
        spdatalab analyze-toll-stations --export-qgis
    """
    try:
        from .fusion.toll_station_analysis import (
            TollStationAnalyzer,
            TollStationAnalysisConfig,
            analyze_toll_station_trajectories
        )
    except ImportError as e:
        click.echo(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        click.echo("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…æ‰€æœ‰ä¾èµ–")
        return
    
    click.echo("ğŸš€ å¼€å§‹æ”¶è´¹ç«™è½¨è¿¹åˆ†æ...")
    click.echo(f"ğŸ“‹ åˆ†æå‚æ•°:")
    click.echo(f"   - æ”¶è´¹ç«™é™åˆ¶: {limit or 'æ— é™åˆ¶'}")
    click.echo(f"   - ç©ºé—´å…³ç³»: ç›´æ¥å‡ ä½•ç›¸äº¤ï¼ˆæ— ç¼“å†²åŒºï¼‰")
    click.echo(f"   - æœ€å¤§è½¨è¿¹è®°å½•: {max_trajectory_records}")
    
    try:
        # é…ç½®åˆ†æå‚æ•°
        config = TollStationAnalysisConfig(
            max_trajectory_records=max_trajectory_records
        )
        
        # æ‰§è¡Œåˆ†æ
        toll_stations, trajectory_results, final_analysis_id = analyze_toll_station_trajectories(
            limit=limit,
            config=config
        )
        
        if analysis_id:
            final_analysis_id = analysis_id
        
        # æ˜¾ç¤ºç»“æœ
        if toll_stations.empty:
            click.echo("âš ï¸ æœªæ‰¾åˆ°æ”¶è´¹ç«™æ•°æ®")
            click.echo("\nğŸ’¡ å¯èƒ½çš„åŸå› :")
            click.echo("   - intersectionè¡¨ä¸­æ²¡æœ‰intersectiontype=2çš„æ•°æ®")
            click.echo("   - è¿œç¨‹æ•°æ®åº“è¿æ¥é—®é¢˜")
            click.echo("   - full_intersectionè¡¨ä¸å­˜åœ¨æˆ–ä¸ºç©º")
            return
        
        click.echo(f"\nâœ… åˆ†æå®Œæˆï¼")
        click.echo(f"ğŸ“Š åˆ†æID: {final_analysis_id}")
        click.echo(f"ğŸ“ æ‰¾åˆ°æ”¶è´¹ç«™: {len(toll_stations)} ä¸ª")
        
        # æ˜¾ç¤ºæ”¶è´¹ç«™ç»Ÿè®¡
        if 'intersectionsubtype' in toll_stations.columns:
            subtype_stats = toll_stations['intersectionsubtype'].value_counts()
            click.echo(f"\nğŸ›ï¸ æ”¶è´¹ç«™å­ç±»å‹åˆ†å¸ƒ:")
            for subtype, count in subtype_stats.head(10).items():
                click.echo(f"   å­ç±»å‹{subtype}: {count} ä¸ªæ”¶è´¹ç«™")
        
        # æ˜¾ç¤ºè½¨è¿¹åˆ†æç»“æœ
        if not trajectory_results.empty:
            total_trajectories = trajectory_results['trajectory_count'].sum()
            total_datasets = trajectory_results['dataset_name'].nunique()
            avg_workstage_2 = trajectory_results['workstage_2_ratio'].mean()
            
            click.echo(f"\nğŸš— è½¨è¿¹æ•°æ®ç»Ÿè®¡:")
            click.echo(f"   - æ€»è½¨è¿¹æ•°: {total_trajectories:,}")
            click.echo(f"   - æ•°æ®é›†æ•°: {total_datasets}")
            click.echo(f"   - å¹³å‡å·¥ä½œé˜¶æ®µ2æ¯”ä¾‹: {avg_workstage_2:.1f}%")
            
            # æ˜¾ç¤ºTopæ•°æ®é›†
            click.echo(f"\nğŸ” Top 10 æ•°æ®é›†:")
            top_datasets = trajectory_results.groupby('dataset_name')['trajectory_count'].sum().sort_values(ascending=False).head(10)
            for i, (dataset, count) in enumerate(top_datasets.items(), 1):
                click.echo(f"   {i:2d}. {dataset}: {count:,} æ¡è½¨è¿¹")
        else:
            click.echo(f"\nâš ï¸ æœªæ‰¾åˆ°è½¨è¿¹æ•°æ®")
        
        # å¯¼å‡ºQGISè§†å›¾
        if export_qgis:
            click.echo(f"\nğŸ—ºï¸ å¯¼å‡ºQGISå¯è§†åŒ–è§†å›¾...")
            try:
                from .fusion.toll_station_analysis import export_toll_station_results_for_qgis
                export_info = export_toll_station_results_for_qgis(final_analysis_id, config)
                
                click.echo(f"âœ… QGISè§†å›¾åˆ›å»ºæˆåŠŸ:")
                for view_type, view_name in export_info.items():
                    click.echo(f"   - {view_name}")
                
                click.echo(f"\nğŸ’¡ QGISä½¿ç”¨è¯´æ˜:")
                click.echo(f"   1. è¿æ¥åˆ°local_pgæ•°æ®åº“ (localhost:5432/postgres)")
                click.echo(f"   2. æ·»åŠ ä¸Šè¿°è§†å›¾ä½œä¸ºå›¾å±‚")
                click.echo(f"   3. æ”¶è´¹ç«™è§†å›¾æ˜¾ç¤ºä½ç½®ï¼Œè½¨è¿¹è§†å›¾æ˜¾ç¤ºç»Ÿè®¡å¯†åº¦")
                
            except Exception as qgis_error:
                click.echo(f"âŒ QGISè§†å›¾å¯¼å‡ºå¤±è´¥: {qgis_error}")
        
        # ä¿å­˜åˆ†æä¿¡æ¯
        click.echo(f"\nğŸ’¾ åˆ†ææ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°æ•°æ®åº“")
        click.echo(f"   - æ”¶è´¹ç«™è¡¨: toll_station_analysis")
        click.echo(f"   - è½¨è¿¹ç»“æœè¡¨: toll_station_trajectories")
        
        # ç»™å‡ºåç»­æ“ä½œå»ºè®®
        click.echo(f"\nğŸ¯ åç»­æ“ä½œå»ºè®®:")
        click.echo(f"   - æŸ¥çœ‹åˆ†ææ±‡æ€»: ä½¿ç”¨ --analysis-id {final_analysis_id}")
        click.echo(f"   - QGISå¯è§†åŒ–: è¿æ¥åˆ°local_pgæ•°æ®åº“æŸ¥çœ‹è§†å›¾")
        click.echo(f"   - æ•°æ®å¯¼å‡º: å¯ä»æ•°æ®åº“è¡¨ä¸­å¯¼å‡ºCSVæˆ–å…¶ä»–æ ¼å¼")
        
    except Exception as e:
        click.echo(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        if click.get_current_context().obj.get('debug', False):
            click.echo(f"\nğŸ› è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            click.echo(traceback.format_exc())
        else:
            click.echo(f"\nğŸ’¡ ä½¿ç”¨ --debug å‚æ•°æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯")

@cli.command()
@click.option('--analysis-id', required=True, help='åˆ†æID')
def toll_stations_summary(analysis_id: str):
    """
    æŸ¥çœ‹æ”¶è´¹ç«™åˆ†æçš„æ±‡æ€»ä¿¡æ¯
    
    ç¤ºä¾‹ï¼š
        spdatalab toll-stations-summary --analysis-id toll_station_20231201_143022
    """
    try:
        from .fusion.toll_station_analysis import get_toll_station_analysis_summary
    except ImportError as e:
        click.echo(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        return
    
    try:
        summary = get_toll_station_analysis_summary(analysis_id)
        
        if 'error' in summary:
            click.echo(f"âŒ è·å–æ±‡æ€»å¤±è´¥: {summary['error']}")
            return
        
        click.echo(f"ğŸ“Š æ”¶è´¹ç«™åˆ†ææ±‡æ€» - {analysis_id}")
        click.echo("=" * 60)
        
        # æ”¶è´¹ç«™ç»Ÿè®¡
        click.echo(f"ğŸ›ï¸ æ”¶è´¹ç«™ç»Ÿè®¡:")
        click.echo(f"   - æ€»æ”¶è´¹ç«™æ•°: {summary.get('total_toll_stations', 0)}")
        click.echo(f"   - æ¶‰åŠåŸå¸‚æ•°: {summary.get('cities_count', 0)}")
        click.echo(f"   - æ¶‰åŠåœºæ™¯æ•°: {summary.get('scenes_count', 0)}")
        
        # è½¨è¿¹ç»Ÿè®¡
        click.echo(f"\nğŸš— è½¨è¿¹ç»Ÿè®¡:")
        click.echo(f"   - å”¯ä¸€æ•°æ®é›†: {summary.get('unique_datasets', 0)}")
        click.echo(f"   - æ€»è½¨è¿¹æ•°: {summary.get('total_trajectories', 0):,}")
        click.echo(f"   - æ€»æ•°æ®ç‚¹: {summary.get('total_points', 0):,}")
        click.echo(f"   - å¹³å‡å·¥ä½œé˜¶æ®µ2æ¯”ä¾‹: {summary.get('avg_workstage_2_ratio', 0)}%")
        
        # åˆ†æä¿¡æ¯  
        click.echo(f"\nğŸ“‹ åˆ†æä¿¡æ¯:")
        click.echo(f"   - åˆ†æID: {summary.get('analysis_id', 'N/A')}")
        click.echo(f"   - åˆ†ææ—¶é—´: {summary.get('analysis_time', 'N/A')}")
        
    except Exception as e:
        click.echo(f"âŒ è·å–æ±‡æ€»å¤±è´¥: {e}")

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®ã€‚"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
        ]
    )

def main():
    """ä¸»å‡½æ•°ã€‚"""
    cli()

if __name__ == '__main__':
    cli()