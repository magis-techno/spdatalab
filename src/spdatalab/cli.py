"""å‘½ä»¤è¡Œæ¥å£æ¨¡å—ã€‚"""

import click
from spdatalab.common.db import get_conn
import argparse
import json
import logging
from pathlib import Path

from .dataset.scene_list_generator import SceneListGenerator
from .dataset.dataset_manager import DatasetManager

logger = logging.getLogger(__name__)

@click.group()
def cli():
    """Spatial-Data-Lab å·¥å…·é›†ã€‚"""
    pass

@cli.command()
@click.option('--name', required=True)
@click.option('--desc', default='')
@click.option('--jsonl', type=click.Path(exists=True), required=True, help='Path to JSONL containing scene_token')
def create(name, desc, jsonl):
    """Create a new scene set from JSONL file"""
    import json
    conn = get_conn()
    with conn, conn.cursor() as cur:
        cur.execute(
            'INSERT INTO scene_sets(name, description) VALUES(%s,%s) RETURNING set_id',
            (name, desc)
        )
        set_id = cur.fetchone()[0]
        rows = []
        with open(jsonl, 'r', encoding='utf-8') as fh:
            for line in fh:
                try:
                    token = json.loads(line)['scene_token']
                    rows.append((set_id, token))
                except (KeyError, json.JSONDecodeError):
                    continue
        cur.executemany(
            'INSERT INTO scene_set_members(set_id, scene_token) VALUES(%s,%s) ON CONFLICT DO NOTHING',
            rows
        )
    click.echo(f'âœ… Created set {set_id} with {len(rows)} scenes')

@cli.command()
@click.option('--index-file', required=True, help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
def generate_scene_list(index_file: str, output: str):
    """ç”Ÿæˆåœºæ™¯æ•°æ®åˆ—è¡¨ã€‚
    
    ä»ç´¢å¼•æ–‡ä»¶è¯»å–æ•°æ®æºä¿¡æ¯ï¼Œç”Ÿæˆåœºæ™¯æ•°æ®åˆ—è¡¨å¹¶ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶ã€‚
    
    Args:
        index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œæ ¼å¼ä¸º obs_path@duplicateN
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¿å­˜ä¸ºJSONæ ¼å¼
    """
    setup_logging()
    
    try:
        generator = SceneListGenerator()
        generator.generate_scene_list(index_file, output)
    except Exception as e:
        logger.error(f"ç”Ÿæˆåœºæ™¯æ•°æ®åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--index-file', required=True, help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
@click.option('--dataset-name', required=True, help='æ•°æ®é›†åç§°')
@click.option('--description', default='', help='æ•°æ®é›†æè¿°')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
@click.option('--format', type=click.Choice(['json', 'parquet']), default='json', help='è¾“å‡ºæ ¼å¼')
def build_dataset(index_file: str, dataset_name: str, description: str, output: str, format: str):
    """æ„å»ºæ•°æ®é›†ç»“æ„ã€‚
    
    ä»ç´¢å¼•æ–‡ä»¶è¯»å–æ•°æ®æºä¿¡æ¯ï¼Œæ„å»ºæ•°æ®é›†ç»“æ„å¹¶ä¿å­˜ã€‚
    
    Args:
        index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œæ ¼å¼ä¸º obs_path@duplicateN
        dataset_name: æ•°æ®é›†åç§°
        description: æ•°æ®é›†æè¿°
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¿å­˜ä¸ºæŒ‡å®šæ ¼å¼
        format: è¾“å‡ºæ ¼å¼ï¼Œjson æˆ– parquet
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.build_dataset_from_index(index_file, dataset_name, description)
        manager.save_dataset(dataset, output, format=format)
        
        click.echo(f"âœ… æ•°æ®é›†æ„å»ºå®Œæˆ: {dataset_name}")
        click.echo(f"   - å­æ•°æ®é›†æ•°é‡: {len(dataset.subdatasets)}")
        click.echo(f"   - æ€»å”¯ä¸€åœºæ™¯æ•°: {dataset.total_unique_scenes}")
        click.echo(f"   - æ€»åœºæ™¯æ•°(å«å€å¢): {dataset.total_scenes}")
        click.echo(f"   - å·²ä¿å­˜åˆ°: {output} ({format}æ ¼å¼)")
        
        if format == 'parquet':
            click.echo(f"   - æ¨èå®‰è£…: pip install pandas pyarrow")
            
    except Exception as e:
        logger.error(f"æ„å»ºæ•°æ®é›†å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--input', required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒJSON/Parquet/æ–‡æœ¬æ ¼å¼ï¼‰')
@click.option('--batch', type=int, default=1000, help='å¤„ç†æ‰¹æ¬¡å¤§å°')
@click.option('--insert-batch', type=int, default=1000, help='æ’å…¥æ‰¹æ¬¡å¤§å°')
@click.option('--buffer-meters', type=int, default=50, help='ç¼“å†²åŒºå¤§å°ï¼ˆç±³ï¼‰')
@click.option('--precise-buffer', is_flag=True, help='ä½¿ç”¨ç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒºï¼ˆéœ€è¦æŠ•å½±è½¬æ¢ï¼‰')
def process_bbox(input: str, batch: int, insert_batch: int, buffer_meters: int, precise_buffer: bool):
    """å¤„ç†è¾¹ç•Œæ¡†æ•°æ®ã€‚
    
    ä»æ•°æ®é›†æ–‡ä»¶ä¸­åŠ è½½åœºæ™¯IDï¼Œè·å–è¾¹ç•Œæ¡†ä¿¡æ¯å¹¶æ’å…¥åˆ°PostGISæ•°æ®åº“ä¸­ã€‚
    æ”¯æŒJSONã€Parquetå’Œæ–‡æœ¬æ ¼å¼çš„æ•°æ®é›†æ–‡ä»¶ã€‚
    
    Args:
        input: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒJSON/Parquet/æ–‡æœ¬æ ¼å¼
        batch: å¤„ç†æ‰¹æ¬¡å¤§å°ï¼Œæ¯æ‰¹ä»æ•°æ®åº“è·å–å¤šå°‘ä¸ªåœºæ™¯çš„ä¿¡æ¯
        insert_batch: æ’å…¥æ‰¹æ¬¡å¤§å°ï¼Œæ¯æ‰¹å‘æ•°æ®åº“æ’å…¥å¤šå°‘æ¡è®°å½•
        buffer_meters: ç¼“å†²åŒºå¤§å°ï¼ˆç±³ï¼‰ï¼Œç”¨äºç‚¹æ•°æ®çš„è¾¹ç•Œæ¡†æ‰©å±•
        precise_buffer: æ˜¯å¦ä½¿ç”¨ç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒºï¼ˆé€šè¿‡æŠ•å½±è½¬æ¢å®ç°ï¼‰
    """
    setup_logging()
    
    try:
        from .dataset.bbox import run as bbox_run
        
        click.echo(f"å¼€å§‹å¤„ç†è¾¹ç•Œæ¡†æ•°æ®:")
        click.echo(f"  - è¾“å…¥æ–‡ä»¶: {input}")
        click.echo(f"  - å¤„ç†æ‰¹æ¬¡: {batch}")
        click.echo(f"  - æ’å…¥æ‰¹æ¬¡: {insert_batch}")
        click.echo(f"  - ç¼“å†²åŒº: {buffer_meters}ç±³")
        click.echo(f"  - ç²¾ç¡®æ¨¡å¼: {'æ˜¯' if precise_buffer else 'å¦'}")
        
        bbox_run(
            input_path=input,
            batch=batch,
            insert_batch=insert_batch,
            buffer_meters=buffer_meters,
            use_precise_buffer=precise_buffer
        )
        
        click.echo("âœ… è¾¹ç•Œæ¡†å¤„ç†å®Œæˆ")
        
    except Exception as e:
        logger.error(f"å¤„ç†è¾¹ç•Œæ¡†å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--index-file', required=True, help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
@click.option('--dataset-name', required=True, help='æ•°æ®é›†åç§°')
@click.option('--description', default='', help='æ•°æ®é›†æè¿°')
@click.option('--output', required=True, help='è¾“å‡ºæ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--format', type=click.Choice(['json', 'parquet']), default='json', help='æ•°æ®é›†ä¿å­˜æ ¼å¼')
@click.option('--batch', type=int, default=1000, help='è¾¹ç•Œæ¡†å¤„ç†æ‰¹æ¬¡å¤§å°')
@click.option('--insert-batch', type=int, default=1000, help='è¾¹ç•Œæ¡†æ’å…¥æ‰¹æ¬¡å¤§å°')
@click.option('--buffer-meters', type=int, default=50, help='ç¼“å†²åŒºå¤§å°ï¼ˆç±³ï¼‰')
@click.option('--precise-buffer', is_flag=True, help='ä½¿ç”¨ç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒº')
@click.option('--skip-bbox', is_flag=True, help='è·³è¿‡è¾¹ç•Œæ¡†å¤„ç†')
def build_dataset_with_bbox(index_file: str, dataset_name: str, description: str, output: str, 
                           format: str, batch: int, insert_batch: int, buffer_meters: int, 
                           precise_buffer: bool, skip_bbox: bool):
    """æ„å»ºæ•°æ®é›†å¹¶å¤„ç†è¾¹ç•Œæ¡†ï¼ˆå®Œæ•´å·¥ä½œæµç¨‹ï¼‰ã€‚
    
    ä»ç´¢å¼•æ–‡ä»¶æ„å»ºæ•°æ®é›†ï¼Œä¿å­˜åè‡ªåŠ¨å¤„ç†è¾¹ç•Œæ¡†æ•°æ®ï¼Œæä¾›ä¸€é”®å¼å®Œæ•´å·¥ä½œæµç¨‹ã€‚
    
    Args:
        index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œæ ¼å¼ä¸º obs_path@duplicateN
        dataset_name: æ•°æ®é›†åç§°
        description: æ•°æ®é›†æè¿°
        output: è¾“å‡ºæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        format: æ•°æ®é›†ä¿å­˜æ ¼å¼ï¼Œjson æˆ– parquet
        batch: è¾¹ç•Œæ¡†å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch: è¾¹ç•Œæ¡†æ’å…¥æ‰¹æ¬¡å¤§å°
        buffer_meters: ç¼“å†²åŒºå¤§å°ï¼ˆç±³ï¼‰
        precise_buffer: æ˜¯å¦ä½¿ç”¨ç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒº
        skip_bbox: æ˜¯å¦è·³è¿‡è¾¹ç•Œæ¡†å¤„ç†
    """
    setup_logging()
    
    try:
        # æ­¥éª¤1ï¼šæ„å»ºæ•°æ®é›†
        click.echo("=== æ­¥éª¤1: æ„å»ºæ•°æ®é›† ===")
        manager = DatasetManager()
        dataset = manager.build_dataset_from_index(index_file, dataset_name, description)
        
        # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_dataset_stats(dataset)
        click.echo(f"æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
        click.echo(f"  - æ•°æ®é›†åç§°: {stats['dataset_name']}")
        click.echo(f"  - å­æ•°æ®é›†æ•°é‡: {stats['subdataset_count']}")
        click.echo(f"  - å”¯ä¸€åœºæ™¯æ•°: {stats['total_unique_scenes']}")
        click.echo(f"  - æ€»åœºæ™¯æ•°(å«å€å¢): {stats['total_scenes_with_duplicates']}")
        
        # æ­¥éª¤2ï¼šä¿å­˜æ•°æ®é›†
        click.echo("=== æ­¥éª¤2: ä¿å­˜æ•°æ®é›† ===")
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(output).parent.mkdir(parents=True, exist_ok=True)
        manager.save_dataset(dataset, output, format=format)
        click.echo(f"âœ… æ•°æ®é›†å·²ä¿å­˜åˆ°: {output} ({format}æ ¼å¼)")
        
        # æ­¥éª¤3ï¼šå¤„ç†è¾¹ç•Œæ¡†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not skip_bbox:
            click.echo("=== æ­¥éª¤3: å¤„ç†è¾¹ç•Œæ¡† ===")
            
            from .dataset.bbox import run as bbox_run
            
            click.echo(f"è¾¹ç•Œæ¡†å¤„ç†é…ç½®:")
            click.echo(f"  - å¤„ç†æ‰¹æ¬¡: {batch}")
            click.echo(f"  - æ’å…¥æ‰¹æ¬¡: {insert_batch}")
            click.echo(f"  - ç¼“å†²åŒº: {buffer_meters}ç±³")
            click.echo(f"  - ç²¾ç¡®æ¨¡å¼: {'æ˜¯' if precise_buffer else 'å¦'}")
            
            bbox_run(
                input_path=output,
                batch=batch,
                insert_batch=insert_batch,
                buffer_meters=buffer_meters,
                use_precise_buffer=precise_buffer
            )
            
            click.echo("âœ… è¾¹ç•Œæ¡†å¤„ç†å®Œæˆ")
        else:
            click.echo("=== æ­¥éª¤3: è·³è¿‡è¾¹ç•Œæ¡†å¤„ç† ===")
        
        click.echo("ğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"å®Œæ•´å·¥ä½œæµç¨‹å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--subdataset', default=None, help='å­æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰')
@click.option('--output', default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
def list_scenes(dataset_file: str, subdataset: str, output: str):
    """åˆ—å‡ºæ•°æ®é›†ä¸­çš„åœºæ™¯IDã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        subdataset: å­æ•°æ®é›†åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™åˆ—å‡ºæ‰€æœ‰åœºæ™¯
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è¾“å‡ºåˆ°æ§åˆ¶å°
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        scene_ids = manager.list_scene_ids(dataset, subdataset)
        
        if output:
            with open(output, 'w', encoding='utf-8') as f:
                for scene_id in scene_ids:
                    f.write(f"{scene_id}\n")
            click.echo(f"âœ… åœºæ™¯IDåˆ—è¡¨å·²ä¿å­˜åˆ°: {output}")
        else:
            click.echo(f"åœºæ™¯IDåˆ—è¡¨ ({'å­æ•°æ®é›†: ' + subdataset if subdataset else 'å…¨éƒ¨'}):")
            for scene_id in scene_ids:
                click.echo(f"  {scene_id}")
        
        click.echo(f"æ€»å…± {len(scene_ids)} ä¸ªåœºæ™¯")
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºåœºæ™¯å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--subdataset', default=None, help='å­æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
def generate_scene_ids(dataset_file: str, subdataset: str, output: str):
    """ç”ŸæˆåŒ…å«å€å¢çš„åœºæ™¯IDåˆ—è¡¨ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        subdataset: å­æ•°æ®é›†åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™å¤„ç†æ‰€æœ‰å­æ•°æ®é›†
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        
        count = 0
        with open(output, 'w', encoding='utf-8') as f:
            for scene_id in manager.generate_scene_list_with_duplication(dataset, subdataset):
                f.write(f"{scene_id}\n")
                count += 1
        
        click.echo(f"âœ… åœºæ™¯IDåˆ—è¡¨ï¼ˆå«å€å¢ï¼‰å·²ä¿å­˜åˆ°: {output}")
        click.echo(f"æ€»å…± {count} ä¸ªåœºæ™¯ï¼ˆå«å€å¢ï¼‰")
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆåœºæ™¯IDåˆ—è¡¨å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
def dataset_info(dataset_file: str):
    """æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        
        click.echo(f"æ•°æ®é›†ä¿¡æ¯:")
        click.echo(f"  åç§°: {dataset.name}")
        click.echo(f"  æè¿°: {dataset.description}")
        click.echo(f"  åˆ›å»ºæ—¶é—´: {dataset.created_at}")
        click.echo(f"  å­æ•°æ®é›†æ•°é‡: {len(dataset.subdatasets)}")
        click.echo(f"  æ€»å”¯ä¸€åœºæ™¯æ•°: {dataset.total_unique_scenes}")
        click.echo(f"  æ€»åœºæ™¯æ•°(å«å€å¢): {dataset.total_scenes}")
        
        if dataset.subdatasets:
            click.echo(f"\nå­æ•°æ®é›†è¯¦æƒ…:")
            for i, subdataset in enumerate(dataset.subdatasets, 1):
                click.echo(f"  {i}. {subdataset.name}")
                click.echo(f"     - OBSè·¯å¾„: {subdataset.obs_path}")
                click.echo(f"     - åœºæ™¯æ•°: {subdataset.scene_count}")
                click.echo(f"     - å€å¢å› å­: {subdataset.duplication_factor}")
                click.echo(f"     - å€å¢ååœºæ™¯æ•°: {subdataset.scene_count * subdataset.duplication_factor}")
        
    except Exception as e:
        logger.error(f"æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
@click.option('--include-duplicates', is_flag=True, help='æ˜¯å¦åŒ…å«å€å¢çš„åœºæ™¯ID')
def export_scene_ids(dataset_file: str, output: str, include_duplicates: bool):
    """å¯¼å‡ºåœºæ™¯IDä¸ºParquetæ ¼å¼ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆparquetæ ¼å¼ï¼‰
        include_duplicates: æ˜¯å¦åŒ…å«å€å¢çš„åœºæ™¯ID
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        manager.export_scene_ids_parquet(dataset, output, include_duplicates)
        
        click.echo(f"âœ… åœºæ™¯IDå·²å¯¼å‡ºåˆ°: {output}")
        if include_duplicates:
            click.echo(f"   - åŒ…å«å€å¢ï¼Œæ€»è®°å½•æ•°: {dataset.total_scenes}")
        else:
            click.echo(f"   - ä¸å«å€å¢ï¼Œæ€»è®°å½•æ•°: {dataset.total_unique_scenes}")
            
    except Exception as e:
        logger.error(f"å¯¼å‡ºåœºæ™¯IDå¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--parquet-file', required=True, help='Parquetæ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--subdataset', default=None, help='æŒ‰å­æ•°æ®é›†è¿‡æ»¤')
@click.option('--duplication-factor', type=int, default=None, help='æŒ‰å€å¢å› å­è¿‡æ»¤')
@click.option('--output', default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
def query_parquet(parquet_file: str, subdataset: str, duplication_factor: int, output: str):
    """æŸ¥è¯¢Parquetæ ¼å¼æ•°æ®é›†ã€‚
    
    Args:
        parquet_file: Parquetæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        subdataset: æŒ‰å­æ•°æ®é›†åç§°è¿‡æ»¤
        duplication_factor: æŒ‰å€å¢å› å­è¿‡æ»¤
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filters = {}
        if subdataset:
            filters['subdataset_name'] = subdataset
        if duplication_factor:
            filters['duplication_factor'] = duplication_factor
            
        df = manager.query_scenes_parquet(parquet_file, **filters)
        
        if output:
            df.to_parquet(output, index=False)
            click.echo(f"âœ… æŸ¥è¯¢ç»“æœå·²ä¿å­˜åˆ°: {output}")
        else:
            click.echo(f"æŸ¥è¯¢ç»“æœ:")
            click.echo(f"  - åŒ¹é…è®°å½•æ•°: {len(df)}")
            if len(df) > 0:
                click.echo(f"  - å”¯ä¸€å­æ•°æ®é›†: {df['subdataset_name'].nunique()}")
                click.echo(f"  - å­æ•°æ®é›†åˆ—è¡¨:")
                for name in df['subdataset_name'].unique():
                    count = len(df[df['subdataset_name'] == name])
                    click.echo(f"    * {name}: {count} ä¸ªåœºæ™¯")
        
        click.echo(f"æ€»è®°å½•æ•°: {len(df)}")
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢Parquetæ•°æ®é›†å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--output-format', type=click.Choice(['json', 'parquet']), default='json', help='è¾“å‡ºæ ¼å¼')
def dataset_stats(dataset_file: str, output_format: str):
    """æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        output_format: è¾“å‡ºæ ¼å¼
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        stats = manager.get_dataset_stats(dataset)
        
        if output_format == 'json':
            click.echo(json.dumps(stats, ensure_ascii=False, indent=2))
        else:
            click.echo(f"æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
            click.echo(f"  æ•°æ®é›†åç§°: {stats['dataset_name']}")
            click.echo(f"  å­æ•°æ®é›†æ•°é‡: {stats['subdataset_count']}")
            click.echo(f"  æ€»å”¯ä¸€åœºæ™¯æ•°: {stats['total_unique_scenes']}")
            click.echo(f"  æ€»åœºæ™¯æ•°(å«å€å¢): {stats['total_scenes_with_duplicates']}")
            
            click.echo(f"\nå­æ•°æ®é›†è¯¦æƒ…:")
            for i, sub_stats in enumerate(stats['subdatasets'], 1):
                click.echo(f"  {i}. {sub_stats['name']}")
                click.echo(f"     - åœºæ™¯æ•°: {sub_stats['scene_count']}")
                click.echo(f"     - å€å¢å› å­: {sub_stats['duplication_factor']}")
                click.echo(f"     - å€å¢ååœºæ™¯æ•°: {sub_stats['total_scenes_with_duplicates']}")
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®ã€‚"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
        ]
    )

def main():
    """ä¸»å‡½æ•°ã€‚"""
    parser = argparse.ArgumentParser(description='Spatial-Data-Lab å·¥å…·é›†')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # æ·»åŠ åœºæ™¯æ•°æ®åˆ—è¡¨ç”Ÿæˆå‘½ä»¤
    scene_list_parser = subparsers.add_parser('generate-scene-list',
                                            help='ç”Ÿæˆåœºæ™¯æ•°æ®åˆ—è¡¨')
    scene_list_parser.add_argument('--index-file', required=True,
                                 help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
    scene_list_parser.add_argument('--output', required=True,
                                 help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    scene_list_parser.set_defaults(func=generate_scene_list)
    
    # æ·»åŠ æ•°æ®é›†æ„å»ºå‘½ä»¤
    build_dataset_parser = subparsers.add_parser('build-dataset',
                                                help='æ„å»ºæ•°æ®é›†ç»“æ„')
    build_dataset_parser.add_argument('--index-file', required=True,
                                     help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
    build_dataset_parser.add_argument('--dataset-name', required=True,
                                     help='æ•°æ®é›†åç§°')
    build_dataset_parser.add_argument('--description', default='',
                                     help='æ•°æ®é›†æè¿°')
    build_dataset_parser.add_argument('--output', required=True,
                                     help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    build_dataset_parser.add_argument('--format', type=click.Choice(['json', 'parquet']), default='json',
                                     help='è¾“å‡ºæ ¼å¼ï¼Œjson æˆ– parquet')
    build_dataset_parser.set_defaults(func=build_dataset)
    
    args = parser.parse_args()
    if args.command:
        if args.command == 'generate-scene-list':
            args.func(args.index_file, args.output)
        elif args.command == 'build-dataset':
            args.func(args.index_file, args.dataset_name, args.description, args.output, args.format)
    else:
        parser.print_help()

if __name__ == '__main__':
    cli()