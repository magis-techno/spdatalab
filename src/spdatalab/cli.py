"""å‘½ä»¤è¡Œæ¥å£æ¨¡å—ã€‚"""

import click
from spdatalab.common.db import get_conn
import argparse
import json
import logging
from pathlib import Path

from .dataset.scene_list_generator import SceneListGenerator
from .dataset.dataset_manager import DatasetManager

logger = logging.getLogger(__name__)

@click.group()
def cli():
    """Spatial-Data-Lab å·¥å…·é›†ã€‚"""
    pass

@cli.command()
@click.option('--name', required=True)
@click.option('--desc', default='')
@click.option('--jsonl', type=click.Path(exists=True), required=True, help='Path to JSONL containing scene_token')
def create(name, desc, jsonl):
    """Create a new scene set from JSONL file"""
    import json
    conn = get_conn()
    with conn, conn.cursor() as cur:
        cur.execute(
            'INSERT INTO scene_sets(name, description) VALUES(%s,%s) RETURNING set_id',
            (name, desc)
        )
        set_id = cur.fetchone()[0]
        rows = []
        with open(jsonl, 'r', encoding='utf-8') as fh:
            for line in fh:
                try:
                    token = json.loads(line)['scene_token']
                    rows.append((set_id, token))
                except (KeyError, json.JSONDecodeError):
                    continue
        cur.executemany(
            'INSERT INTO scene_set_members(set_id, scene_token) VALUES(%s,%s) ON CONFLICT DO NOTHING',
            rows
        )
    click.echo(f'âœ… Created set {set_id} with {len(rows)} scenes')

@cli.command()
@click.option('--index-file', required=True, help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
def generate_scene_list(index_file: str, output: str):
    """ç”Ÿæˆåœºæ™¯æ•°æ®åˆ—è¡¨ã€‚
    
    ä»ç´¢å¼•æ–‡ä»¶è¯»å–æ•°æ®æºä¿¡æ¯ï¼Œç”Ÿæˆåœºæ™¯æ•°æ®åˆ—è¡¨å¹¶ä¿å­˜åˆ°è¾“å‡ºæ–‡ä»¶ã€‚
    
    Args:
        index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œæ ¼å¼ä¸º obs_path@duplicateN
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¿å­˜ä¸ºJSONæ ¼å¼
    """
    setup_logging()
    
    try:
        generator = SceneListGenerator()
        generator.generate_scene_list(index_file, output)
    except Exception as e:
        logger.error(f"ç”Ÿæˆåœºæ™¯æ•°æ®åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--index-file', required=True, help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
@click.option('--dataset-name', required=True, help='æ•°æ®é›†åç§°')
@click.option('--description', default='', help='æ•°æ®é›†æè¿°')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
@click.option('--format', type=click.Choice(['json', 'parquet']), default='json', help='è¾“å‡ºæ ¼å¼')
def build_dataset(index_file: str, dataset_name: str, description: str, output: str, format: str):
    """æ„å»ºæ•°æ®é›†ç»“æ„ã€‚
    
    ä»ç´¢å¼•æ–‡ä»¶è¯»å–æ•°æ®æºä¿¡æ¯ï¼Œæ„å»ºæ•°æ®é›†ç»“æ„å¹¶ä¿å­˜ã€‚
    
    Args:
        index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œæ ¼å¼ä¸º obs_path@duplicateN
        dataset_name: æ•°æ®é›†åç§°
        description: æ•°æ®é›†æè¿°
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œä¿å­˜ä¸ºæŒ‡å®šæ ¼å¼
        format: è¾“å‡ºæ ¼å¼ï¼Œjson æˆ– parquet
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.build_dataset_from_index(index_file, dataset_name, description)
        manager.save_dataset(dataset, output, format=format)
        
        click.echo(f"âœ… æ•°æ®é›†æ„å»ºå®Œæˆ: {dataset_name}")
        click.echo(f"   - å­æ•°æ®é›†æ•°é‡: {len(dataset.subdatasets)}")
        click.echo(f"   - æ€»å”¯ä¸€åœºæ™¯æ•°: {dataset.total_unique_scenes}")
        click.echo(f"   - æ€»åœºæ™¯æ•°(å«å€å¢): {dataset.total_scenes}")
        click.echo(f"   - å·²ä¿å­˜åˆ°: {output} ({format}æ ¼å¼)")
        
        if format == 'parquet':
            click.echo(f"   - æ¨èå®‰è£…: pip install pandas pyarrow")
            
    except Exception as e:
        logger.error(f"æ„å»ºæ•°æ®é›†å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--input', required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒJSON/Parquet/æ–‡æœ¬æ ¼å¼ï¼‰')
@click.option('--batch', type=int, default=1000, help='å¤„ç†æ‰¹æ¬¡å¤§å°')
@click.option('--insert-batch', type=int, default=1000, help='æ’å…¥æ‰¹æ¬¡å¤§å°')
@click.option('--buffer-meters', type=int, default=50, help='ç¼“å†²åŒºå¤§å°ï¼ˆç±³ï¼‰')
@click.option('--precise-buffer', is_flag=True, help='ä½¿ç”¨ç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒºï¼ˆéœ€è¦æŠ•å½±è½¬æ¢ï¼‰')
def process_bbox(input: str, batch: int, insert_batch: int, buffer_meters: int, precise_buffer: bool):
    """å¤„ç†è¾¹ç•Œæ¡†æ•°æ®ã€‚
    
    ä»æ•°æ®é›†æ–‡ä»¶ä¸­åŠ è½½åœºæ™¯IDï¼Œè·å–è¾¹ç•Œæ¡†ä¿¡æ¯å¹¶æ’å…¥åˆ°PostGISæ•°æ®åº“ä¸­ã€‚
    æ”¯æŒJSONã€Parquetå’Œæ–‡æœ¬æ ¼å¼çš„æ•°æ®é›†æ–‡ä»¶ã€‚
    
    Args:
        input: è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œæ”¯æŒJSON/Parquet/æ–‡æœ¬æ ¼å¼
        batch: å¤„ç†æ‰¹æ¬¡å¤§å°ï¼Œæ¯æ‰¹ä»æ•°æ®åº“è·å–å¤šå°‘ä¸ªåœºæ™¯çš„ä¿¡æ¯
        insert_batch: æ’å…¥æ‰¹æ¬¡å¤§å°ï¼Œæ¯æ‰¹å‘æ•°æ®åº“æ’å…¥å¤šå°‘æ¡è®°å½•
        buffer_meters: ç¼“å†²åŒºå¤§å°ï¼ˆç±³ï¼‰ï¼Œç”¨äºç‚¹æ•°æ®çš„è¾¹ç•Œæ¡†æ‰©å±•
        precise_buffer: æ˜¯å¦ä½¿ç”¨ç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒºï¼ˆé€šè¿‡æŠ•å½±è½¬æ¢å®ç°ï¼‰
    """
    setup_logging()
    
    try:
        from .dataset.bbox import run as bbox_run
        
        click.echo(f"å¼€å§‹å¤„ç†è¾¹ç•Œæ¡†æ•°æ®:")
        click.echo(f"  - è¾“å…¥æ–‡ä»¶: {input}")
        click.echo(f"  - å¤„ç†æ‰¹æ¬¡: {batch}")
        click.echo(f"  - æ’å…¥æ‰¹æ¬¡: {insert_batch}")
        click.echo(f"  - ç¼“å†²åŒº: {buffer_meters}ç±³")
        click.echo(f"  - ç²¾ç¡®æ¨¡å¼: {'æ˜¯' if precise_buffer else 'å¦'}")
        
        bbox_run(
            input_path=input,
            batch=batch,
            insert_batch=insert_batch,
            buffer_meters=buffer_meters,
            use_precise_buffer=precise_buffer
        )
        
        click.echo("âœ… è¾¹ç•Œæ¡†å¤„ç†å®Œæˆ")
        
    except Exception as e:
        logger.error(f"å¤„ç†è¾¹ç•Œæ¡†å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--index-file', required=True, help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
@click.option('--dataset-name', required=True, help='æ•°æ®é›†åç§°')
@click.option('--description', default='', help='æ•°æ®é›†æè¿°')
@click.option('--output', required=True, help='è¾“å‡ºæ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--format', type=click.Choice(['json', 'parquet']), default='json', help='æ•°æ®é›†ä¿å­˜æ ¼å¼')
@click.option('--batch', type=int, default=1000, help='è¾¹ç•Œæ¡†å¤„ç†æ‰¹æ¬¡å¤§å°')
@click.option('--insert-batch', type=int, default=1000, help='è¾¹ç•Œæ¡†æ’å…¥æ‰¹æ¬¡å¤§å°')
@click.option('--buffer-meters', type=int, default=50, help='ç¼“å†²åŒºå¤§å°ï¼ˆç±³ï¼‰')
@click.option('--precise-buffer', is_flag=True, help='ä½¿ç”¨ç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒº')
@click.option('--skip-bbox', is_flag=True, help='è·³è¿‡è¾¹ç•Œæ¡†å¤„ç†')
def build_dataset_with_bbox(index_file: str, dataset_name: str, description: str, output: str, 
                           format: str, batch: int, insert_batch: int, buffer_meters: int, 
                           precise_buffer: bool, skip_bbox: bool):
    """æ„å»ºæ•°æ®é›†å¹¶å¤„ç†è¾¹ç•Œæ¡†ï¼ˆå®Œæ•´å·¥ä½œæµç¨‹ï¼‰ã€‚
    
    ä»ç´¢å¼•æ–‡ä»¶æ„å»ºæ•°æ®é›†ï¼Œä¿å­˜åè‡ªåŠ¨å¤„ç†è¾¹ç•Œæ¡†æ•°æ®ï¼Œæä¾›ä¸€é”®å¼å®Œæ•´å·¥ä½œæµç¨‹ã€‚
    
    Args:
        index_file: ç´¢å¼•æ–‡ä»¶è·¯å¾„ï¼Œæ¯è¡Œæ ¼å¼ä¸º obs_path@duplicateN
        dataset_name: æ•°æ®é›†åç§°
        description: æ•°æ®é›†æè¿°
        output: è¾“å‡ºæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        format: æ•°æ®é›†ä¿å­˜æ ¼å¼ï¼Œjson æˆ– parquet
        batch: è¾¹ç•Œæ¡†å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch: è¾¹ç•Œæ¡†æ’å…¥æ‰¹æ¬¡å¤§å°
        buffer_meters: ç¼“å†²åŒºå¤§å°ï¼ˆç±³ï¼‰
        precise_buffer: æ˜¯å¦ä½¿ç”¨ç²¾ç¡®çš„ç±³çº§ç¼“å†²åŒº
        skip_bbox: æ˜¯å¦è·³è¿‡è¾¹ç•Œæ¡†å¤„ç†
    """
    setup_logging()
    
    try:
        # æ­¥éª¤1ï¼šæ„å»ºæ•°æ®é›†
        click.echo("=== æ­¥éª¤1: æ„å»ºæ•°æ®é›† ===")
        manager = DatasetManager()
        dataset = manager.build_dataset_from_index(index_file, dataset_name, description)
        
        # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_dataset_stats(dataset)
        click.echo(f"æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
        click.echo(f"  - æ•°æ®é›†åç§°: {stats['dataset_name']}")
        click.echo(f"  - å­æ•°æ®é›†æ•°é‡: {stats['subdataset_count']}")
        click.echo(f"  - å”¯ä¸€åœºæ™¯æ•°: {stats['total_unique_scenes']}")
        click.echo(f"  - æ€»åœºæ™¯æ•°(å«å€å¢): {stats['total_scenes_with_duplicates']}")
        
        # æ­¥éª¤2ï¼šä¿å­˜æ•°æ®é›†
        click.echo("=== æ­¥éª¤2: ä¿å­˜æ•°æ®é›† ===")
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(output).parent.mkdir(parents=True, exist_ok=True)
        manager.save_dataset(dataset, output, format=format)
        click.echo(f"âœ… æ•°æ®é›†å·²ä¿å­˜åˆ°: {output} ({format}æ ¼å¼)")
        
        # æ­¥éª¤3ï¼šå¤„ç†è¾¹ç•Œæ¡†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not skip_bbox:
            click.echo("=== æ­¥éª¤3: å¤„ç†è¾¹ç•Œæ¡† ===")
            
            from .dataset.bbox import run as bbox_run
            
            click.echo(f"è¾¹ç•Œæ¡†å¤„ç†é…ç½®:")
            click.echo(f"  - å¤„ç†æ‰¹æ¬¡: {batch}")
            click.echo(f"  - æ’å…¥æ‰¹æ¬¡: {insert_batch}")
            click.echo(f"  - ç¼“å†²åŒº: {buffer_meters}ç±³")
            click.echo(f"  - ç²¾ç¡®æ¨¡å¼: {'æ˜¯' if precise_buffer else 'å¦'}")
            
            bbox_run(
                input_path=output,
                batch=batch,
                insert_batch=insert_batch,
                buffer_meters=buffer_meters,
                use_precise_buffer=precise_buffer
            )
            
            click.echo("âœ… è¾¹ç•Œæ¡†å¤„ç†å®Œæˆ")
        else:
            click.echo("=== æ­¥éª¤3: è·³è¿‡è¾¹ç•Œæ¡†å¤„ç† ===")
        
        click.echo("ğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"å®Œæ•´å·¥ä½œæµç¨‹å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--subdataset', default=None, help='å­æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰')
@click.option('--output', default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
def list_scenes(dataset_file: str, subdataset: str, output: str):
    """åˆ—å‡ºæ•°æ®é›†ä¸­çš„åœºæ™¯IDã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        subdataset: å­æ•°æ®é›†åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™åˆ—å‡ºæ‰€æœ‰åœºæ™¯
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è¾“å‡ºåˆ°æ§åˆ¶å°
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        scene_ids = manager.list_scene_ids(dataset, subdataset)
        
        if output:
            with open(output, 'w', encoding='utf-8') as f:
                for scene_id in scene_ids:
                    f.write(f"{scene_id}\n")
            click.echo(f"âœ… åœºæ™¯IDåˆ—è¡¨å·²ä¿å­˜åˆ°: {output}")
        else:
            click.echo(f"åœºæ™¯IDåˆ—è¡¨ ({'å­æ•°æ®é›†: ' + subdataset if subdataset else 'å…¨éƒ¨'}):")
            for scene_id in scene_ids:
                click.echo(f"  {scene_id}")
        
        click.echo(f"æ€»å…± {len(scene_ids)} ä¸ªåœºæ™¯")
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºåœºæ™¯å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--subdataset', default=None, help='å­æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
def generate_scene_ids(dataset_file: str, subdataset: str, output: str):
    """ç”ŸæˆåŒ…å«å€å¢çš„åœºæ™¯IDåˆ—è¡¨ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        subdataset: å­æ•°æ®é›†åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™å¤„ç†æ‰€æœ‰å­æ•°æ®é›†
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        
        count = 0
        with open(output, 'w', encoding='utf-8') as f:
            for scene_id in manager.generate_scene_list_with_duplication(dataset, subdataset):
                f.write(f"{scene_id}\n")
                count += 1
        
        click.echo(f"âœ… åœºæ™¯IDåˆ—è¡¨ï¼ˆå«å€å¢ï¼‰å·²ä¿å­˜åˆ°: {output}")
        click.echo(f"æ€»å…± {count} ä¸ªåœºæ™¯ï¼ˆå«å€å¢ï¼‰")
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆåœºæ™¯IDåˆ—è¡¨å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
def dataset_info(dataset_file: str):
    """æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        
        click.echo(f"æ•°æ®é›†ä¿¡æ¯:")
        click.echo(f"  åç§°: {dataset.name}")
        click.echo(f"  æè¿°: {dataset.description}")
        click.echo(f"  åˆ›å»ºæ—¶é—´: {dataset.created_at}")
        click.echo(f"  å­æ•°æ®é›†æ•°é‡: {len(dataset.subdatasets)}")
        click.echo(f"  æ€»å”¯ä¸€åœºæ™¯æ•°: {dataset.total_unique_scenes}")
        click.echo(f"  æ€»åœºæ™¯æ•°(å«å€å¢): {dataset.total_scenes}")
        
        if dataset.subdatasets:
            click.echo(f"\nå­æ•°æ®é›†è¯¦æƒ…:")
            for i, subdataset in enumerate(dataset.subdatasets, 1):
                click.echo(f"  {i}. {subdataset.name}")
                click.echo(f"     - OBSè·¯å¾„: {subdataset.obs_path}")
                click.echo(f"     - åœºæ™¯æ•°: {subdataset.scene_count}")
                click.echo(f"     - å€å¢å› å­: {subdataset.duplication_factor}")
                click.echo(f"     - å€å¢ååœºæ™¯æ•°: {subdataset.scene_count * subdataset.duplication_factor}")
        
    except Exception as e:
        logger.error(f"æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--output', required=True, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
@click.option('--include-duplicates', is_flag=True, help='æ˜¯å¦åŒ…å«å€å¢çš„åœºæ™¯ID')
def export_scene_ids(dataset_file: str, output: str, include_duplicates: bool):
    """å¯¼å‡ºåœºæ™¯IDä¸ºParquetæ ¼å¼ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆparquetæ ¼å¼ï¼‰
        include_duplicates: æ˜¯å¦åŒ…å«å€å¢çš„åœºæ™¯ID
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        manager.export_scene_ids_parquet(dataset, output, include_duplicates)
        
        click.echo(f"âœ… åœºæ™¯IDå·²å¯¼å‡ºåˆ°: {output}")
        if include_duplicates:
            click.echo(f"   - åŒ…å«å€å¢ï¼Œæ€»è®°å½•æ•°: {dataset.total_scenes}")
        else:
            click.echo(f"   - ä¸å«å€å¢ï¼Œæ€»è®°å½•æ•°: {dataset.total_unique_scenes}")
            
    except Exception as e:
        logger.error(f"å¯¼å‡ºåœºæ™¯IDå¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--parquet-file', required=True, help='Parquetæ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--subdataset', default=None, help='æŒ‰å­æ•°æ®é›†è¿‡æ»¤')
@click.option('--duplication-factor', type=int, default=None, help='æŒ‰å€å¢å› å­è¿‡æ»¤')
@click.option('--output', default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
def query_parquet(parquet_file: str, subdataset: str, duplication_factor: int, output: str):
    """æŸ¥è¯¢Parquetæ ¼å¼æ•°æ®é›†ã€‚
    
    Args:
        parquet_file: Parquetæ•°æ®é›†æ–‡ä»¶è·¯å¾„
        subdataset: æŒ‰å­æ•°æ®é›†åç§°è¿‡æ»¤
        duplication_factor: æŒ‰å€å¢å› å­è¿‡æ»¤
        output: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filters = {}
        if subdataset:
            filters['subdataset_name'] = subdataset
        if duplication_factor:
            filters['duplication_factor'] = duplication_factor
            
        df = manager.query_scenes_parquet(parquet_file, **filters)
        
        if output:
            df.to_parquet(output, index=False)
            click.echo(f"âœ… æŸ¥è¯¢ç»“æœå·²ä¿å­˜åˆ°: {output}")
        else:
            click.echo(f"æŸ¥è¯¢ç»“æœ:")
            click.echo(f"  - åŒ¹é…è®°å½•æ•°: {len(df)}")
            if len(df) > 0:
                click.echo(f"  - å”¯ä¸€å­æ•°æ®é›†: {df['subdataset_name'].nunique()}")
                click.echo(f"  - å­æ•°æ®é›†åˆ—è¡¨:")
                for name in df['subdataset_name'].unique():
                    count = len(df[df['subdataset_name'] == name])
                    click.echo(f"    * {name}: {count} ä¸ªåœºæ™¯")
        
        click.echo(f"æ€»è®°å½•æ•°: {len(df)}")
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢Parquetæ•°æ®é›†å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
@click.option('--output-format', type=click.Choice(['json', 'parquet']), default='json', help='è¾“å‡ºæ ¼å¼')
def dataset_stats(dataset_file: str, output_format: str):
    """æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ã€‚
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        output_format: è¾“å‡ºæ ¼å¼
    """
    setup_logging()
    
    try:
        manager = DatasetManager()
        dataset = manager.load_dataset(dataset_file)
        stats = manager.get_dataset_stats(dataset)
        
        if output_format == 'json':
            click.echo(json.dumps(stats, ensure_ascii=False, indent=2))
        else:
            click.echo(f"æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
            click.echo(f"  æ•°æ®é›†åç§°: {stats['dataset_name']}")
            click.echo(f"  å­æ•°æ®é›†æ•°é‡: {stats['subdataset_count']}")
            click.echo(f"  æ€»å”¯ä¸€åœºæ™¯æ•°: {stats['total_unique_scenes']}")
            click.echo(f"  æ€»åœºæ™¯æ•°(å«å€å¢): {stats['total_scenes_with_duplicates']}")
            
            click.echo(f"\nå­æ•°æ®é›†è¯¦æƒ…:")
            for i, sub_stats in enumerate(stats['subdatasets'], 1):
                click.echo(f"  {i}. {sub_stats['name']}")
                click.echo(f"     - åœºæ™¯æ•°: {sub_stats['scene_count']}")
                click.echo(f"     - å€å¢å› å­: {sub_stats['duplication_factor']}")
                click.echo(f"     - å€å¢ååœºæ™¯æ•°: {sub_stats['total_scenes_with_duplicates']}")
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--analysis-type', type=click.Choice([
    'trajectory-junction', 'trajectory-road', 'trajectory-region', 
    'trajectory-to-trajectory', 'comprehensive'
]), required=True, help='äº¤é›†åˆ†æç±»å‹')
@click.option('--trajectory-table', default='clips_bbox', help='è½¨è¿¹æ•°æ®è¡¨å')
@click.option('--target-table', help='ç›®æ ‡æ•°æ®è¡¨åï¼ˆè·¯å£ã€é“è·¯ã€åŒºåŸŸç­‰ï¼‰')
@click.option('--buffer-meters', type=float, default=20.0, help='ç¼“å†²åŒºåŠå¾„ï¼ˆç±³ï¼‰')
@click.option('--time-tolerance', type=int, help='æ—¶é—´å®¹å·®ï¼ˆç§’ï¼‰ï¼Œä»…ç”¨äºè½¨è¿¹é—´åˆ†æ')
@click.option('--output-table', help='è¾“å‡ºæ•°æ®åº“è¡¨å')
@click.option('--output-dir', default='data/intersection_results', help='è¾“å‡ºç›®å½•')
@click.option('--export-formats', multiple=True, default=['csv', 'geojson'], 
              help='å¯¼å‡ºæ ¼å¼ï¼Œå¯é€‰: csv, geojson, gpkg, shp')
@click.option('--parallel', is_flag=True, help='å¯ç”¨å¹¶è¡Œå¤„ç†ï¼ˆæŒ‰åŸå¸‚åˆ†ç»„ï¼‰')
@click.option('--max-workers', type=int, default=4, help='æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°')
@click.option('--quality-check', is_flag=True, help='æ‰§è¡Œç»“æœè´¨é‡æ£€æŸ¥')
def analyze_intersection(analysis_type: str, trajectory_table: str, target_table: str,
                        buffer_meters: float, time_tolerance: int, output_table: str,
                        output_dir: str, export_formats: tuple, parallel: bool,
                        max_workers: int, quality_check: bool):
    """è¿è¡Œè½¨è¿¹äº¤é›†åˆ†æã€‚
    
    æ”¯æŒå¤šç§ç±»å‹çš„äº¤é›†åˆ†æï¼š
    - trajectory-junction: è½¨è¿¹ä¸è·¯å£äº¤é›†
    - trajectory-road: è½¨è¿¹ä¸é“è·¯äº¤é›†  
    - trajectory-region: è½¨è¿¹ä¸åŒºåŸŸäº¤é›†
    - trajectory-to-trajectory: è½¨è¿¹é—´äº¤é›†
    - comprehensive: ç»¼åˆåˆ†æï¼ˆæ ¹æ®é…ç½®è¿è¡Œå¤šç§åˆ†æï¼‰
    
    Args:
        analysis_type: åˆ†æç±»å‹
        trajectory_table: è½¨è¿¹æ•°æ®è¡¨å
        target_table: ç›®æ ‡æ•°æ®è¡¨å
        buffer_meters: ç¼“å†²åŒºåŠå¾„
        time_tolerance: æ—¶é—´å®¹å·®ï¼ˆç§’ï¼‰
        output_table: è¾“å‡ºè¡¨å
        output_dir: è¾“å‡ºç›®å½•
        export_formats: å¯¼å‡ºæ ¼å¼åˆ—è¡¨
        parallel: æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
        max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        quality_check: æ˜¯å¦æ‰§è¡Œè´¨é‡æ£€æŸ¥
    """
    setup_logging()
    
    try:
        from .fusion import TrajectoryIntersectionAnalyzer, IntersectionProcessor
        
        click.echo(f"å¼€å§‹æ‰§è¡Œäº¤é›†åˆ†æ:")
        click.echo(f"  - åˆ†æç±»å‹: {analysis_type}")
        click.echo(f"  - è½¨è¿¹è¡¨: {trajectory_table}")
        click.echo(f"  - ç›®æ ‡è¡¨: {target_table or 'è‡ªåŠ¨ç¡®å®š'}")
        click.echo(f"  - ç¼“å†²åŒº: {buffer_meters}ç±³")
        click.echo(f"  - è¾“å‡ºç›®å½•: {output_dir}")
        click.echo(f"  - å¹¶è¡Œå¤„ç†: {'æ˜¯' if parallel else 'å¦'}")
        
        if analysis_type == 'comprehensive':
            # ç»¼åˆåˆ†æ
            processor = IntersectionProcessor(max_workers=max_workers)
            
            # æ„å»ºåˆ†æé…ç½®
            config = {
                'trajectory_junction_analysis': {
                    'enabled': True,
                    'trajectory_table': trajectory_table,
                    'junction_table': target_table or 'intersections',
                    'buffer_meters': buffer_meters,
                    'output_table': f"{output_table}_junction" if output_table else None
                },
                'trajectory_to_trajectory_analysis': {
                    'enabled': True,
                    'trajectory_table1': trajectory_table,
                    'buffer_meters': buffer_meters,
                    'time_tolerance_seconds': time_tolerance,
                    'output_table': f"{output_table}_traj" if output_table else None
                },
                'generate_visualizations': True
            }
            
            if target_table and 'road' in target_table.lower():
                config['trajectory_road_analysis'] = {
                    'enabled': True,
                    'trajectory_table': trajectory_table,
                    'road_table': target_table,
                    'buffer_meters': buffer_meters,
                    'output_table': f"{output_table}_road" if output_table else None
                }
            
            if target_table and 'region' in target_table.lower():
                config['trajectory_region_analysis'] = {
                    'enabled': True,
                    'trajectory_table': trajectory_table,
                    'region_table': target_table,
                    'buffer_meters': buffer_meters,
                    'output_table': f"{output_table}_region" if output_table else None
                }
            
            results = processor.run_comprehensive_intersection_analysis(
                analysis_config=config,
                output_dir=output_dir,
                export_formats=list(export_formats)
            )
            
            click.echo(f"âœ… ç»¼åˆåˆ†æå®Œæˆï¼Œå…±å¯¼å‡º {len(results['exported_files'])} ä¸ªæ–‡ä»¶")
            
        else:
            # å•ä¸ªåˆ†æç±»å‹
            analyzer = TrajectoryIntersectionAnalyzer()
            
            if analysis_type == 'trajectory-junction':
                if not target_table:
                    target_table = 'intersections'
                
                result_gdf = analyzer.analyze_trajectory_intersection_with_junctions(
                    trajectory_table=trajectory_table,
                    junction_table=target_table,
                    buffer_meters=buffer_meters,
                    output_table=output_table
                )
                
            elif analysis_type == 'trajectory-road':
                if not target_table:
                    target_table = 'roads'
                    
                result_gdf = analyzer.analyze_trajectory_intersection_with_roads(
                    trajectory_table=trajectory_table,
                    road_table=target_table,
                    buffer_meters=buffer_meters,
                    output_table=output_table
                )
                
            elif analysis_type == 'trajectory-region':
                if not target_table:
                    target_table = 'regions'
                    
                result_gdf = analyzer.analyze_trajectory_intersection_with_regions(
                    trajectory_table=trajectory_table,
                    region_table=target_table,
                    buffer_meters=buffer_meters,
                    output_table=output_table
                )
                
            elif analysis_type == 'trajectory-to-trajectory':
                result_gdf = analyzer.analyze_trajectory_to_trajectory_intersection(
                    trajectory_table1=trajectory_table,
                    trajectory_table2=target_table,
                    buffer_meters=buffer_meters,
                    time_tolerance_seconds=time_tolerance,
                    output_table=output_table
                )
            
            click.echo(f"âœ… åˆ†æå®Œæˆï¼Œæ‰¾åˆ° {len(result_gdf)} ä¸ªäº¤é›†")
            
            # å¯¼å‡ºç»“æœ
            if len(result_gdf) > 0:
                processor = IntersectionProcessor()
                exported_files = processor._export_results(
                    result_gdf,
                    Path(output_dir) / f"{analysis_type}_results",
                    list(export_formats)
                )
                click.echo(f"ç»“æœå·²å¯¼å‡ºåˆ° {len(exported_files)} ä¸ªæ–‡ä»¶")
                
                # è´¨é‡æ£€æŸ¥
                if quality_check:
                    quality_report = processor.evaluate_intersection_quality(result_gdf)
                    click.echo(f"è´¨é‡è¯„ä¼° - å¾—åˆ†: {quality_report['quality_score']}, ç­‰çº§: {quality_report['quality_level']}")
                    
                    if quality_report['recommendations']:
                        click.echo("æ”¹è¿›å»ºè®®:")
                        for rec in quality_report['recommendations']:
                            click.echo(f"  - {rec}")
            else:
                click.warning("æ²¡æœ‰æ‰¾åˆ°äº¤é›†ç»“æœ")
        
    except Exception as e:
        logger.error(f"äº¤é›†åˆ†æå¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--config-file', required=True, help='åˆ†æé…ç½®JSONæ–‡ä»¶è·¯å¾„')
@click.option('--output-dir', default='data/batch_intersection_results', help='è¾“å‡ºç›®å½•')
@click.option('--max-workers', type=int, default=4, help='æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°')
def batch_intersection_analysis(config_file: str, output_dir: str, max_workers: int):
    """æ‰¹é‡äº¤é›†åˆ†æã€‚
    
    ä»JSONé…ç½®æ–‡ä»¶è¯»å–å¤šä¸ªåˆ†æä»»åŠ¡å¹¶æ‰¹é‡æ‰§è¡Œã€‚
    
    é…ç½®æ–‡ä»¶æ ¼å¼ç¤ºä¾‹:
    {
        "analyses": [
            {
                "name": "analysis1",
                "type": "trajectory-junction",
                "trajectory_table": "clips_bbox",
                "target_table": "intersections", 
                "buffer_meters": 20.0,
                "export_formats": ["csv", "geojson"]
            }
        ]
    }
    
    Args:
        config_file: é…ç½®æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    """
    setup_logging()
    
    try:
        import json
        from .fusion import IntersectionProcessor
        
        # è¯»å–é…ç½®æ–‡ä»¶
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        click.echo(f"ä»é…ç½®æ–‡ä»¶åŠ è½½äº† {len(config.get('analyses', []))} ä¸ªåˆ†æä»»åŠ¡")
        
        processor = IntersectionProcessor(max_workers=max_workers)
        
        for i, analysis_config in enumerate(config.get('analyses', [])):
            analysis_name = analysis_config.get('name', f'analysis_{i+1}')
            click.echo(f"æ‰§è¡Œåˆ†æ: {analysis_name}")
            
            # æ„å»ºè¾“å‡ºç›®å½•
            analysis_output_dir = Path(output_dir) / analysis_name
            
            if analysis_config.get('type') == 'comprehensive':
                # ç»¼åˆåˆ†æ
                results = processor.run_comprehensive_intersection_analysis(
                    analysis_config=analysis_config.get('config', {}),
                    output_dir=str(analysis_output_dir),
                    export_formats=analysis_config.get('export_formats', ['csv', 'geojson'])
                )
            else:
                # å•ä¸ªåˆ†æ - è¿™é‡Œå¯ä»¥æ‰©å±•å…·ä½“çš„å•ä¸ªåˆ†æé€»è¾‘
                click.echo(f"å•ä¸ªåˆ†æç±»å‹ {analysis_config.get('type')} æš‚æœªåœ¨æ‰¹é‡æ¨¡å¼ä¸­å®ç°")
                continue
            
            click.echo(f"âœ… {analysis_name} å®Œæˆ")
        
        click.echo(f"âœ… æ‰¹é‡åˆ†æå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {output_dir}")
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
        raise

@cli.command()
@click.option('--left-table', default='clips_bbox', help='å·¦è¡¨åï¼ˆå¦‚clips_bboxï¼‰')
@click.option('--right-table', required=True, help='å³è¡¨åï¼ˆå¦‚intersectionsï¼‰')
@click.option('--spatial-relation', type=click.Choice([
    'intersects', 'within', 'contains', 'touches', 'crosses', 'overlaps', 'dwithin'
]), default='intersects', help='ç©ºé—´å…³ç³»')
@click.option('--distance-meters', type=float, help='è·ç¦»é˜ˆå€¼ï¼ˆä»…ç”¨äºdwithinå…³ç³»ï¼‰')
@click.option('--buffer-meters', type=float, default=0.0, help='ç¼“å†²åŒºåŠå¾„ï¼ˆç±³ï¼‰')
@click.option('--output-table', help='è¾“å‡ºæ•°æ®åº“è¡¨å')
@click.option('--output-file', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆCSVæ ¼å¼ï¼‰')
@click.option('--select-fields', help='é€‰æ‹©å­—æ®µï¼Œæ ¼å¼: field1,field2:method')
def spatial_join(left_table: str, right_table: str, spatial_relation: str,
                distance_meters: float, buffer_meters: float, output_table: str,
                output_file: str, select_fields: str):
    """æ‰§è¡Œç©ºé—´è¿æ¥åˆ†æ - ç±»ä¼¼QGISçš„join attributes by locationã€‚
    
    Examples:
        # åŸºç¡€ç›¸äº¤åˆ†æ
        spdatalab spatial-join --right-table intersections
        
        # è·ç¦»èŒƒå›´å†…è¿æ¥
        spdatalab spatial-join --right-table intersections --spatial-relation dwithin --distance-meters 50
        
        # è‡ªå®šä¹‰å­—æ®µé€‰æ‹©
        spdatalab spatial-join --right-table intersections --select-fields "inter_id,inter_type,count:count"
    """
    setup_logging()
    
    try:
        from .fusion import SpatialJoin, SpatialRelation
        
        click.echo(f"æ‰§è¡Œç©ºé—´è¿æ¥:")
        click.echo(f"  - å·¦è¡¨: {left_table}")
        click.echo(f"  - å³è¡¨: {right_table}")
        click.echo(f"  - ç©ºé—´å…³ç³»: {spatial_relation}")
        if distance_meters:
            click.echo(f"  - è·ç¦»é˜ˆå€¼: {distance_meters}ç±³")
        if buffer_meters > 0:
            click.echo(f"  - ç¼“å†²åŒº: {buffer_meters}ç±³")
        
        # è§£æå­—æ®µé€‰æ‹©
        parsed_fields = None
        if select_fields:
            parsed_fields = {}
            for field_spec in select_fields.split(','):
                if ':' in field_spec:
                    field, method = field_spec.split(':')
                    parsed_fields[field] = method
                else:
                    parsed_fields[field_spec] = field_spec
        
        # æ‰§è¡Œç©ºé—´è¿æ¥
        spatial_joiner = SpatialJoin()
        
        if buffer_meters > 0:
            # ä½¿ç”¨ç®€åŒ–æ¥å£
            result = spatial_joiner.bbox_intersect_features(
                feature_table=right_table,
                feature_type=right_table.replace('s', ''),  # ç®€å•å¤æ•°è½¬å•æ•°
                buffer_meters=buffer_meters,
                output_table=output_table
            )
        else:
            # ä½¿ç”¨å®Œæ•´æ¥å£
            result = spatial_joiner.join_attributes_by_location(
                left_table=left_table,
                right_table=right_table,
                spatial_relation=spatial_relation,
                distance_meters=distance_meters,
                select_fields=parsed_fields,
                output_table=output_table
            )
        
        click.echo(f"âœ… ç©ºé—´è¿æ¥å®Œæˆï¼Œå…± {len(result)} æ¡è®°å½•")
        
        # æ˜¾ç¤ºåŸºç¡€ç»Ÿè®¡
        if len(result) > 0:
            if 'scene_token' in result.columns:
                click.echo(f"  - å”¯ä¸€åœºæ™¯æ•°: {result['scene_token'].nunique()}")
            if 'city_id' in result.columns:
                click.echo(f"  - æ¶‰åŠåŸå¸‚æ•°: {result['city_id'].nunique()}")
        
        # å¯¼å‡ºæ–‡ä»¶
        if output_file and len(result) > 0:
            # ç§»é™¤å‡ ä½•åˆ—å¹¶ä¿å­˜ä¸ºCSV
            df = result.drop(columns=['geometry']) if 'geometry' in result.columns else result
            df.to_csv(output_file, index=False, encoding='utf-8')
            click.echo(f"  - ç»“æœå·²å¯¼å‡ºåˆ°: {output_file}")
        
    except Exception as e:
        logger.error(f"ç©ºé—´è¿æ¥å¤±è´¥: {str(e)}")
        raise

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®ã€‚"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
        ]
    )

def main():
    """ä¸»å‡½æ•°ã€‚"""
    parser = argparse.ArgumentParser(description='Spatial-Data-Lab å·¥å…·é›†')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # æ·»åŠ åœºæ™¯æ•°æ®åˆ—è¡¨ç”Ÿæˆå‘½ä»¤
    scene_list_parser = subparsers.add_parser('generate-scene-list',
                                            help='ç”Ÿæˆåœºæ™¯æ•°æ®åˆ—è¡¨')
    scene_list_parser.add_argument('--index-file', required=True,
                                 help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
    scene_list_parser.add_argument('--output', required=True,
                                 help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    scene_list_parser.set_defaults(func=generate_scene_list)
    
    # æ·»åŠ æ•°æ®é›†æ„å»ºå‘½ä»¤
    build_dataset_parser = subparsers.add_parser('build-dataset',
                                                help='æ„å»ºæ•°æ®é›†ç»“æ„')
    build_dataset_parser.add_argument('--index-file', required=True,
                                     help='ç´¢å¼•æ–‡ä»¶è·¯å¾„')
    build_dataset_parser.add_argument('--dataset-name', required=True,
                                     help='æ•°æ®é›†åç§°')
    build_dataset_parser.add_argument('--description', default='',
                                     help='æ•°æ®é›†æè¿°')
    build_dataset_parser.add_argument('--output', required=True,
                                     help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    build_dataset_parser.add_argument('--format', type=click.Choice(['json', 'parquet']), default='json',
                                     help='è¾“å‡ºæ ¼å¼ï¼Œjson æˆ– parquet')
    build_dataset_parser.set_defaults(func=build_dataset)
    
    args = parser.parse_args()
    if args.command:
        if args.command == 'generate-scene-list':
            args.func(args.index_file, args.output)
        elif args.command == 'build-dataset':
            args.func(args.index_file, args.dataset_name, args.description, args.output, args.format)
    else:
        parser.print_help()

if __name__ == '__main__':
    cli()