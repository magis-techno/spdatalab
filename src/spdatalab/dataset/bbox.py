from __future__ import annotations
import argparse
import json
import signal
import sys
import re
from pathlib import Path
from datetime import datetime
import geopandas as gpd, pandas as pd
from sqlalchemy import text, create_engine
from spdatalab.common.io_hive import hive_cursor
from typing import List, Dict
import multiprocessing as mp
from multiprocessing import Pool, Manager
from concurrent.futures import ProcessPoolExecutor, as_completed
import threading
import time
import os

# æ£€æŸ¥æ˜¯å¦æœ‰parquetæ”¯æŒ
try:
    import pyarrow as pa
    import pyarrow.parquet as pq
    PARQUET_AVAILABLE = True
except ImportError:
    PARQUET_AVAILABLE = False

LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"
POINT_TABLE = "public.ddi_data_points"

# å…¨å±€å˜é‡ç”¨äºä¼˜é›…é€€å‡º
interrupted = False

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å‡½æ•°ï¼Œç”¨äºä¼˜é›…é€€å‡º"""
    global interrupted
    print(f"\næ¥æ”¶åˆ°ä¸­æ–­ä¿¡å· ({signum})ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
    print("ç­‰å¾…å½“å‰æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œè¯·ç¨å€™...")
    interrupted = True

def setup_signal_handlers():
    """è®¾ç½®ä¿¡å·å¤„ç†å™¨"""
    signal.signal(signal.SIGINT, signal_handler)  # Ctrl+C
    signal.signal(signal.SIGTERM, signal_handler)  # ç»ˆæ­¢ä¿¡å·

class LightweightProgressTracker:
    """è½»é‡çº§è¿›åº¦è·Ÿè¸ªå™¨ï¼Œä½¿ç”¨Parquetæ–‡ä»¶å­˜å‚¨çŠ¶æ€ï¼Œé’ˆå¯¹å¤§è§„æ¨¡æ•°æ®ä¼˜åŒ–"""
    
    def __init__(self, work_dir="./bbox_import_logs"):
        self.work_dir = Path(work_dir).resolve()  # ä½¿ç”¨ç»å¯¹è·¯å¾„
        try:
            self.work_dir.mkdir(exist_ok=True, parents=True)
        except PermissionError:
            # å¦‚æœå½“å‰ç›®å½•æƒé™ä¸è¶³ï¼Œå°è¯•ä½¿ç”¨ä¸´æ—¶ç›®å½•
            import tempfile
            self.work_dir = Path(tempfile.gettempdir()) / "bbox_import_logs"
            self.work_dir.mkdir(exist_ok=True, parents=True)
            print(f"æƒé™ä¸è¶³ï¼Œä½¿ç”¨ä¸´æ—¶ç›®å½•: {self.work_dir}")
        
        # çŠ¶æ€æ–‡ä»¶è·¯å¾„
        self.success_file = self.work_dir / "successful_tokens.parquet"
        self.failed_file = self.work_dir / "failed_tokens.parquet"
        self.progress_file = self.work_dir / "progress.json"
        
        # å†…å­˜ç¼“å­˜ï¼ˆç”¨äºæ‰¹é‡æ“ä½œï¼‰
        self._success_cache = self._load_success_cache()
        self._failed_buffer = []  # å¤±è´¥è®°å½•ç¼“å†²åŒº
        self._success_buffer = []  # æˆåŠŸè®°å½•ç¼“å†²åŒº
        self._buffer_size = 1000  # ç¼“å†²åŒºå¤§å°
        
    def _load_success_cache(self):
        """åŠ è½½æˆåŠŸtokençš„ç¼“å­˜"""
        if self.success_file.exists() and PARQUET_AVAILABLE:
            try:
                df = pd.read_parquet(self.success_file)
                cache = set(df['scene_token'].tolist())
                print(f"å·²åŠ è½½ {len(cache)} ä¸ªæˆåŠŸå¤„ç†çš„scene_token")
                return cache
            except Exception as e:
                print(f"åŠ è½½æˆåŠŸè®°å½•å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶")
                return set()
        else:
            return set()
    
    def save_successful_batch(self, scene_tokens, batch_num=None):
        """æ‰¹é‡ä¿å­˜æˆåŠŸå¤„ç†çš„tokenï¼ˆä½¿ç”¨ç¼“å†²åŒºä¼˜åŒ–ï¼‰"""
        if not scene_tokens:
            return
            
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        timestamp = datetime.now()
        for token in scene_tokens:
            if token not in self._success_cache:
                self._success_buffer.append({
                    'scene_token': token,
                    'processed_at': timestamp,
                    'batch_num': batch_num
                })
                self._success_cache.add(token)
        
        # å¦‚æœç¼“å†²åŒºè¾¾åˆ°é˜ˆå€¼ï¼Œåˆ™å†™å…¥æ–‡ä»¶
        if len(self._success_buffer) >= self._buffer_size:
            self._flush_success_buffer()
    
    def _flush_success_buffer(self):
        """å°†æˆåŠŸè®°å½•ç¼“å†²åŒºå†™å…¥æ–‡ä»¶"""
        if not self._success_buffer or not PARQUET_AVAILABLE:
            return
            
        new_df = pd.DataFrame(self._success_buffer)
        
        try:
            if self.success_file.exists():
                # è¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶
                existing_df = pd.read_parquet(self.success_file)
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                # å»é‡ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
                combined_df = combined_df.drop_duplicates(subset=['scene_token'], keep='last')
            else:
                combined_df = new_df
            
            # å†™å…¥æ–‡ä»¶
            combined_df.to_parquet(self.success_file, index=False)
            print(f"å·²ä¿å­˜ {len(self._success_buffer)} ä¸ªæˆåŠŸè®°å½•åˆ°æ–‡ä»¶")
            
        except Exception as e:
            print(f"ä¿å­˜æˆåŠŸè®°å½•å¤±è´¥: {e}")
        
        # æ¸…ç©ºç¼“å†²åŒº
        self._success_buffer = []
    
    def save_failed_record(self, scene_token, error_msg, batch_num=None, step="unknown"):
        """ä¿å­˜å¤±è´¥è®°å½•ï¼ˆä½¿ç”¨ç¼“å†²åŒºä¼˜åŒ–ï¼‰"""
        self._failed_buffer.append({
            'scene_token': scene_token,
            'error_msg': str(error_msg),
            'batch_num': batch_num,
            'step': step,
            'failed_at': datetime.now()
        })
        
        # å¦‚æœç¼“å†²åŒºè¾¾åˆ°é˜ˆå€¼ï¼Œåˆ™å†™å…¥æ–‡ä»¶
        if len(self._failed_buffer) >= self._buffer_size:
            self._flush_failed_buffer()
    
    def _flush_failed_buffer(self):
        """å°†å¤±è´¥è®°å½•ç¼“å†²åŒºå†™å…¥æ–‡ä»¶"""
        if not self._failed_buffer or not PARQUET_AVAILABLE:
            return
            
        new_df = pd.DataFrame(self._failed_buffer)
        
        try:
            if self.failed_file.exists():
                # è¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶
                existing_df = pd.read_parquet(self.failed_file)
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            else:
                combined_df = new_df
            
            # å†™å…¥æ–‡ä»¶
            combined_df.to_parquet(self.failed_file, index=False)
            print(f"å·²ä¿å­˜ {len(self._failed_buffer)} ä¸ªå¤±è´¥è®°å½•åˆ°æ–‡ä»¶")
            
        except Exception as e:
            print(f"ä¿å­˜å¤±è´¥è®°å½•å¤±è´¥: {e}")
        
        # æ¸…ç©ºç¼“å†²åŒº
        self._failed_buffer = []
    
    def get_remaining_tokens(self, all_tokens):
        """è·å–è¿˜éœ€è¦å¤„ç†çš„token"""
        remaining = [token for token in all_tokens if token not in self._success_cache]
        print(f"æ€»è®¡ {len(all_tokens)} ä¸ªåœºæ™¯ï¼Œå·²æˆåŠŸå¤„ç† {len(self._success_cache)} ä¸ªï¼Œå‰©ä½™ {len(remaining)} ä¸ª")
        return remaining
    
    def check_tokens_exist(self, tokens):
        """æ‰¹é‡æ£€æŸ¥tokensæ˜¯å¦å·²å­˜åœ¨"""
        return set(tokens) & self._success_cache
    
    def load_failed_tokens(self):
        """åŠ è½½å¤±è´¥çš„tokensï¼Œç”¨äºé‡è¯•"""
        if not self.failed_file.exists() or not PARQUET_AVAILABLE:
            return []
        
        try:
            failed_df = pd.read_parquet(self.failed_file)
            # æ’é™¤å·²æˆåŠŸå¤„ç†çš„tokens
            failed_tokens = failed_df[~failed_df['scene_token'].isin(self._success_cache)]['scene_token'].unique().tolist()
            print(f"åŠ è½½äº† {len(failed_tokens)} ä¸ªå¤±è´¥çš„scene_token")
            return failed_tokens
        except Exception as e:
            print(f"åŠ è½½å¤±è´¥è®°å½•å¤±è´¥: {e}")
            return []
    
    def save_progress(self, total_scenes, processed_scenes, inserted_records, current_batch):
        """ä¿å­˜æ€»ä½“è¿›åº¦"""
        progress = {
            "total_scenes": total_scenes,
            "processed_scenes": processed_scenes,
            "inserted_records": inserted_records,
            "current_batch": current_batch,
            "timestamp": datetime.now().isoformat(),
            "successful_count": len(self._success_cache),
            "failed_count": len(self._failed_buffer) + (
                len(pd.read_parquet(self.failed_file)) if self.failed_file.exists() and PARQUET_AVAILABLE else 0
            )
        }
        
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        success_count = len(self._success_cache)
        
        failed_count = 0
        failed_by_step = {}
        
        if self.failed_file.exists() and PARQUET_AVAILABLE:
            try:
                failed_df = pd.read_parquet(self.failed_file)
                # æ’é™¤å·²æˆåŠŸå¤„ç†çš„
                active_failed = failed_df[~failed_df['scene_token'].isin(self._success_cache)]
                failed_count = len(active_failed['scene_token'].unique())
                
                # æŒ‰æ­¥éª¤ç»Ÿè®¡
                if not active_failed.empty:
                    failed_by_step = active_failed['step'].value_counts().to_dict()
                    
            except Exception as e:
                print(f"ç»Ÿè®¡å¤±è´¥è®°å½•æ—¶å‡ºé”™: {e}")
        
        return {
            'success_count': success_count,
            'failed_count': failed_count,
            'failed_by_step': failed_by_step
        }
    
    def finalize(self):
        """å®Œæˆå¤„ç†ï¼Œåˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº"""
        self._flush_success_buffer()
        self._flush_failed_buffer()

def create_table_if_not_exists(eng, table_name='clips_bbox'):
    """å¦‚æœè¡¨ä¸å­˜åœ¨åˆ™åˆ›å»ºè¡¨ - ä¸cleanup_clips_bbox.sqlä¿æŒä¸€è‡´"""
    # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
    check_table_sql = text(f"""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_schema = 'public' 
            AND table_name = '{table_name}'
        );
    """)
    
    try:
        with eng.connect() as conn:
            result = conn.execute(check_table_sql)
            table_exists = result.scalar()
            
            if table_exists:
                print(f"è¡¨ {table_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return True
                
            print(f"è¡¨ {table_name} ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»º...")
            
            # ä¸cleanup_clips_bbox.sqlä¿æŒä¸€è‡´çš„è¡¨ç»“æ„
            create_sql = text(f"""
                CREATE TABLE {table_name}(
                    id serial PRIMARY KEY,
                    scene_token text,
                    data_name text UNIQUE,
                    event_id text,
                    city_id text,
                    "timestamp" bigint,
                    all_good boolean
                );
            """)
            
            # ä½¿ç”¨PostGISæ·»åŠ å‡ ä½•åˆ—
            add_geom_sql = text(f"""
                SELECT AddGeometryColumn('public', '{table_name}', 'geometry', 4326, 'POLYGON', 2);
            """)
            
            # æ·»åŠ å‡ ä½•çº¦æŸ
            constraint_sql = text(f"""
                ALTER TABLE {table_name} ADD CONSTRAINT check_geom_type 
                    CHECK (ST_GeometryType(geometry) IN ('ST_Polygon', 'ST_Point'));
            """)
            
            # åˆ›å»ºç´¢å¼•
            index_sql = text(f"""
                CREATE INDEX idx_{table_name}_geometry ON {table_name} USING GIST(geometry);
                CREATE INDEX idx_{table_name}_data_name ON {table_name}(data_name);
                CREATE INDEX idx_{table_name}_scene_token ON {table_name}(scene_token);
            """)
            
            # æ‰§è¡ŒSQLè¯­å¥ï¼Œéœ€è¦åˆ†æ­¥æäº¤ä»¥ç¡®ä¿PostGISå‡½æ•°èƒ½æ‰¾åˆ°è¡¨
            conn.execute(create_sql)
            conn.commit()  # å…ˆæäº¤è¡¨åˆ›å»º
            
            # æ‰§è¡ŒPostGISç›¸å…³æ“ä½œ
            conn.execute(add_geom_sql)
            conn.execute(constraint_sql)
            conn.commit()  # æäº¤å‡ ä½•åˆ—å’Œçº¦æŸ
            
            # åˆ›å»ºç´¢å¼•
            conn.execute(index_sql)
            conn.commit()  # æœ€åæäº¤ç´¢å¼•
            
            print(f"æˆåŠŸåˆ›å»ºè¡¨ {table_name} åŠç›¸å…³ç´¢å¼•")
            return True
            
    except Exception as e:
        print(f"åˆ›å»ºè¡¨æ—¶å‡ºé”™: {str(e)}")
        print("å»ºè®®ï¼šå¦‚æœè¡¨å·²é€šè¿‡cleanup_clips_bbox.sqlåˆ›å»ºï¼Œè¯·ä½¿ç”¨ --no-create-table é€‰é¡¹")
        return False

def chunk(lst, n):
    """å°†åˆ—è¡¨åˆ†å—å¤„ç†"""
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

def load_scene_ids_from_json(file_path):
    """ä»JSONæ ¼å¼çš„æ•°æ®é›†æ–‡ä»¶ä¸­åŠ è½½scene_idåˆ—è¡¨"""
    with open(file_path, 'r', encoding='utf-8') as f:
        dataset_data = json.load(f)
    
    scene_ids = []
    subdatasets = dataset_data.get('subdatasets', [])
    
    for subdataset in subdatasets:
        scene_ids.extend(subdataset.get('scene_ids', []))
    
    print(f"ä»JSONæ–‡ä»¶åŠ è½½äº† {len(scene_ids)} ä¸ªscene_id")
    return scene_ids

def load_scene_ids_from_parquet(file_path):
    """ä»Parquetæ ¼å¼çš„æ•°æ®é›†æ–‡ä»¶ä¸­åŠ è½½scene_idåˆ—è¡¨"""
    if not PARQUET_AVAILABLE:
        raise ImportError("éœ€è¦å®‰è£… pandas å’Œ pyarrow æ‰èƒ½ä½¿ç”¨ parquet æ ¼å¼: pip install pandas pyarrow")
    
    df = pd.read_parquet(file_path)
    scene_ids = df['scene_id'].unique().tolist()
    
    print(f"ä»Parquetæ–‡ä»¶åŠ è½½äº† {len(scene_ids)} ä¸ªscene_id")
    return scene_ids

def load_scene_ids_from_text(file_path):
    """ä»æ–‡æœ¬æ–‡ä»¶ä¸­åŠ è½½scene_idåˆ—è¡¨ï¼ˆå…¼å®¹åŸæ ¼å¼ï¼‰"""
    scene_ids = [t.strip() for t in Path(file_path).read_text().splitlines() if t.strip()]
    print(f"ä»æ–‡æœ¬æ–‡ä»¶åŠ è½½äº† {len(scene_ids)} ä¸ªscene_id")
    return scene_ids

def load_scene_ids(file_path):
    """æ™ºèƒ½åŠ è½½scene_idåˆ—è¡¨ï¼Œè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼"""
    file_path = Path(file_path)
    
    if file_path.suffix.lower() == '.json':
        return load_scene_ids_from_json(file_path)
    elif file_path.suffix.lower() == '.parquet':
        return load_scene_ids_from_parquet(file_path)
    else:
        # é»˜è®¤æŒ‰æ–‡æœ¬æ ¼å¼å¤„ç†
        return load_scene_ids_from_text(file_path)

def fetch_meta(tokens):
    """æ‰¹é‡è·å–åœºæ™¯å…ƒæ•°æ®"""
    sql = ("SELECT id AS scene_token,origin_name AS data_name,event_id,city_id,timestamp "
           "FROM transform.ods_t_data_fragment_datalake WHERE id IN %(tok)s")
    with hive_cursor() as cur:
        cur.execute(sql, {"tok": tuple(tokens)})
        cols = [d[0] for d in cur.description]
        return pd.DataFrame(cur.fetchall(), columns=cols)

def fetch_bbox_with_geometry(names, eng):
    """æ‰¹é‡è·å–è¾¹ç•Œæ¡†ä¿¡æ¯å¹¶ç›´æ¥åœ¨PostGISä¸­æ„å»ºå‡ ä½•å¯¹è±¡"""
    sql_query = text(f"""
        WITH bbox_data AS (
            SELECT 
                dataset_name,
                ST_XMin(ST_Extent(point_lla)) AS xmin,
                ST_YMin(ST_Extent(point_lla)) AS ymin,
                ST_XMax(ST_Extent(point_lla)) AS xmax,
                ST_YMax(ST_Extent(point_lla)) AS ymax,
                bool_and(workstage = 2) AS all_good
            FROM {POINT_TABLE}
            WHERE dataset_name = ANY(:names_param)
            GROUP BY dataset_name
        )
        SELECT 
            dataset_name,
            all_good,
            CASE 
                WHEN xmin = xmax OR ymin = ymax THEN
                    -- å¯¹äºç‚¹æ•°æ®ï¼Œç›´æ¥åˆ›å»ºç‚¹å‡ ä½•å¯¹è±¡
                    ST_Point((xmin + xmax) / 2, (ymin + ymax) / 2)
                ELSE
                    -- å¯¹äºå·²æœ‰è¾¹ç•Œæ¡†çš„æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨è¾¹ç•Œæ¡†
                    ST_MakeEnvelope(xmin, ymin, xmax, ymax, 4326)
            END AS geometry
        FROM bbox_data;""")
    
    return gpd.read_postgis(
        sql_query, 
        eng, 
        params={"names_param": names},
        geom_col='geometry'
    )

def batch_insert_to_postgis(gdf, eng, table_name='clips_bbox', batch_size=1000, tracker=None, batch_num=None):
    """æ‰¹é‡æ’å…¥åˆ°PostGISï¼Œä¾èµ–æ•°æ®åº“çº¦æŸå¤„ç†é‡å¤æ•°æ®"""
    total_rows = len(gdf)
    inserted_rows = 0
    successful_tokens = []
    
    # åˆ†æ‰¹æ’å…¥
    for i in range(0, total_rows, batch_size):
        batch_gdf = gdf.iloc[i:i+batch_size]
        batch_tokens = batch_gdf['scene_token'].tolist()
        
        try:
            # ç›´æ¥æ’å…¥ï¼Œè®©æ•°æ®åº“å¤„ç†é‡å¤
            batch_gdf.to_postgis(
                table_name, 
                eng, 
                if_exists='append', 
                index=False
            )
            inserted_rows += len(batch_gdf)
            successful_tokens.extend(batch_tokens)
            print(f'[æ‰¹é‡æ’å…¥] å·²æ’å…¥: {inserted_rows}/{total_rows} è¡Œ')
            
        except Exception as e:
            error_str = str(e).lower()
            
            # å¦‚æœæ˜¯é‡å¤é”®è¿åçº¦æŸï¼Œå°è¯•é€è¡Œæ’å…¥ä»¥è¯†åˆ«å…·ä½“çš„é‡å¤è®°å½•
            if 'unique' in error_str or 'duplicate' in error_str or 'constraint' in error_str:
                print(f'[æ‰¹é‡æ’å…¥] æ‰¹æ¬¡ {i//batch_size + 1} é‡åˆ°é‡å¤æ•°æ®ï¼Œè¿›è¡Œé€è¡Œæ’å…¥')
                
                for idx, row in batch_gdf.iterrows():
                    scene_token = row['scene_token']
                    try:
                        # åˆ›å»ºå•è¡ŒGeoDataFrame
                        single_gdf = gpd.GeoDataFrame(
                            [row.drop('geometry')], 
                            geometry=[row.geometry], 
                            crs=4326
                        )
                        single_gdf.to_postgis(table_name, eng, if_exists='append', index=False)
                        inserted_rows += 1
                        successful_tokens.append(scene_token)
                    except Exception as row_e:
                        row_error_str = str(row_e).lower()
                        if 'unique' in row_error_str or 'duplicate' in row_error_str:
                            # é‡å¤æ•°æ®ï¼Œä¸è®°å½•ä¸ºå¤±è´¥ï¼Œåªæ˜¯è·³è¿‡
                            print(f'[è·³è¿‡é‡å¤] scene_token: {scene_token}')
                            successful_tokens.append(scene_token)  # è§†ä¸ºæˆåŠŸï¼ˆå·²å­˜åœ¨ï¼‰
                        else:
                            # å…¶ä»–ç±»å‹çš„é”™è¯¯æ‰è®°å½•ä¸ºå¤±è´¥
                            error_msg = f'æ’å…¥å¤±è´¥: {str(row_e)}'
                            print(f'[æ’å…¥é”™è¯¯] scene_token: {scene_token}: {error_msg}')
                            if tracker:
                                tracker.save_failed_record(scene_token, error_msg, batch_num, "database_insert")
            else:
                # éé‡å¤æ•°æ®é—®é¢˜ï¼Œè®°å½•ä¸ºå¤±è´¥
                print(f'[æ‰¹é‡æ’å…¥é”™è¯¯] æ‰¹æ¬¡ {i//batch_size + 1}: {str(e)}')
                for token in batch_tokens:
                    if tracker:
                        tracker.save_failed_record(token, f"æ‰¹é‡æ’å…¥å¼‚å¸¸: {str(e)}", batch_num, "database_insert")
    
    # æ‰¹é‡ä¿å­˜æˆåŠŸçš„tokensï¼ˆåŒ…æ‹¬é‡å¤è·³è¿‡çš„ï¼‰
    if tracker and successful_tokens:
        tracker.save_successful_batch(successful_tokens, batch_num)
    
    return inserted_rows

def normalize_subdataset_name(subdataset_name: str) -> str:
    """è§„èŒƒåŒ–å­æ•°æ®é›†åç§°
    
    è§„åˆ™ï¼š
    1. å»æ‰å¼€å¤´çš„ "GOD_E2E_"
    2. å¦‚æœæœ‰ "_sub_ddi_" åˆ™æˆªæ–­åˆ°è¿™é‡Œï¼ˆä¸åŒ…å«_sub_ddi_ï¼‰
    3. å¦‚æœä¸­é—´å‡ºç°ç±»ä¼¼ "_277736e2e_"ï¼ˆ277æ‰“å¤´ï¼Œe2eç»“å°¾ï¼‰çš„å­—ç¬¦ä¸²ï¼Œè¿™éƒ¨åˆ†åŠä¹‹åéƒ½ä¸è¦
    4. ç§»é™¤ç»“å°¾çš„æ—¶é—´æˆ³æ ¼å¼ "_YYYY_MM_DD_HH_MM_SS"
    
    Args:
        subdataset_name: åŸå§‹å­æ•°æ®é›†åç§°
        
    Returns:
        è§„èŒƒåŒ–åçš„åç§°
    """
    original_name = subdataset_name
    
    # 1. å»æ‰å¼€å¤´çš„ GOD_E2E_
    if subdataset_name.startswith("GOD_E2E_"):
        subdataset_name = subdataset_name[8:]  # len("GOD_E2E_") = 8
    
    # 2. å¤„ç† _sub_ddi_ æˆªæ–­
    sub_ddi_pos = subdataset_name.find("_sub_ddi_")
    if sub_ddi_pos != -1:
        subdataset_name = subdataset_name[:sub_ddi_pos]
    
    # 3. å¤„ç†ç±»ä¼¼ _277736e2e_ çš„æ¨¡å¼æˆªæ–­ï¼ˆ277æ‰“å¤´ï¼Œe2eç»“å°¾ï¼‰
    # åŒ¹é…æ¨¡å¼ï¼š_277å¼€å¤´ï¼Œä»»æ„å­—ç¬¦ï¼Œe2eç»“å°¾çš„å­—ç¬¦ä¸²
    hash_pattern = r'_277[^_]*e2e_'
    hash_match = re.search(hash_pattern, subdataset_name)
    if hash_match:
        # æˆªæ–­åˆ°åŒ¹é…ä½ç½®ä¹‹å‰
        subdataset_name = subdataset_name[:hash_match.start()]
    
    # 4. ç§»é™¤ç»“å°¾çš„æ—¶é—´æˆ³æ ¼å¼ "_YYYY_MM_DD_HH_MM_SS"
    timestamp_pattern = r'_\d{4}_\d{2}_\d{2}_\d{2}_\d{2}_\d{2}$'
    subdataset_name = re.sub(timestamp_pattern, '', subdataset_name)
    
    # 5. æ¸…ç†å’ŒéªŒè¯ç»“æœ
    subdataset_name = subdataset_name.strip('_')
    
    # ç¡®ä¿åç§°ä¸ä¸ºç©º
    if not subdataset_name:
        subdataset_name = "unnamed_dataset"
    
    print(f"åç§°è§„èŒƒåŒ–: '{original_name}' -> '{subdataset_name}'")
    return subdataset_name

def get_table_name_for_subdataset(subdataset_name: str) -> str:
    """ä¸ºå­æ•°æ®é›†ç”Ÿæˆåˆæ³•çš„PostgreSQLè¡¨å"""
    # å…ˆè§„èŒƒåŒ–åç§°
    normalized_name = normalize_subdataset_name(subdataset_name)
    
    # æ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼Œç¡®ä¿è¡¨ååˆæ³•
    clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', normalized_name)
    
    # è½¬æ¢ä¸ºå°å†™ï¼ˆPostgreSQLè¡¨åæœ€ä½³å®è·µï¼‰
    clean_name = clean_name.lower()
    
    # å¤„ç†è¿ç»­ä¸‹åˆ’çº¿é—®é¢˜
    original_underscores = len(re.findall(r'_{2,}', clean_name))
    clean_name = re.sub(r'_+', '_', clean_name)  # å¤šä¸ªä¸‹åˆ’çº¿åˆå¹¶ä¸ºä¸€ä¸ª
    clean_name = clean_name.strip('_')  # å»é™¤é¦–å°¾ä¸‹åˆ’çº¿
    
    if original_underscores > 0:
        print(f"è­¦å‘Š: å‘ç° {original_underscores} å¤„è¿ç»­ä¸‹åˆ’çº¿ï¼Œå·²è‡ªåŠ¨åˆå¹¶")
        
    # ç¡®ä¿æ²¡æœ‰ç©ºçš„è¡¨åéƒ¨åˆ†ï¼ˆç”±è¿ç»­ä¸‹åˆ’çº¿å¯¼è‡´ï¼‰
    name_parts = clean_name.split('_')
    # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²éƒ¨åˆ†
    valid_parts = [part for part in name_parts if part.strip()]
    if len(valid_parts) != len(name_parts):
        print(f"è­¦å‘Š: æ¸…ç†äº†ç©ºçš„è¡¨åæ®µï¼Œä» {len(name_parts)} æ®µå‡å°‘åˆ° {len(valid_parts)} æ®µ")
        clean_name = '_'.join(valid_parts)
    
    # ä½¿ç”¨ä¿å®ˆçš„é•¿åº¦é™åˆ¶ï¼Œä¸ºPostGISå…¼å®¹æ€§é¢„ç•™ç©ºé—´
    # clips_bbox_ = 12å­—ç¬¦ï¼Œæ‰€ä»¥ä¸»ä½“éƒ¨åˆ†é™åˆ¶åœ¨ 50-12=38 å­—ç¬¦
    max_main_length = 38
    
    if len(clean_name) > max_main_length:
        # ç›´æ¥æˆªæ–­ï¼ˆä¸éœ€è¦å¤„ç†æ—¶é—´æˆ³ï¼Œå› ä¸ºå·²ç»åœ¨normalizeé˜¶æ®µç§»é™¤äº†ï¼‰
        clean_name = clean_name[:max_main_length]
    
    # æ„å»ºæœ€ç»ˆè¡¨å
    table_name = f"clips_bbox_{clean_name}"
    
    # ç¡®ä¿è¡¨åç¬¦åˆPostgreSQLè§„èŒƒï¼ˆä»¥å­—æ¯å¼€å¤´ï¼‰
    if table_name[0].isdigit():
        table_name = "t_" + table_name
        # å¦‚æœåŠ å‰ç¼€åè¶…é•¿ï¼Œå†æ¬¡æˆªæ–­
        if len(table_name) > 50:  # ä¿å®ˆæˆªæ–­
            table_name = table_name[:50]
    
    # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
    if len(table_name) > 50:
        table_name = table_name[:50]
    
    # æœ€ç»ˆæ¸…ç†ï¼šç¡®ä¿æ²¡æœ‰è¿ç»­ä¸‹åˆ’çº¿å’Œç©ºæ®µ
    table_name = re.sub(r'_+', '_', table_name)  # åˆå¹¶è¿ç»­ä¸‹åˆ’çº¿
    table_name = table_name.strip('_')  # å»é™¤é¦–å°¾ä¸‹åˆ’çº¿
    
    # æ¸…ç†ç©ºæ®µ
    name_parts = table_name.split('_')
    valid_parts = [part for part in name_parts if part.strip()]
    if len(valid_parts) != len(name_parts):
        print(f"è­¦å‘Š: æœ€ç»ˆæ¸…ç†äº†ç©ºçš„è¡¨åæ®µï¼Œä» {len(name_parts)} æ®µå‡å°‘åˆ° {len(valid_parts)} æ®µ")
        table_name = '_'.join(valid_parts)
    
    # æœ€ç»ˆéªŒè¯è¡¨ååˆæ³•æ€§
    validation_result = validate_table_name(table_name)
    if not validation_result['valid']:
        print(f"è­¦å‘Š: è¡¨åéªŒè¯å¤±è´¥: {validation_result['issues']}")
    
    print(f"è¡¨åç”Ÿæˆ: '{subdataset_name}' -> '{table_name}' (é•¿åº¦: {len(table_name)})")
    return table_name

def validate_table_name(table_name: str) -> dict:
    """éªŒè¯è¡¨åçš„åˆæ³•æ€§
    
    Args:
        table_name: è¦éªŒè¯çš„è¡¨å
        
    Returns:
        åŒ…å«éªŒè¯ç»“æœçš„å­—å…¸
    """
    issues = []
    
    # æ£€æŸ¥é•¿åº¦
    if len(table_name) > 63:
        issues.append(f"è¡¨åè¿‡é•¿: {len(table_name)} > 63")
    
    # æ£€æŸ¥å­—ç¬¦
    if not re.match(r'^[a-zA-Z][a-zA-Z0-9_]*$', table_name):
        issues.append("è¡¨ååŒ…å«éæ³•å­—ç¬¦æˆ–ä¸ä»¥å­—æ¯å¼€å¤´")
    
    # æ£€æŸ¥å¤§å†™å­—æ¯ï¼ˆPostgreSQLæœ€ä½³å®è·µæ˜¯å°å†™ï¼‰
    if re.search(r'[A-Z]', table_name):
        issues.append("è¡¨ååŒ…å«å¤§å†™å­—æ¯ï¼Œå»ºè®®ä½¿ç”¨å°å†™")
    
    # æ£€æŸ¥è¿ç»­ä¸‹åˆ’çº¿
    if re.search(r'_{2,}', table_name):
        issues.append("è¡¨ååŒ…å«è¿ç»­ä¸‹åˆ’çº¿")
    
    # æ£€æŸ¥é¦–å°¾ä¸‹åˆ’çº¿
    if table_name.startswith('_') or table_name.endswith('_'):
        issues.append("è¡¨åä»¥ä¸‹åˆ’çº¿å¼€å¤´æˆ–ç»“å°¾")
    
    # æ£€æŸ¥ç©ºçš„æ®µ
    parts = table_name.split('_')
    empty_parts = [i for i, part in enumerate(parts) if not part.strip()]
    if empty_parts:
        issues.append(f"è¡¨ååŒ…å«ç©ºæ®µ: ä½ç½® {empty_parts}")
    
    return {
        'valid': len(issues) == 0,
        'issues': issues,
        'length': len(table_name)
    }

def create_table_for_subdataset(eng, subdataset_name, subdataset_metadata=None, base_table_name='clips_bbox'):
    """ä¸ºç‰¹å®šå­æ•°æ®é›†åˆ›å»ºåˆ†è¡¨ï¼Œæ”¯æŒæ ¹æ®metadataåŠ¨æ€æ·»åŠ å­—æ®µ"""
    table_name = get_table_name_for_subdataset(subdataset_name)
    
    # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
    check_table_sql = text(f"""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_schema = 'public' 
            AND table_name = '{table_name}'
        );
    """)
    
    try:
        with eng.connect() as conn:
            result = conn.execute(check_table_sql)
            table_exists = result.scalar()
            
            if table_exists:
                print(f"åˆ†è¡¨ {table_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return True, table_name
                
            print(f"åˆ›å»ºå­æ•°æ®é›†åˆ†è¡¨: {table_name}")
            
            # åŸºç¡€å­—æ®µï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            base_fields = [
                "id serial PRIMARY KEY",
                "scene_token text",
                "data_name text UNIQUE", 
                "event_id text",
                "city_id text",
                '"timestamp" bigint',
                "all_good boolean"
            ]
            
            # æ ¹æ®metadataåŠ¨æ€æ·»åŠ å­—æ®µ
            dynamic_fields = []
            if subdataset_metadata:
                # æ£€æµ‹æ˜¯å¦ä¸ºé—®é¢˜å•æ•°æ®é›†
                data_type = subdataset_metadata.get('data_type')
                if data_type == 'defect':
                    # ä¸ºé—®é¢˜å•æ•°æ®é›†æ·»åŠ ç‰¹æ®Šå­—æ®µ
                    dynamic_fields.append("data_type text DEFAULT 'defect'")
                    dynamic_fields.append("original_url text")
                    dynamic_fields.append("line_number integer")
                    
                    # åˆ†æscene_attributesä¸­çš„æ‰€æœ‰å­—æ®µç±»å‹
                    scene_attributes = subdataset_metadata.get('scene_attributes', {})
                    all_custom_fields = set()
                    field_types = {}
                    
                    for scene_attrs in scene_attributes.values():
                        for key, value in scene_attrs.items():
                            if key not in {'original_url', 'data_name', 'line_number'}:
                                all_custom_fields.add(key)
                                # æ¨æ–­å­—æ®µç±»å‹ï¼ˆå–æœ€å®½æ³›çš„ç±»å‹ï¼‰
                                inferred_type = infer_field_type(value)
                                if key not in field_types:
                                    field_types[key] = inferred_type
                                else:
                                    # å¦‚æœå·²æœ‰ç±»å‹ï¼Œé€‰æ‹©æ›´å®½æ³›çš„ç±»å‹
                                    current_type = field_types[key]
                                    new_type = merge_field_types(current_type, inferred_type)
                                    field_types[key] = new_type
                    
                    # æ·»åŠ åŠ¨æ€å­—æ®µ
                    for field_name in sorted(all_custom_fields):
                        field_type = field_types.get(field_name, 'text')
                        dynamic_fields.append(f"{field_name} {field_type}")
                        print(f"  æ·»åŠ åŠ¨æ€å­—æ®µ: {field_name} ({field_type})")
                else:
                    # æ ‡å‡†æ•°æ®é›†ï¼Œæ·»åŠ data_typeæ ‡è¯†
                    dynamic_fields.append("data_type text DEFAULT 'standard'")
            else:
                # å‘åå…¼å®¹ï¼šæ²¡æœ‰metadataçš„æƒ…å†µ
                dynamic_fields.append("data_type text DEFAULT 'standard'")
            
            # ç»„åˆæ‰€æœ‰å­—æ®µ
            all_fields = base_fields + dynamic_fields
            fields_sql = ",\n                ".join(all_fields)
            
            # åˆ›å»ºè¡¨çš„SQL
            create_sql = text(f"""
                CREATE TABLE {table_name}(
                {fields_sql}
                );
            """)
            
            # ä½¿ç”¨PostGISæ·»åŠ å‡ ä½•åˆ—
            add_geom_sql = text(f"""
                SELECT AddGeometryColumn('public', '{table_name}', 'geometry', 4326, 'POLYGON', 2);
            """)
            
            # æ·»åŠ å‡ ä½•çº¦æŸ
            constraint_sql = text(f"""
                ALTER TABLE {table_name} ADD CONSTRAINT check_{table_name}_geom_type 
                    CHECK (ST_GeometryType(geometry) IN ('ST_Polygon', 'ST_Point'));
            """)
            
            # åˆ›å»ºç´¢å¼•
            index_sql = text(f"""
                CREATE INDEX idx_{table_name}_geometry ON {table_name} USING GIST(geometry);
                CREATE INDEX idx_{table_name}_data_name ON {table_name}(data_name);
                CREATE INDEX idx_{table_name}_scene_token ON {table_name}(scene_token);
                CREATE INDEX idx_{table_name}_data_type ON {table_name}(data_type);
            """)
            
            # æ‰§è¡ŒSQLè¯­å¥ï¼Œéœ€è¦åˆ†æ­¥æäº¤ä»¥ç¡®ä¿PostGISå‡½æ•°èƒ½æ‰¾åˆ°è¡¨
            conn.execute(create_sql)
            conn.commit()  # å…ˆæäº¤è¡¨åˆ›å»º
            
            # æ‰§è¡ŒPostGISç›¸å…³æ“ä½œ
            conn.execute(add_geom_sql)
            conn.execute(constraint_sql)
            conn.commit()  # æäº¤å‡ ä½•åˆ—å’Œçº¦æŸ
            
            # åˆ›å»ºç´¢å¼•
            conn.execute(index_sql)
            conn.commit()  # æœ€åæäº¤ç´¢å¼•
            
            print(f"æˆåŠŸåˆ›å»ºåˆ†è¡¨ {table_name} åŠç›¸å…³ç´¢å¼•")
            if dynamic_fields:
                print(f"  åŒ…å« {len(dynamic_fields)} ä¸ªåŠ¨æ€å­—æ®µ")
            
            return True, table_name
            
    except Exception as e:
        print(f"åˆ›å»ºåˆ†è¡¨ {table_name} æ—¶å‡ºé”™: {str(e)}")
        return False, table_name

def infer_field_type(value):
    """æ¨æ–­å­—æ®µç±»å‹"""
    if isinstance(value, bool):
        return "boolean"
    elif isinstance(value, int):
        return "integer"
    elif isinstance(value, float):
        return "numeric"
    elif isinstance(value, str):
        # å°è¯•æ£€æµ‹ç‰¹æ®Šæ ¼å¼
        if value.lower() in ('true', 'false'):
            return "boolean"
        try:
            int(value)
            return "integer"
        except ValueError:
            try:
                float(value)
                return "numeric"
            except ValueError:
                return "text"
    else:
        return "text"

def merge_field_types(type1: str, type2: str) -> str:
    """åˆå¹¶ä¸¤ä¸ªå­—æ®µç±»å‹ï¼Œé€‰æ‹©æ›´å®½æ³›çš„ç±»å‹
    
    ç±»å‹ä¼˜å…ˆçº§ï¼ˆä»çª„åˆ°å®½ï¼‰ï¼šboolean < integer < numeric < text
    
    Args:
        type1: ç¬¬ä¸€ä¸ªç±»å‹
        type2: ç¬¬äºŒä¸ªç±»å‹
        
    Returns:
        åˆå¹¶åçš„ç±»å‹
    """
    # å®šä¹‰ç±»å‹ä¼˜å…ˆçº§
    type_priority = {
        'boolean': 1,
        'integer': 2, 
        'numeric': 3,
        'text': 4
    }
    
    # è·å–ä¼˜å…ˆçº§ï¼ŒæœªçŸ¥ç±»å‹é»˜è®¤ä¸ºtext
    priority1 = type_priority.get(type1, 4)
    priority2 = type_priority.get(type2, 4)
    
    # è¿”å›ä¼˜å…ˆçº§æ›´é«˜ï¼ˆæ›´å®½æ³›ï¼‰çš„ç±»å‹
    if priority1 >= priority2:
        return type1
    else:
        return type2

def convert_value_to_expected_type(field_name: str, value):
    """æ ¹æ®å­—æ®µåå’Œå€¼ï¼Œè½¬æ¢ä¸ºåˆé€‚çš„æ•°æ®ç±»å‹
    
    Args:
        field_name: å­—æ®µåç§°
        value: åŸå§‹å€¼
        
    Returns:
        è½¬æ¢åçš„å€¼
    """
    if value is None:
        return None
    
    # ç‰¹æ®Šå­—æ®µçš„ç±»å‹å¤„ç†
    if field_name == 'line_number':
        try:
            if isinstance(value, str):
                return int(float(value))  # å¤„ç†"15.0"è¿™ç§æƒ…å†µ
            elif isinstance(value, float):
                return int(value)
            elif isinstance(value, int):
                return value
            else:
                return 0
        except (ValueError, TypeError):
            return 0
    
    # æ¨æ–­å­—æ®µç±»å‹å¹¶è½¬æ¢
    inferred_type = infer_field_type(value)
    
    try:
        if inferred_type == "boolean":
            if isinstance(value, str):
                return value.lower() in ('true', '1', 'yes', 'on')
            else:
                return bool(value)
        elif inferred_type == "integer":
            if isinstance(value, str):
                return int(float(value))  # å¤„ç†"15.0"è¿™ç§æƒ…å†µ
            elif isinstance(value, float):
                return int(value)
            else:
                return int(value)
        elif inferred_type == "numeric":
            return float(value)
        else:
            return str(value)
    except (ValueError, TypeError) as e:
        print(f"è­¦å‘Š: å­—æ®µ {field_name} çš„å€¼ {value} ç±»å‹è½¬æ¢å¤±è´¥: {e}ï¼Œä½¿ç”¨å­—ç¬¦ä¸²ç±»å‹")
        return str(value)

def group_scenes_by_subdataset(dataset_file: str) -> Dict[str, Dict]:
    """æŒ‰å­æ•°æ®é›†åˆ†ç»„scene_idsï¼ŒåŒ…å«metadataä¿¡æ¯
    
    Args:
        dataset_file: datasetæ–‡ä»¶è·¯å¾„ï¼ˆJSON/Parquetæ ¼å¼ï¼‰
        
    Returns:
        å­—å…¸ï¼Œkeyä¸ºå­æ•°æ®é›†åç§°ï¼Œvalueä¸ºåŒ…å«scene_idså’Œmetadataçš„å­—å…¸
        æ ¼å¼ï¼š{subdataset_name: {'scene_ids': [...], 'metadata': {...}}}
    """
    from ..dataset.dataset_manager import DatasetManager
    
    try:
        dataset_manager = DatasetManager()
        dataset = dataset_manager.load_dataset(dataset_file)
        
        groups = {}
        total_scenes = 0
        
        print(f"ä»æ•°æ®é›†æ–‡ä»¶åŠ è½½: {dataset_file}")
        print(f"æ•°æ®é›†åç§°: {dataset.name}")
        print(f"å­æ•°æ®é›†æ•°é‡: {len(dataset.subdatasets)}")
        
        for subdataset in dataset.subdatasets:
            subdataset_name = subdataset.name
            scene_ids = subdataset.scene_ids
            metadata = subdataset.metadata or {}
            
            if scene_ids:
                groups[subdataset_name] = {
                    'scene_ids': scene_ids,
                    'metadata': metadata
                }
                total_scenes += len(scene_ids)
                
                # æ˜¾ç¤ºæ•°æ®é›†ç±»å‹
                data_type = metadata.get('data_type', 'standard')
                type_info = f" (ç±»å‹: {data_type})"
                if data_type == 'defect':
                    dynamic_fields = [k for k in metadata.keys() 
                                    if k not in {'data_type', 'original_url', 'data_name', 'line_number'} 
                                    and not k.startswith('data_')]
                    if dynamic_fields:
                        type_info += f", åŠ¨æ€å­—æ®µ: {len(dynamic_fields)}ä¸ª"
                
                print(f"  {subdataset_name}: {len(scene_ids)} ä¸ªåœºæ™¯{type_info}")
            else:
                print(f"  {subdataset_name}: æ— åœºæ™¯æ•°æ®ï¼Œè·³è¿‡")
        
        print(f"æ€»è®¡: {len(groups)} ä¸ªæœ‰æ•ˆå­æ•°æ®é›†ï¼Œ{total_scenes} ä¸ªåœºæ™¯")
        return groups
        
    except Exception as e:
        print(f"åˆ†ç»„scene_idså¤±è´¥: {str(e)}")
        raise

def batch_create_tables_for_subdatasets(eng, subdataset_groups: Dict[str, Dict]) -> Dict[str, str]:
    """æ‰¹é‡ä¸ºå­æ•°æ®é›†åˆ›å»ºåˆ†è¡¨ï¼Œæ”¯æŒåŠ¨æ€å­—æ®µ
    
    Args:
        eng: æ•°æ®åº“å¼•æ“
        subdataset_groups: å­æ•°æ®é›†åˆ†ç»„ä¿¡æ¯ï¼ŒåŒ…å«scene_idså’Œmetadata
                          æ ¼å¼ï¼š{subdataset_name: {'scene_ids': [...], 'metadata': {...}}}
        
    Returns:
        å­—å…¸ï¼Œkeyä¸ºåŸå§‹å­æ•°æ®é›†åç§°ï¼Œvalueä¸ºåˆ›å»ºçš„è¡¨å
    """
    table_mapping = {}
    success_count = 0
    
    print(f"å¼€å§‹æ‰¹é‡åˆ›å»º {len(subdataset_groups)} ä¸ªåˆ†è¡¨...")
    
    for i, (subdataset_name, subdataset_info) in enumerate(subdataset_groups.items(), 1):
        metadata = subdataset_info.get('metadata', {})
        data_type = metadata.get('data_type', 'standard')
        
        print(f"[{i}/{len(subdataset_groups)}] å¤„ç†: {subdataset_name} (ç±»å‹: {data_type})")
        
        success, table_name = create_table_for_subdataset(eng, subdataset_name, metadata)
        table_mapping[subdataset_name] = table_name
        
        if success:
            success_count += 1
        else:
            print(f"è­¦å‘Š: å­æ•°æ®é›† {subdataset_name} çš„åˆ†è¡¨åˆ›å»ºå¤±è´¥")
    
    print(f"æ‰¹é‡åˆ›å»ºå®Œæˆ: æˆåŠŸ {success_count}/{len(subdataset_groups)} ä¸ªåˆ†è¡¨")
    return table_mapping

def filter_partition_tables(tables: List[str], exclude_view: str = None, exclude_defect_tables: bool = True) -> List[str]:
    """è¿‡æ»¤å‡ºçœŸæ­£çš„åˆ†è¡¨ï¼Œæ’é™¤ä¸»è¡¨ã€è§†å›¾ã€ä¸´æ—¶è¡¨ç­‰
    
    Args:
        tables: è¡¨ååˆ—è¡¨
        exclude_view: è¦æ’é™¤çš„è§†å›¾åç§°
        exclude_defect_tables: æ˜¯å¦æ’é™¤é—®é¢˜å•æ•°æ®è¡¨
        
    Returns:
        è¿‡æ»¤åçš„åˆ†è¡¨åˆ—è¡¨
    """
    filtered = []
    
    for table in tables:
        # æ’é™¤ä¸»è¡¨
        if table == 'clips_bbox':
            continue
            
        # æ’é™¤æŒ‡å®šçš„è§†å›¾ï¼ˆé¿å…å¾ªç¯å¼•ç”¨ï¼‰
        if exclude_view and table == exclude_view:
            continue
            
        # æ’é™¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„è¡¨
        exclude_keywords = ['unified', 'temp', 'backup', 'test', 'tmp']
        if any(keyword in table.lower() for keyword in exclude_keywords):
            continue
            
        # åªåŒ…å«åˆ†è¡¨æ ¼å¼çš„è¡¨ï¼ˆå¿…é¡»ä»¥clips_bbox_å¼€å¤´ï¼‰
        if not (table.startswith('clips_bbox_') and table != 'clips_bbox'):
            continue
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé—®é¢˜å•æ•°æ®è¡¨ï¼ˆç®€åŒ–å®ç°ï¼ŒåŸºäºè¡¨åæ¨æ–­ï¼‰
        if exclude_defect_tables:
            try:
                from sqlalchemy import create_engine, text
                eng = create_engine(LOCAL_DSN, future=True)
                with eng.connect() as conn:
                    # æ£€æŸ¥è¡¨æ˜¯å¦åŒ…å«data_typeå­—æ®µä¸”å€¼ä¸º'defect'
                    check_defect_sql = text(f"""
                        SELECT EXISTS (
                            SELECT 1 FROM information_schema.columns 
                            WHERE table_name = '{table}' 
                            AND column_name = 'data_type'
                        );
                    """)
                    
                    has_data_type = conn.execute(check_defect_sql).scalar()
                    
                    if has_data_type:
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜å•æ•°æ®
                        check_defect_data_sql = text(f"""
                            SELECT EXISTS (
                                SELECT 1 FROM {table} 
                                WHERE data_type = 'defect' 
                                LIMIT 1
                            );
                        """)
                        
                        has_defect_data = conn.execute(check_defect_data_sql).scalar()
                        
                        if has_defect_data:
                            print(f"æ’é™¤é—®é¢˜å•æ•°æ®è¡¨: {table}")
                            continue
                            
            except Exception as e:
                # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œè®°å½•ä½†ä¸å½±å“è¿‡æ»¤
                print(f"æ£€æŸ¥è¡¨ {table} çš„æ•°æ®ç±»å‹æ—¶å‡ºé”™: {str(e)}")
        
        filtered.append(table)
    
    return filtered

def list_bbox_tables(eng) -> List[str]:
    """åˆ—å‡ºæ‰€æœ‰bboxç›¸å…³çš„è¡¨
    
    Args:
        eng: æ•°æ®åº“å¼•æ“
        
    Returns:
        bboxè¡¨ååˆ—è¡¨
    """
    list_tables_sql = text("""
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = 'public' 
        AND table_name LIKE 'clips_bbox%'
        ORDER BY table_name;
    """)
    
    try:
        with eng.connect() as conn:
            result = conn.execute(list_tables_sql)
            tables = [row[0] for row in result.fetchall()]
            return tables
    except Exception as e:
        print(f"åˆ—å‡ºbboxè¡¨å¤±è´¥: {str(e)}")
        return []

def create_unified_view(eng, view_name: str = 'clips_bbox_unified') -> bool:
    """åˆ›å»ºç»Ÿä¸€è§†å›¾ï¼Œèšåˆæ‰€æœ‰åˆ†è¡¨æ•°æ®
    
    Args:
        eng: æ•°æ®åº“å¼•æ“
        view_name: ç»Ÿä¸€è§†å›¾åç§°
        
    Returns:
        åˆ›å»ºæ˜¯å¦æˆåŠŸ
    """
    try:
        # è·å–æ‰€æœ‰bboxç›¸å…³è¡¨
        all_tables = list_bbox_tables(eng)
        if not all_tables:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•bboxè¡¨ï¼Œæ— æ³•åˆ›å»ºç»Ÿä¸€è§†å›¾")
            return False
        
        # è¿‡æ»¤å‡ºçœŸæ­£çš„åˆ†è¡¨ï¼Œæ’é™¤è§†å›¾ã€ä¸»è¡¨ç­‰
        bbox_tables = filter_partition_tables(all_tables, exclude_view=view_name)
        if not bbox_tables:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†è¡¨ï¼Œæ— æ³•åˆ›å»ºç»Ÿä¸€è§†å›¾")
            print(f"å¯ç”¨çš„è¡¨: {all_tables}")
            return False
        
        # æ„å»ºUNION ALLæŸ¥è¯¢
        union_parts = []
        for table_name in bbox_tables:
            # æå–å­æ•°æ®é›†åç§°ï¼ˆå»æ‰clips_bbox_å‰ç¼€ï¼‰
            subdataset_name = table_name.replace('clips_bbox_', '') if table_name.startswith('clips_bbox_') else table_name
            
            union_part = f"""
            SELECT 
                {table_name}.id,
                {table_name}.scene_token,
                {table_name}.data_name,
                {table_name}.event_id,
                {table_name}.city_id,
                {table_name}.timestamp,
                {table_name}.all_good,
                {table_name}.geometry,
                '{subdataset_name}' as subdataset_name,
                '{table_name}' as source_table
            FROM {table_name}
            """
            union_parts.append(union_part)
        
        # ç»„åˆå®Œæ•´çš„è§†å›¾æŸ¥è¯¢
        view_query = "UNION ALL\n".join(union_parts)
        
        # å…ˆåˆ é™¤ç°æœ‰è§†å›¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        drop_view_sql = text(f"DROP VIEW IF EXISTS {view_name};")
        
        # åˆ›å»ºæ–°è§†å›¾
        create_view_sql = text(f"""
            CREATE OR REPLACE VIEW {view_name} AS
            {view_query};
        """)
        
        with eng.connect() as conn:
            conn.execute(drop_view_sql)
            conn.execute(create_view_sql)
            conn.commit()
        
        print(f"æˆåŠŸåˆ›å»ºç»Ÿä¸€è§†å›¾ {view_name}ï¼ŒåŒ…å« {len(bbox_tables)} ä¸ªåˆ†è¡¨:")
        for table in bbox_tables:
            print(f"  - {table}")
        
        return True
        
    except Exception as e:
        print(f"åˆ›å»ºç»Ÿä¸€è§†å›¾å¤±è´¥: {str(e)}")
        
        # æä¾›è°ƒè¯•ä¿¡æ¯
        try:
            print(f"è°ƒè¯•ä¿¡æ¯:")
            print(f"  - æ‰¾åˆ°çš„è¡¨æ•°é‡: {len(bbox_tables) if 'bbox_tables' in locals() else 'N/A'}")
            if 'bbox_tables' in locals() and bbox_tables:
                print(f"  - è¡¨åˆ—è¡¨: {', '.join(bbox_tables)}")
            
            # æ˜¾ç¤ºç”Ÿæˆçš„æŸ¥è¯¢ï¼ˆå‰100ä¸ªå­—ç¬¦ï¼‰
            if 'view_query' in locals():
                query_preview = view_query[:200] + "..." if len(view_query) > 200 else view_query
                print(f"  - ç”Ÿæˆçš„æŸ¥è¯¢é¢„è§ˆ: {query_preview}")
                
        except Exception as debug_e:
            print(f"  - æ— æ³•æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯: {str(debug_e)}")
        
        return False

def create_qgis_compatible_unified_view(eng, view_name: str = 'clips_bbox_unified_qgis') -> bool:
    """
    åˆ›å»ºQGISå…¼å®¹çš„ç»Ÿä¸€è§†å›¾ï¼Œå¸¦å…¨å±€å”¯ä¸€ID
    
    Args:
        eng: SQLAlchemy engine
        view_name: è§†å›¾åç§°
        
    Returns:
        bool: åˆ›å»ºæ˜¯å¦æˆåŠŸ
    """
    try:
        # è·å–åˆ†è¡¨åˆ—è¡¨
        all_tables = list_bbox_tables(eng)
        bbox_tables = filter_partition_tables(all_tables, exclude_view=view_name)
        
        if not bbox_tables:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†è¡¨ï¼Œæ— æ³•åˆ›å»ºQGISå…¼å®¹çš„ç»Ÿä¸€è§†å›¾")
            return False
        
        print(f"æ­£åœ¨ä¸º {len(bbox_tables)} ä¸ªåˆ†è¡¨åˆ›å»ºQGISå…¼å®¹çš„ç»Ÿä¸€è§†å›¾...")
        
        # æ„å»ºå¸¦ROW_NUMBERçš„UNIONæŸ¥è¯¢
        union_parts = []
        for table_name in bbox_tables:
            subdataset_name = table_name.replace('clips_bbox_', '') if table_name.startswith('clips_bbox_') else table_name
            
            union_part = f"""
            SELECT 
                {table_name}.id as original_id,
                {table_name}.scene_token,
                {table_name}.data_name,
                {table_name}.event_id,
                {table_name}.city_id,
                {table_name}.timestamp,
                {table_name}.all_good,
                {table_name}.geometry,
                '{subdataset_name}' as subdataset_name,
                '{table_name}' as source_table
            FROM {table_name}
            """
            union_parts.append(union_part)
        
        # åŒ…è£…åœ¨ROW_NUMBERä¸­åˆ›å»ºå…¨å±€å”¯ä¸€ID
        inner_query = "UNION ALL\n".join(union_parts)
        
        view_query = f"""
        SELECT 
            ROW_NUMBER() OVER (ORDER BY source_table, original_id) as qgis_id,
            original_id,
            scene_token,
            data_name,
            event_id,
            city_id,
            timestamp,
            all_good,
            geometry,
            subdataset_name,
            source_table
        FROM (
            {inner_query}
        ) as unified_data
        """
        
        # åˆ›å»ºè§†å›¾
        drop_view_sql = text(f"DROP VIEW IF EXISTS {view_name};")
        create_view_sql = text(f"CREATE OR REPLACE VIEW {view_name} AS {view_query};")
        
        with eng.connect() as conn:
            conn.execute(drop_view_sql)
            conn.execute(create_view_sql)
            conn.commit()
        
        print(f"âœ… æˆåŠŸåˆ›å»ºQGISå…¼å®¹çš„ç»Ÿä¸€è§†å›¾ {view_name}")
        print(f"ğŸ“‹ åœ¨QGISä¸­åŠ è½½æ—¶ï¼Œè¯·é€‰æ‹© 'qgis_id' ä½œä¸ºä¸»é”®åˆ—")
        print(f"ğŸ” è§†å›¾åŒ…å«ä»¥ä¸‹åˆ†è¡¨: {', '.join(bbox_tables)}")
        
        return True
        
    except Exception as e:
        print(f"åˆ›å»ºQGISå…¼å®¹ç»Ÿä¸€è§†å›¾å¤±è´¥: {str(e)}")
        return False

def create_materialized_unified_view(eng, view_name: str = 'clips_bbox_unified_mat') -> bool:
    """
    åˆ›å»ºç‰©åŒ–è§†å›¾ï¼Œæä¾›æ›´å¥½çš„QGISæ€§èƒ½
    
    Args:
        eng: SQLAlchemy engine
        view_name: ç‰©åŒ–è§†å›¾åç§°
        
    Returns:
        bool: åˆ›å»ºæ˜¯å¦æˆåŠŸ
    """
    try:
        # è·å–åˆ†è¡¨åˆ—è¡¨
        all_tables = list_bbox_tables(eng)
        bbox_tables = filter_partition_tables(all_tables, exclude_view=view_name)
        
        if not bbox_tables:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†è¡¨ï¼Œæ— æ³•åˆ›å»ºç‰©åŒ–è§†å›¾")
            return False
        
        print(f"æ­£åœ¨ä¸º {len(bbox_tables)} ä¸ªåˆ†è¡¨åˆ›å»ºç‰©åŒ–è§†å›¾...")
        
        # æ„å»ºUNIONæŸ¥è¯¢
        union_parts = []
        for table_name in bbox_tables:
            subdataset_name = table_name.replace('clips_bbox_', '') if table_name.startswith('clips_bbox_') else table_name
            
            union_part = f"""
            SELECT 
                {table_name}.id as original_id,
                {table_name}.scene_token,
                {table_name}.data_name,
                {table_name}.event_id,
                {table_name}.city_id,
                {table_name}.timestamp,
                {table_name}.all_good,
                {table_name}.geometry,
                '{subdataset_name}' as subdataset_name,
                '{table_name}' as source_table
            FROM {table_name}
            """
            union_parts.append(union_part)
        
        inner_query = "UNION ALL\n".join(union_parts)
        
        # åˆ›å»ºç‰©åŒ–è§†å›¾SQL
        drop_view_sql = text(f"DROP MATERIALIZED VIEW IF EXISTS {view_name};")
        
        create_mat_view_sql = text(f"""
            CREATE MATERIALIZED VIEW {view_name} AS
            SELECT 
                ROW_NUMBER() OVER (ORDER BY source_table, original_id) as qgis_id,
                original_id,
                scene_token,
                data_name,
                event_id,
                city_id,
                timestamp,
                all_good,
                geometry,
                subdataset_name,
                source_table
            FROM (
                {inner_query}
            ) as unified_data;
        """)
        
        # åˆ›å»ºç´¢å¼•
        create_index_sql = text(f"""
            CREATE UNIQUE INDEX IF NOT EXISTS {view_name}_qgis_id_idx 
            ON {view_name} (qgis_id);
        """)
        
        create_spatial_index_sql = text(f"""
            CREATE INDEX IF NOT EXISTS {view_name}_geom_idx 
            ON {view_name} USING GIST (geometry);
        """)
        
        with eng.connect() as conn:
            conn.execute(drop_view_sql)
            conn.execute(create_mat_view_sql)
            conn.execute(create_index_sql)
            conn.execute(create_spatial_index_sql)
            conn.commit()
        
        print(f"âœ… æˆåŠŸåˆ›å»ºç‰©åŒ–è§†å›¾ {view_name}")
        print(f"ğŸ“‹ åœ¨QGISä¸­ä½¿ç”¨ 'qgis_id' ä½œä¸ºä¸»é”®åˆ—")
        print(f"ğŸ’¡ æç¤ºï¼šæ•°æ®æ›´æ–°åéœ€è¦åˆ·æ–°ç‰©åŒ–è§†å›¾ï¼šREFRESH MATERIALIZED VIEW {view_name};")
        print(f"ğŸ” ç‰©åŒ–è§†å›¾åŒ…å«ä»¥ä¸‹åˆ†è¡¨: {', '.join(bbox_tables)}")
        
        return True
        
    except Exception as e:
        print(f"åˆ›å»ºç‰©åŒ–è§†å›¾å¤±è´¥: {str(e)}")
        return False

def refresh_materialized_view(eng, view_name: str = 'clips_bbox_unified_mat') -> bool:
    """
    åˆ·æ–°ç‰©åŒ–è§†å›¾
    
    Args:
        eng: SQLAlchemy engine
        view_name: ç‰©åŒ–è§†å›¾åç§°
        
    Returns:
        bool: åˆ·æ–°æ˜¯å¦æˆåŠŸ
    """
    try:
        refresh_sql = text(f"REFRESH MATERIALIZED VIEW {view_name};")
        
        with eng.connect() as conn:
            print(f"æ­£åœ¨åˆ·æ–°ç‰©åŒ–è§†å›¾ {view_name}...")
            conn.execute(refresh_sql)
            conn.commit()
        
        print(f"âœ… ç‰©åŒ–è§†å›¾ {view_name} åˆ·æ–°å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"åˆ·æ–°ç‰©åŒ–è§†å›¾å¤±è´¥: {str(e)}")
        return False

def maintain_unified_view(eng, view_name: str = 'clips_bbox_unified') -> bool:
    """ç»´æŠ¤ç»Ÿä¸€è§†å›¾ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å½“å‰çš„åˆ†è¡¨
    
    Args:
        eng: æ•°æ®åº“å¼•æ“  
        view_name: ç»Ÿä¸€è§†å›¾åç§°
        
    Returns:
        ç»´æŠ¤æ˜¯å¦æˆåŠŸ
    """
    try:
        # æ£€æŸ¥è§†å›¾æ˜¯å¦å­˜åœ¨
        check_view_sql = text(f"""
            SELECT EXISTS (
                SELECT FROM information_schema.views 
                WHERE table_schema = 'public' 
                AND table_name = '{view_name}'
            );
        """)
        
        with eng.connect() as conn:
            result = conn.execute(check_view_sql)
            view_exists = result.scalar()
        
        if not view_exists:
            print(f"è§†å›¾ {view_name} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°è§†å›¾")
            return create_unified_view(eng, view_name)
        else:
            print(f"è§†å›¾ {view_name} å·²å­˜åœ¨ï¼Œé‡æ–°åˆ›å»ºä»¥åŒ…å«æœ€æ–°çš„åˆ†è¡¨")
            return create_unified_view(eng, view_name)
        
    except Exception as e:
        print(f"ç»´æŠ¤ç»Ÿä¸€è§†å›¾å¤±è´¥: {str(e)}")
        return False

def process_subdataset_parallel(args):
    """å¹¶è¡Œå¤„ç†å•ä¸ªå­æ•°æ®é›†çš„åŒ…è£…å‡½æ•°
    
    Args:
        args: (subdataset_name, scene_ids, table_name, batch_size, insert_batch_size, work_dir, dsn, metadata)
        
    Returns:
        (subdataset_name, processed_count, inserted_count, success)
    """
    subdataset_name, scene_ids, table_name, batch_size, insert_batch_size, work_dir, dsn, metadata = args
    
    try:
        # ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥
        from sqlalchemy import create_engine
        eng = create_engine(dsn, future=True)
        
        # åˆ›å»ºç‹¬ç«‹çš„è¿›åº¦è·Ÿè¸ªå™¨
        sub_work_dir = f"{work_dir}/{subdataset_name}"
        sub_tracker = LightweightProgressTracker(sub_work_dir)
        
        # è·å–éœ€è¦å¤„ç†çš„åœºæ™¯ID
        remaining_scene_ids = sub_tracker.get_remaining_tokens(scene_ids)
        
        if not remaining_scene_ids:
            print(f"  ğŸ”„ [{subdataset_name}] æ‰€æœ‰åœºæ™¯å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡")
            return subdataset_name, 0, 0, True
        
        print(f"  ğŸš€ [{subdataset_name}] å¼€å§‹å¤„ç† {len(remaining_scene_ids)} ä¸ªåœºæ™¯")
        
        # å¤„ç†å½“å‰å­æ•°æ®é›†çš„æ•°æ®
        processed_count, inserted_count = process_subdataset_scenes(
            eng, remaining_scene_ids, table_name, batch_size, insert_batch_size, sub_tracker, metadata
        )
        
        print(f"  âœ… [{subdataset_name}] å®Œæˆ: å¤„ç† {processed_count} ä¸ªï¼Œæ’å…¥ {inserted_count} æ¡è®°å½•")
        
        return subdataset_name, processed_count, inserted_count, True
        
    except Exception as e:
        print(f"  âŒ [{subdataset_name}] å¤„ç†å¤±è´¥: {str(e)}")
        return subdataset_name, 0, 0, False

def run_with_partitioning_parallel(input_path, batch=1000, insert_batch=1000, work_dir="./bbox_import_logs", 
                                 create_unified_view_flag=True, maintain_view_only=False, max_workers=None):
    """ä½¿ç”¨å¹¶è¡Œåˆ†è¡¨æ¨¡å¼è¿è¡Œè¾¹ç•Œæ¡†å¤„ç†
    
    Args:
        input_path: è¾“å…¥æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        batch: å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch: æ’å…¥æ‰¹æ¬¡å¤§å°  
        work_dir: å·¥ä½œç›®å½•
        create_unified_view_flag: æ˜¯å¦åˆ›å»ºç»Ÿä¸€è§†å›¾
        maintain_view_only: æ˜¯å¦åªç»´æŠ¤è§†å›¾ï¼ˆä¸å¤„ç†æ•°æ®ï¼‰
        max_workers: æœ€å¤§å¹¶è¡Œworkeræ•°é‡ï¼ŒNoneä¸ºè‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
    """
    global interrupted
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()
    
    # ç¡®å®šå¹¶è¡Œworkeræ•°é‡
    if max_workers is None:
        # æ™ºèƒ½é»˜è®¤å€¼ï¼šCPUæ ¸å¿ƒæ•° * 1.5ï¼Œä½†ä¸è¶…è¿‡16ï¼ˆå¯é€šè¿‡å‚æ•°è¦†ç›–ï¼‰
        cpu_count = mp.cpu_count()
        max_workers = min(int(cpu_count * 1.5), 16)
        print(f"ğŸ” æ£€æµ‹åˆ° {cpu_count} ä¸ªCPUæ ¸å¿ƒï¼Œé»˜è®¤ä½¿ç”¨ {max_workers} ä¸ªworkers")
    else:
        print(f"ğŸ¯ ç”¨æˆ·æŒ‡å®šä½¿ç”¨ {max_workers} ä¸ªworkers")
    
    print(f"=== å¹¶è¡Œåˆ†è¡¨æ¨¡å¼å¤„ç†å¼€å§‹ ===")
    print(f"è¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"å·¥ä½œç›®å½•: {work_dir}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch}")
    print(f"æ’å…¥æ‰¹æ¬¡å¤§å°: {insert_batch}")
    print(f"å¹¶è¡Œworkeræ•°: {max_workers}")
    print(f"åˆ›å»ºç»Ÿä¸€è§†å›¾: {create_unified_view_flag}")
    print(f"ä»…ç»´æŠ¤è§†å›¾: {maintain_view_only}")
    
    eng = create_engine(LOCAL_DSN, future=True)
    
    # å¦‚æœåªæ˜¯ç»´æŠ¤è§†å›¾
    if maintain_view_only:
        print("\n=== ç»´æŠ¤ç»Ÿä¸€è§†å›¾æ¨¡å¼ ===")
        success = maintain_unified_view(eng)
        if success:
            print("âœ… ç»Ÿä¸€è§†å›¾ç»´æŠ¤å®Œæˆ")
        else:
            print("âŒ ç»Ÿä¸€è§†å›¾ç»´æŠ¤å¤±è´¥")
        return
    
    try:
        # æ­¥éª¤1: æŒ‰å­æ•°æ®é›†åˆ†ç»„åœºæ™¯
        print("\n=== æ­¥éª¤1: åˆ†ç»„åœºæ™¯æ•°æ® ===")
        scene_groups = group_scenes_by_subdataset(input_path)
        
        if not scene_groups:
            print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åœºæ™¯åˆ†ç»„æ•°æ®")
            return
        
        print(f"æ‰¾åˆ° {len(scene_groups)} ä¸ªå­æ•°æ®é›†")
        
        # æ­¥éª¤2: æ‰¹é‡åˆ›å»ºåˆ†è¡¨
        print("\n=== æ­¥éª¤2: åˆ›å»ºåˆ†è¡¨ ===")
        table_mapping = batch_create_tables_for_subdatasets(eng, scene_groups)
        
        # æ­¥éª¤3: å¹¶è¡Œå¤„ç†æ¯ä¸ªå­æ•°æ®é›†
        print(f"\n=== æ­¥éª¤3: å¹¶è¡Œåˆ†è¡¨æ•°æ®å¤„ç† ({max_workers} workers) ===")
        
        # å‡†å¤‡å¹¶è¡Œä»»åŠ¡å‚æ•°
        task_args = []
        for subdataset_name, subdataset_info in scene_groups.items():
            scene_ids = subdataset_info['scene_ids']
            metadata = subdataset_info.get('metadata', {})
            table_name = table_mapping[subdataset_name]
            task_args.append((
                subdataset_name, scene_ids, table_name, 
                batch, insert_batch, work_dir, LOCAL_DSN, metadata
            ))
        
        # æ‰§è¡Œå¹¶è¡Œå¤„ç†
        total_processed = 0
        total_inserted = 0
        completed_count = 0
        failed_count = 0
        
        print(f"å¯åŠ¨ {len(task_args)} ä¸ªå¹¶è¡Œä»»åŠ¡...")
        
        start_time = time.time()
        
        # ä½¿ç”¨ProcessPoolExecutorè¿›è¡Œå¹¶è¡Œå¤„ç†
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_subdataset = {
                executor.submit(process_subdataset_parallel, args): args[0] 
                for args in task_args
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_subdataset):
                if interrupted:
                    print("\nâš ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢å‰©ä½™ä»»åŠ¡...")
                    executor.shutdown(wait=False)
                    break
                
                subdataset_name = future_to_subdataset[future]
                try:
                    result_name, processed, inserted, success = future.result()
                    completed_count += 1
                    
                    if success:
                        total_processed += processed
                        total_inserted += inserted
                        print(f"âœ… [{completed_count}/{len(task_args)}] {subdataset_name}: {processed}å¤„ç†/{inserted}æ’å…¥")
                    else:
                        failed_count += 1
                        print(f"âŒ [{completed_count}/{len(task_args)}] {subdataset_name}: å¤„ç†å¤±è´¥")
                        
                except Exception as e:
                    failed_count += 1
                    print(f"âŒ [{completed_count}/{len(task_args)}] {subdataset_name}: å¼‚å¸¸ - {str(e)}")
        
        processing_time = time.time() - start_time
        
        # æ­¥éª¤4: åˆ›å»ºç»Ÿä¸€è§†å›¾ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if create_unified_view_flag and not interrupted:
            print("\n=== æ­¥éª¤4: åˆ›å»ºç»Ÿä¸€è§†å›¾ ===")
            success = create_unified_view(eng)
            if success:
                print("âœ… ç»Ÿä¸€è§†å›¾åˆ›å»ºå®Œæˆ")
            else:
                print("âŒ ç»Ÿä¸€è§†å›¾åˆ›å»ºå¤±è´¥")
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print(f"\n=== å¹¶è¡Œåˆ†è¡¨å¤„ç†å®Œæˆ ===")
        print(f"å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        print(f"æ€»è®¡å¤„ç†: {total_processed} æ¡è®°å½•")
        print(f"æ€»è®¡æ’å…¥: {total_inserted} æ¡è®°å½•")
        print(f"æˆåŠŸå­æ•°æ®é›†: {completed_count - failed_count}/{len(scene_groups)}")
        if failed_count > 0:
            print(f"å¤±è´¥å­æ•°æ®é›†: {failed_count}")
        
        if interrupted:
            print("âš ï¸  å¤„ç†è¢«ä¸­æ–­ï¼Œéƒ¨åˆ†æ•°æ®å¯èƒ½æœªå®Œæˆ")
        else:
            print("âœ… å¹¶è¡Œåˆ†è¡¨å¤„ç†å®Œæˆ")
            
        # è®¡ç®—æ€§èƒ½æå‡ä¼°ç®—
        if processing_time > 0:
            estimated_sequential_time = processing_time * max_workers
            speedup = estimated_sequential_time / processing_time
            print(f"ğŸš€ é¢„è®¡æ€§èƒ½æå‡: {speedup:.1f}x (ç›¸æ¯”é¡ºåºå¤„ç†)")
            
    except KeyboardInterrupt:
        print(f"\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        interrupted = True
    except Exception as e:
        print(f"\nå¹¶è¡Œåˆ†è¡¨å¤„ç†é‡åˆ°é”™è¯¯: {str(e)}")
    finally:
        print(f"\næ—¥å¿—å’Œè¿›åº¦æ–‡ä»¶ä¿å­˜åœ¨: {work_dir}")

def run_with_partitioning(input_path, batch=1000, insert_batch=1000, work_dir="./bbox_import_logs", 
                         create_unified_view_flag=True, maintain_view_only=False, use_parallel=False, 
                         max_workers=None):
    """ä½¿ç”¨åˆ†è¡¨æ¨¡å¼è¿è¡Œè¾¹ç•Œæ¡†å¤„ç†ï¼ˆæ”¯æŒå¹¶è¡Œå’Œé¡ºåºæ¨¡å¼ï¼‰
    
    Args:
        input_path: è¾“å…¥æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        batch: å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch: æ’å…¥æ‰¹æ¬¡å¤§å°  
        work_dir: å·¥ä½œç›®å½•
        create_unified_view_flag: æ˜¯å¦åˆ›å»ºç»Ÿä¸€è§†å›¾
        maintain_view_only: æ˜¯å¦åªç»´æŠ¤è§†å›¾ï¼ˆä¸å¤„ç†æ•°æ®ï¼‰
        use_parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼
        max_workers: æœ€å¤§å¹¶è¡Œworkeræ•°é‡ï¼ŒNoneä¸ºè‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
    """
    if use_parallel:
        # ä½¿ç”¨å¹¶è¡Œæ¨¡å¼
        return run_with_partitioning_parallel(
            input_path, batch, insert_batch, work_dir, 
            create_unified_view_flag, maintain_view_only, max_workers
        )
    else:
        # ä½¿ç”¨é¡ºåºæ¨¡å¼ï¼ˆåŸå§‹å®ç°ï¼‰
        return run_with_partitioning_sequential(
            input_path, batch, insert_batch, work_dir, 
            create_unified_view_flag, maintain_view_only
        )

def run_with_partitioning_sequential(input_path, batch=1000, insert_batch=1000, work_dir="./bbox_import_logs", 
                                   create_unified_view_flag=True, maintain_view_only=False):
    """ä½¿ç”¨é¡ºåºåˆ†è¡¨æ¨¡å¼è¿è¡Œè¾¹ç•Œæ¡†å¤„ç†ï¼ˆåŸå§‹å®ç°ï¼‰
    
    Args:
        input_path: è¾“å…¥æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        batch: å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch: æ’å…¥æ‰¹æ¬¡å¤§å°  
        work_dir: å·¥ä½œç›®å½•
        create_unified_view_flag: æ˜¯å¦åˆ›å»ºç»Ÿä¸€è§†å›¾
        maintain_view_only: æ˜¯å¦åªç»´æŠ¤è§†å›¾ï¼ˆä¸å¤„ç†æ•°æ®ï¼‰
    """
    global interrupted
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()
    
    print(f"=== åˆ†è¡¨æ¨¡å¼å¤„ç†å¼€å§‹ ===")
    print(f"è¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"å·¥ä½œç›®å½•: {work_dir}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch}")
    print(f"æ’å…¥æ‰¹æ¬¡å¤§å°: {insert_batch}")
    print(f"åˆ›å»ºç»Ÿä¸€è§†å›¾: {create_unified_view_flag}")
    print(f"ä»…ç»´æŠ¤è§†å›¾: {maintain_view_only}")
    
    eng = create_engine(LOCAL_DSN, future=True)
    
    # å¦‚æœåªæ˜¯ç»´æŠ¤è§†å›¾
    if maintain_view_only:
        print("\n=== ç»´æŠ¤ç»Ÿä¸€è§†å›¾æ¨¡å¼ ===")
        success = maintain_unified_view(eng)
        if success:
            print("âœ… ç»Ÿä¸€è§†å›¾ç»´æŠ¤å®Œæˆ")
        else:
            print("âŒ ç»Ÿä¸€è§†å›¾ç»´æŠ¤å¤±è´¥")
        return
    
    try:
        # æ­¥éª¤1: æŒ‰å­æ•°æ®é›†åˆ†ç»„åœºæ™¯
        print("\n=== æ­¥éª¤1: åˆ†ç»„åœºæ™¯æ•°æ® ===")
        scene_groups = group_scenes_by_subdataset(input_path)
        
        if not scene_groups:
            print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åœºæ™¯åˆ†ç»„æ•°æ®")
            return
        
        # æ­¥éª¤2: æ‰¹é‡åˆ›å»ºåˆ†è¡¨
        print("\n=== æ­¥éª¤2: åˆ›å»ºåˆ†è¡¨ ===")
        table_mapping = batch_create_tables_for_subdatasets(eng, scene_groups)
        
        # æ­¥éª¤3: åˆ†åˆ«å¤„ç†æ¯ä¸ªå­æ•°æ®é›†
        print("\n=== æ­¥éª¤3: åˆ†è¡¨æ•°æ®å¤„ç† ===")
        total_processed = 0
        total_inserted = 0
        
        for i, (subdataset_name, subdataset_info) in enumerate(scene_groups.items(), 1):
            scene_ids = subdataset_info['scene_ids']
            metadata = subdataset_info.get('metadata', {})
            if interrupted:
                print(f"\nç¨‹åºè¢«ä¸­æ–­ï¼Œå·²å¤„ç† {i-1}/{len(scene_groups)} ä¸ªå­æ•°æ®é›†")
                break
                
            table_name = table_mapping[subdataset_name]
            print(f"\n[{i}/{len(scene_groups)}] å¤„ç†å­æ•°æ®é›†: {subdataset_name}")
            print(f"  - ç›®æ ‡è¡¨: {table_name}")
            print(f"  - åœºæ™¯æ•°: {len(scene_ids)}")
            
            # ä¸ºæ¯ä¸ªå­æ•°æ®é›†åˆ›å»ºç‹¬ç«‹çš„è¿›åº¦è·Ÿè¸ªå™¨
            sub_work_dir = f"{work_dir}/{subdataset_name}"
            sub_tracker = LightweightProgressTracker(sub_work_dir)
            
            try:
                # è·å–éœ€è¦å¤„ç†çš„åœºæ™¯ID
                remaining_scene_ids = sub_tracker.get_remaining_tokens(scene_ids)
                
                if not remaining_scene_ids:
                    print(f"  - å­æ•°æ®é›† {subdataset_name} æ‰€æœ‰åœºæ™¯å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡")
                    continue
                
                print(f"  - éœ€è¦å¤„ç†: {len(remaining_scene_ids)} ä¸ªåœºæ™¯")
                
                # å¤„ç†å½“å‰å­æ•°æ®é›†çš„æ•°æ®
                sub_processed, sub_inserted = process_subdataset_scenes(
                    eng, remaining_scene_ids, table_name, batch, insert_batch, sub_tracker, metadata
                )
                
                total_processed += sub_processed
                total_inserted += sub_inserted
                
                print(f"  - å®Œæˆ: å¤„ç† {sub_processed} ä¸ªï¼Œæ’å…¥ {sub_inserted} æ¡è®°å½•")
                
            except Exception as e:
                print(f"  - å¤„ç†å­æ•°æ®é›† {subdataset_name} å¤±è´¥: {str(e)}")
                continue
        
        # æ­¥éª¤4: åˆ›å»ºç»Ÿä¸€è§†å›¾ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if create_unified_view_flag and not interrupted:
            print("\n=== æ­¥éª¤4: åˆ›å»ºç»Ÿä¸€è§†å›¾ ===")
            success = create_unified_view(eng)
            if success:
                print("âœ… ç»Ÿä¸€è§†å›¾åˆ›å»ºå®Œæˆ")
            else:
                print("âŒ ç»Ÿä¸€è§†å›¾åˆ›å»ºå¤±è´¥")
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print(f"\n=== åˆ†è¡¨å¤„ç†å®Œæˆ ===")
        print(f"æ€»è®¡å¤„ç†: {total_processed} æ¡è®°å½•")
        print(f"æ€»è®¡æ’å…¥: {total_inserted} æ¡è®°å½•")
        print(f"å¤„ç†å­æ•°æ®é›†: {len(scene_groups)} ä¸ª")
        
        if interrupted:
            print("âš ï¸  å¤„ç†è¢«ä¸­æ–­ï¼Œéƒ¨åˆ†æ•°æ®å¯èƒ½æœªå®Œæˆ")
        else:
            print("âœ… åˆ†è¡¨å¤„ç†å…¨éƒ¨æˆåŠŸå®Œæˆ")
            
    except KeyboardInterrupt:
        print(f"\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        interrupted = True
    except Exception as e:
        print(f"\nåˆ†è¡¨å¤„ç†é‡åˆ°é”™è¯¯: {str(e)}")
    finally:
        print(f"\næ—¥å¿—å’Œè¿›åº¦æ–‡ä»¶ä¿å­˜åœ¨: {work_dir}")

def process_subdataset_scenes(eng, scene_ids, table_name, batch_size, insert_batch_size, tracker, metadata=None):
    """å¤„ç†å•ä¸ªå­æ•°æ®é›†çš„åœºæ™¯æ•°æ®
    
    Args:
        eng: æ•°æ®åº“å¼•æ“
        scene_ids: åœºæ™¯IDåˆ—è¡¨
        table_name: ç›®æ ‡è¡¨å
        batch_size: å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch_size: æ’å…¥æ‰¹æ¬¡å¤§å°
        tracker: è¿›åº¦è·Ÿè¸ªå™¨
        metadata: å­æ•°æ®é›†å…ƒæ•°æ®ï¼Œç”¨äºæ·»åŠ é¢å¤–å­—æ®µ
        
    Returns:
        (processed_count, inserted_count) å…ƒç»„
    """
    processed_count = 0
    inserted_count = 0
    
    try:
        for batch_num, token_batch in enumerate(chunk(scene_ids, batch_size), 1):
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                print(f"    æ‰¹æ¬¡å¤„ç†è¢«ä¸­æ–­ï¼Œå·²å¤„ç† {batch_num-1} ä¸ªæ‰¹æ¬¡")
                break
            
            print(f"    [æ‰¹æ¬¡ {batch_num}] å¤„ç† {len(token_batch)} ä¸ªåœºæ™¯")
            
            # è¿‡æ»¤å·²å¤„ç†çš„è®°å½•
            existing_in_progress = tracker.check_tokens_exist(token_batch)
            token_batch = [token for token in token_batch if token not in existing_in_progress]
            
            if not token_batch:
                print(f"    [æ‰¹æ¬¡ {batch_num}] æ‰€æœ‰æ•°æ®å·²å¤„ç†ï¼Œè·³è¿‡")
                continue
            
            if existing_in_progress:
                print(f"    [æ‰¹æ¬¡ {batch_num}] è·³è¿‡ {len(existing_in_progress)} ä¸ªå·²å¤„ç†çš„è®°å½•")
            
            # è·å–å…ƒæ•°æ®
            try:
                meta = fetch_meta(token_batch)
                if meta.empty:
                    print(f"    [æ‰¹æ¬¡ {batch_num}] æ²¡æœ‰æ‰¾åˆ°å…ƒæ•°æ®ï¼Œè·³è¿‡")
                    for token in token_batch:
                        tracker.save_failed_record(token, "æ— æ³•è·å–å…ƒæ•°æ®", batch_num, "fetch_meta")
                    continue
                
                print(f"    [æ‰¹æ¬¡ {batch_num}] è·å–åˆ° {len(meta)} æ¡å…ƒæ•°æ®")
                
            except Exception as e:
                print(f"    [æ‰¹æ¬¡ {batch_num}] è·å–å…ƒæ•°æ®å¤±è´¥: {str(e)}")
                for token in token_batch:
                    tracker.save_failed_record(token, f"è·å–å…ƒæ•°æ®å¼‚å¸¸: {str(e)}", batch_num, "fetch_meta")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                break
            
            # è·å–è¾¹ç•Œæ¡†å’Œå‡ ä½•å¯¹è±¡
            try:
                bbox_gdf = fetch_bbox_with_geometry(meta.data_name.tolist(), eng)
                if bbox_gdf.empty:
                    print(f"    [æ‰¹æ¬¡ {batch_num}] æ²¡æœ‰æ‰¾åˆ°è¾¹ç•Œæ¡†æ•°æ®ï¼Œè·³è¿‡")
                    for token in meta.scene_token:
                        tracker.save_failed_record(token, "æ— æ³•è·å–è¾¹ç•Œæ¡†æ•°æ®", batch_num, "fetch_bbox")
                    continue
                
                print(f"    [æ‰¹æ¬¡ {batch_num}] è·å–åˆ° {len(bbox_gdf)} æ¡è¾¹ç•Œæ¡†æ•°æ®")
                
            except Exception as e:
                print(f"    [æ‰¹æ¬¡ {batch_num}] è·å–è¾¹ç•Œæ¡†å¤±è´¥: {str(e)}")
                for token in meta.scene_token:
                    tracker.save_failed_record(token, f"è·å–è¾¹ç•Œæ¡†å¼‚å¸¸: {str(e)}", batch_num, "fetch_bbox")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                break
            
                        # åˆå¹¶æ•°æ®
            try:
                merged = meta.merge(bbox_gdf, left_on='data_name', right_on='dataset_name', how='inner')
                if merged.empty:
                    print(f"    [æ‰¹æ¬¡ {batch_num}] åˆå¹¶åæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                    for token in meta.scene_token:
                        tracker.save_failed_record(token, "å…ƒæ•°æ®ä¸è¾¹ç•Œæ¡†æ•°æ®æ— æ³•åŒ¹é…", batch_num, "data_merge")
                    continue
                    
                print(f"    [æ‰¹æ¬¡ {batch_num}] åˆå¹¶åå¾—åˆ° {len(merged)} æ¡è®°å½•")
                
                # åˆ›å»ºåŸºç¡€å­—æ®µçš„æ•°æ®
                base_columns = ['scene_token', 'data_name', 'event_id', 'city_id', 'timestamp', 'all_good']
                final_data = merged[base_columns].copy()
                
                # æ·»åŠ é¢å¤–å­—æ®µï¼ˆå¦‚æœæœ‰metadataï¼‰
                if metadata:
                    data_type = metadata.get('data_type', 'standard')
                    final_data['data_type'] = data_type
                    
                    if data_type == 'defect':
                        # è·å–scene_attributes
                        scene_attributes = metadata.get('scene_attributes', {})
                        
                        # ä¸ºæ¯ä¸ªåœºæ™¯æ·»åŠ ç‰¹å®šå±æ€§
                        for idx, scene_token in enumerate(final_data['scene_token']):
                            scene_attrs = scene_attributes.get(scene_token, {})
                            
                            # æ·»åŠ åŸºç¡€é—®é¢˜å•å­—æ®µï¼ˆå¸¦ç±»å‹è½¬æ¢ï¼‰
                            final_data.loc[idx, 'original_url'] = str(scene_attrs.get('original_url', ''))
                            
                            # line_numberéœ€è¦ç¡®ä¿æ˜¯æ•´æ•°ç±»å‹
                            line_number = scene_attrs.get('line_number', 0)
                            try:
                                if isinstance(line_number, str):
                                    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢ä¸ºæ•´æ•°
                                    line_number = int(float(line_number))  # å…ˆè½¬floatå†è½¬intï¼Œå¤„ç†"15.0"è¿™ç§æƒ…å†µ
                                elif isinstance(line_number, float):
                                    # å¦‚æœæ˜¯æµ®ç‚¹æ•°ï¼Œè½¬æ¢ä¸ºæ•´æ•°
                                    line_number = int(line_number)
                                final_data.loc[idx, 'line_number'] = line_number
                            except (ValueError, TypeError) as e:
                                print(f"è­¦å‘Š: line_numberè½¬æ¢å¤±è´¥ {line_number}: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
                                final_data.loc[idx, 'line_number'] = 0
                            
                            # æ·»åŠ å…¶ä»–è‡ªå®šä¹‰å­—æ®µï¼ˆå¸¦ç±»å‹è½¬æ¢ï¼‰
                            system_fields = {'data_type', 'original_url', 'data_name', 'line_number'}
                            for key, value in scene_attrs.items():
                                if key not in system_fields and not key.startswith('data_'):
                                    # æ ¹æ®å­—æ®µåæ¨æ–­é¢„æœŸç±»å‹å¹¶è½¬æ¢
                                    converted_value = convert_value_to_expected_type(key, value)
                                    final_data.loc[idx, key] = converted_value
                                    
                        print(f"    [æ‰¹æ¬¡ {batch_num}] æ·»åŠ äº†é—®é¢˜å•ç‰¹å®šå­—æ®µï¼ŒåŒ…å« {len(scene_attributes)} ä¸ªåœºæ™¯çš„å±æ€§")
                else:
                    # å‘åå…¼å®¹ï¼šæ·»åŠ é»˜è®¤data_type
                    final_data['data_type'] = 'standard'
                
                # åˆ›å»ºæœ€ç»ˆçš„GeoDataFrame
                final_gdf = gpd.GeoDataFrame(
                    final_data, 
                    geometry=merged['geometry'], 
                    crs=4326
                )
                
            except Exception as e:
                print(f"    [æ‰¹æ¬¡ {batch_num}] æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
                for token in meta.scene_token:
                    tracker.save_failed_record(token, f"æ•°æ®åˆå¹¶å¼‚å¸¸: {str(e)}", batch_num, "data_merge")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                break
            
            # æ‰¹é‡æ’å…¥åˆ°æŒ‡å®šè¡¨
            try:
                batch_inserted = batch_insert_to_postgis(
                    final_gdf, eng, 
                    table_name=table_name,  # ä½¿ç”¨æŒ‡å®šçš„åˆ†è¡¨åç§°
                    batch_size=insert_batch_size, 
                    tracker=tracker, 
                    batch_num=batch_num
                )
                inserted_count += batch_inserted
                processed_count += len(final_gdf)
                
                print(f"    [æ‰¹æ¬¡ {batch_num}] å®Œæˆï¼Œæ’å…¥ {batch_inserted} æ¡è®°å½•åˆ° {table_name}")
                
            except Exception as e:
                print(f"    [æ‰¹æ¬¡ {batch_num}] æ’å…¥æ•°æ®åº“å¤±è´¥: {str(e)}")
                for token in final_gdf.scene_token:
                    tracker.save_failed_record(token, f"æ‰¹é‡æ’å…¥å¼‚å¸¸: {str(e)}", batch_num, "batch_insert")
                continue
        
        return processed_count, inserted_count
        
    except Exception as e:
        print(f"    å¤„ç†å­æ•°æ®é›†åœºæ™¯å¤±è´¥: {str(e)}")
        return processed_count, inserted_count
    finally:
        # ä¿å­˜è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯
        tracker.finalize()

def run(input_path, batch=1000, insert_batch=1000, create_table=False, retry_failed=False, work_dir="./bbox_import_logs", show_stats=False):
    """ä¸»è¿è¡Œå‡½æ•°
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        batch: å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch: æ’å…¥æ‰¹æ¬¡å¤§å°
        create_table: æ˜¯å¦åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        retry_failed: æ˜¯å¦åªé‡è¯•å¤±è´¥çš„æ•°æ®
        work_dir: å·¥ä½œç›®å½•ï¼Œç”¨äºå­˜å‚¨æ—¥å¿—å’Œè¿›åº¦æ–‡ä»¶
        show_stats: æ˜¯å¦æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯å¹¶é€€å‡º
    """
    global interrupted
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()
    
    # æ£€æŸ¥Parquetæ”¯æŒ
    if not PARQUET_AVAILABLE:
        print("è­¦å‘Š: æœªå®‰è£…pyarrowï¼Œå°†ä½¿ç”¨é™çº§çš„æ–‡æœ¬æ–‡ä»¶æ¨¡å¼")
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªå™¨
    tracker = LightweightProgressTracker(work_dir)
    
    # å¦‚æœåªæ˜¯æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    if show_stats:
        stats = tracker.get_statistics()
        print("\n=== å¤„ç†ç»Ÿè®¡ä¿¡æ¯ ===")
        print(f"æˆåŠŸå¤„ç†: {stats['success_count']} ä¸ªåœºæ™¯")
        print(f"å¤±è´¥åœºæ™¯: {stats['failed_count']} ä¸ª")
        
        if stats['failed_by_step']:
            print("\næŒ‰æ­¥éª¤åˆ†ç±»çš„å¤±è´¥ç»Ÿè®¡:")
            for step, count in stats['failed_by_step'].items():
                print(f"  {step}: {count} ä¸ª")
        
        return
    
    print(f"å¼€å§‹å¤„ç†è¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"å·¥ä½œç›®å½•: {work_dir}")
    
    # æ™ºèƒ½åŠ è½½scene_idåˆ—è¡¨
    try:
        if retry_failed:
            scene_ids = tracker.load_failed_tokens()
            print(f"é‡è¯•æ¨¡å¼ï¼šåŠ è½½äº† {len(scene_ids)} ä¸ªå¤±è´¥çš„scene_id")
        else:
            all_scene_ids = load_scene_ids(input_path)
            scene_ids = tracker.get_remaining_tokens(all_scene_ids)
    except Exception as e:
        print(f"åŠ è½½è¾“å…¥æ–‡ä»¶å¤±è´¥: {str(e)}")
        return
    
    if not scene_ids:
        print("æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„scene_id")
        return
    
    eng = create_engine(LOCAL_DSN, future=True)
    
    # åˆ›å»ºè¡¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if create_table:
        if not create_table_if_not_exists(eng):
            print("åˆ›å»ºè¡¨å¤±è´¥ï¼Œé€€å‡º")
            return
    
    total_processed = 0
    total_inserted = 0
    
    print(f"å¼€å§‹å¤„ç† {len(scene_ids)} ä¸ªåœºæ™¯ï¼Œæ‰¹æ¬¡å¤§å°: {batch}")
    print("ä½¿ç”¨åŸå§‹è¾¹ç•Œæ¡†æ•°æ®ï¼Œä¸è¿›è¡Œç¼“å†²åŒºæ‰©å±•")
    
    try:
        for batch_num, token_batch in enumerate(chunk(scene_ids, batch), 1):
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                print(f"\nç¨‹åºè¢«ä¸­æ–­ï¼Œå·²å¤„ç† {batch_num-1} ä¸ªæ‰¹æ¬¡")
                break
                
            print(f"[æ‰¹æ¬¡ {batch_num}] å¤„ç† {len(token_batch)} ä¸ªåœºæ™¯")
            
            # åªæ£€æŸ¥è¿›åº¦è·Ÿè¸ªå™¨ä¸­çš„è®°å½•ï¼ˆå†…å­˜ä¸­çš„æˆåŠŸç¼“å­˜ï¼‰
            existing_in_progress = tracker.check_tokens_exist(token_batch)
            token_batch = [token for token in token_batch if token not in existing_in_progress]
            
            if not token_batch:
                print(f"[æ‰¹æ¬¡ {batch_num}] æ‰€æœ‰æ•°æ®å·²åœ¨è¿›åº¦ä¸­æ ‡è®°ä¸ºå¤„ç†è¿‡ï¼Œè·³è¿‡")
                continue
            
            if existing_in_progress:
                print(f"[æ‰¹æ¬¡ {batch_num}] è·³è¿‡ {len(existing_in_progress)} ä¸ªå·²å¤„ç†çš„è®°å½•")
            
            # è·å–å…ƒæ•°æ®
            try:
                meta = fetch_meta(token_batch)
                if meta.empty:
                    print(f"[æ‰¹æ¬¡ {batch_num}] æ²¡æœ‰æ‰¾åˆ°å…ƒæ•°æ®ï¼Œè·³è¿‡")
                    # è®°å½•è·å–å…ƒæ•°æ®å¤±è´¥çš„tokens
                    for token in token_batch:
                        tracker.save_failed_record(token, "æ— æ³•è·å–å…ƒæ•°æ®", batch_num, "fetch_meta")
                    continue
                    
                print(f"[æ‰¹æ¬¡ {batch_num}] è·å–åˆ° {len(meta)} æ¡å…ƒæ•°æ®")
                
            except Exception as e:
                print(f"[æ‰¹æ¬¡ {batch_num}] è·å–å…ƒæ•°æ®å¤±è´¥: {str(e)}")
                for token in token_batch:
                    tracker.save_failed_record(token, f"è·å–å…ƒæ•°æ®å¼‚å¸¸: {str(e)}", batch_num, "fetch_meta")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                print(f"\nç¨‹åºè¢«ä¸­æ–­ï¼Œæ­£åœ¨å¤„ç†æ‰¹æ¬¡ {batch_num}")
                break
            
            # è·å–è¾¹ç•Œæ¡†å’Œå‡ ä½•å¯¹è±¡ï¼ˆç›´æ¥ä»PostGISè·å–åŸå§‹å‡ ä½•ï¼‰
            try:
                bbox_gdf = fetch_bbox_with_geometry(meta.data_name.tolist(), eng)
                if bbox_gdf.empty:
                    print(f"[æ‰¹æ¬¡ {batch_num}] æ²¡æœ‰æ‰¾åˆ°è¾¹ç•Œæ¡†æ•°æ®ï¼Œè·³è¿‡")
                    # è®°å½•è·å–è¾¹ç•Œæ¡†å¤±è´¥çš„tokens
                    for token in meta.scene_token:
                        tracker.save_failed_record(token, "æ— æ³•è·å–è¾¹ç•Œæ¡†æ•°æ®", batch_num, "fetch_bbox")
                    continue
                    
                print(f"[æ‰¹æ¬¡ {batch_num}] è·å–åˆ° {len(bbox_gdf)} æ¡è¾¹ç•Œæ¡†æ•°æ®")
                
            except Exception as e:
                print(f"[æ‰¹æ¬¡ {batch_num}] è·å–è¾¹ç•Œæ¡†å¤±è´¥: {str(e)}")
                for token in meta.scene_token:
                    tracker.save_failed_record(token, f"è·å–è¾¹ç•Œæ¡†å¼‚å¸¸: {str(e)}", batch_num, "fetch_bbox")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                print(f"\nç¨‹åºè¢«ä¸­æ–­ï¼Œæ­£åœ¨å¤„ç†æ‰¹æ¬¡ {batch_num}")
                break
            
            # åˆå¹¶æ•°æ®
            try:
                merged = meta.merge(bbox_gdf, left_on='data_name', right_on='dataset_name', how='inner')
                if merged.empty:
                    print(f"[æ‰¹æ¬¡ {batch_num}] åˆå¹¶åæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                    # è®°å½•åˆå¹¶å¤±è´¥çš„tokens
                    for token in meta.scene_token:
                        tracker.save_failed_record(token, "å…ƒæ•°æ®ä¸è¾¹ç•Œæ¡†æ•°æ®æ— æ³•åŒ¹é…", batch_num, "data_merge")
                    continue
                    
                print(f"[æ‰¹æ¬¡ {batch_num}] åˆå¹¶åå¾—åˆ° {len(merged)} æ¡è®°å½•")
                
                # åˆ›å»ºæœ€ç»ˆçš„GeoDataFrame
                final_gdf = gpd.GeoDataFrame(
                    merged[['scene_token', 'data_name', 'event_id', 'city_id', 'timestamp', 'all_good']], 
                    geometry=merged['geometry'], 
                    crs=4326
                )
                
            except Exception as e:
                print(f"[æ‰¹æ¬¡ {batch_num}] æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
                for token in meta.scene_token:
                    tracker.save_failed_record(token, f"æ•°æ®åˆå¹¶å¼‚å¸¸: {str(e)}", batch_num, "data_merge")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                print(f"\nç¨‹åºè¢«ä¸­æ–­ï¼Œæ­£åœ¨å¤„ç†æ‰¹æ¬¡ {batch_num}")
                break
            
            # æ‰¹é‡æ’å…¥æ•°æ®åº“
            try:
                batch_inserted = batch_insert_to_postgis(
                    final_gdf, eng, 
                    batch_size=insert_batch, 
                    tracker=tracker, 
                    batch_num=batch_num
                )
                total_inserted += batch_inserted
                total_processed += len(final_gdf)
                
                print(f"[æ‰¹æ¬¡ {batch_num}] å®Œæˆï¼Œæ’å…¥ {batch_inserted} æ¡è®°å½•")
                print(f"[ç´¯è®¡è¿›åº¦] å·²å¤„ç†: {total_processed}, å·²æ’å…¥: {total_inserted}")
                
                # æ¯10ä¸ªæ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡è¿›åº¦
                if batch_num % 10 == 0:
                    tracker.save_progress(len(scene_ids), total_processed, total_inserted, batch_num)
                
            except Exception as e:
                print(f"[æ‰¹æ¬¡ {batch_num}] æ’å…¥æ•°æ®åº“å¤±è´¥: {str(e)}")
                for token in final_gdf.scene_token:
                    tracker.save_failed_record(token, f"æ‰¹é‡æ’å…¥å¼‚å¸¸: {str(e)}", batch_num, "batch_insert")
                continue
                
    except KeyboardInterrupt:
        print(f"\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        interrupted = True
    except Exception as e:
        print(f"\nç¨‹åºé‡åˆ°æœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
    finally:
        # æœ€ç»ˆä¿å­˜è¿›åº¦å’Œåˆ·æ–°ç¼“å†²åŒº
        tracker.finalize()
        tracker.save_progress(len(scene_ids), total_processed, total_inserted, batch_num if 'batch_num' in locals() else 0)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        stats = tracker.get_statistics()
        
        if interrupted:
            print(f"ç¨‹åºä¼˜é›…é€€å‡ºï¼å·²å¤„ç†: {total_processed} æ¡è®°å½•ï¼ŒæˆåŠŸæ’å…¥: {total_inserted} æ¡è®°å½•")
        else:
            print(f"å¤„ç†å®Œæˆï¼æ€»è®¡å¤„ç†: {total_processed} æ¡è®°å½•ï¼ŒæˆåŠŸæ’å…¥: {total_inserted} æ¡è®°å½•")
        
        print(f"\n=== æœ€ç»ˆç»Ÿè®¡ ===")
        print(f"æˆåŠŸå¤„ç†: {stats['success_count']} ä¸ªåœºæ™¯")
        print(f"å¤±è´¥åœºæ™¯: {stats['failed_count']} ä¸ª")
        
        print(f"\nçŠ¶æ€æ–‡ä»¶ä½ç½®:")
        print(f"- æˆåŠŸè®°å½•: {tracker.success_file}")
        print(f"- å¤±è´¥è®°å½•: {tracker.failed_file}")
        print(f"- è¿›åº¦æ–‡ä»¶: {tracker.progress_file}")

if __name__ == '__main__':
    ap = argparse.ArgumentParser(description='ä»æ•°æ®é›†æ–‡ä»¶ç”Ÿæˆè¾¹ç•Œæ¡†æ•°æ®')
    ap.add_argument('--input', required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒJSON/Parquet/æ–‡æœ¬æ ¼å¼ï¼‰')
    ap.add_argument('--batch', type=int, default=1000, help='å¤„ç†æ‰¹æ¬¡å¤§å°')
    ap.add_argument('--insert-batch', type=int, default=1000, help='æ’å…¥æ‰¹æ¬¡å¤§å°')
    ap.add_argument('--create-table', action='store_true', help='åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ã€‚é»˜è®¤å‡è®¾è¡¨å·²é€šè¿‡SQLè„šæœ¬åˆ›å»º')
    ap.add_argument('--retry-failed', action='store_true', help='æ˜¯å¦åªé‡è¯•å¤±è´¥çš„æ•°æ®')
    ap.add_argument('--work-dir', default='./bbox_import_logs', help='å·¥ä½œç›®å½•ï¼Œç”¨äºå­˜å‚¨æ—¥å¿—å’Œè¿›åº¦æ–‡ä»¶')
    ap.add_argument('--show-stats', action='store_true', help='æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡ä¿¡æ¯å¹¶é€€å‡º')
    
    args = ap.parse_args()
    run(args.input, args.batch, args.insert_batch, args.create_table, args.retry_failed, args.work_dir, args.show_stats)