"""è´¨æ£€è½¨è¿¹æŸ¥è¯¢æ¨¡å—

åŸºäºExcelè´¨æ£€ç»“æœæŸ¥è¯¢è½¨è¿¹æ•°æ®ï¼Œæ”¯æŒæ—¶é—´åˆ†æ®µå’Œç»“æœåˆå¹¶åŠŸèƒ½ï¼š
- æ‰¹é‡è¯»å–å¤šä¸ªExcelè´¨æ£€ç»“æœæ–‡ä»¶
- æ™ºèƒ½è¿‡æ»¤æœ‰æ•ˆæ•°æ®ï¼ˆæœ‰resultæˆ–other_scenarioçš„è®°å½•ï¼‰
- æ ¹æ®autoscene_idæŸ¥æ‰¾å¯¹åº”çš„å®Œæ•´è½¨è¿¹
- æ ¹æ®æ—¶é—´åŒºé—´descriptionè¿›è¡Œè½¨è¿¹åˆ†æ®µ
- åˆå¹¶å¤„ç†resultå’Œother_scenarioå­—æ®µ
- ç»Ÿä¸€è¾“å‡ºMultiLineStringå‡ ä½•æ ¼å¼
- é«˜æ€§èƒ½å¹¶è¡Œå¤„ç†å’Œæ‰¹é‡ä¿å­˜

åŠŸèƒ½ç‰¹æ€§ï¼š
1. å¤šExcelæ–‡ä»¶æ‰¹é‡å¤„ç†å’Œæ™ºèƒ½æ•°æ®è¿‡æ»¤
2. ä¸‡çº§æ•°æ®é«˜æ•ˆå¹¶è¡Œå¤„ç†
3. é«˜æ•ˆçš„scene_idåˆ°dataset_nameæ˜ å°„æŸ¥è¯¢
4. æ™ºèƒ½è½¨è¿¹æ—¶é—´åˆ†æ®µç®—æ³•
5. ç»“æœå­—æ®µåˆå¹¶å»é‡å¤„ç†
6. ç»Ÿä¸€MultiLineStringå‡ ä½•æ ¼å¼
7. æ‰¹é‡æ•°æ®åº“å†™å…¥å’Œæ–‡ä»¶å¯¼å‡º
8. å†…å­˜ä¼˜åŒ–å’Œå¹¶è¡ŒæŸ¥è¯¢æ”¯æŒ
"""

import argparse
import json
import logging
import sys
import time
import ast
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Union
from dataclasses import dataclass
import warnings
import concurrent.futures
from itertools import islice
import gc

import pandas as pd
import geopandas as gpd
from shapely.geometry import LineString, MultiLineString, Point
from sqlalchemy import text, create_engine
from spdatalab.common.io_hive import hive_cursor

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)

# æ•°æ®åº“é…ç½®
LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"
POINT_TABLE = "public.ddi_data_points"

# æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)

@dataclass
class QualityCheckRecord:
    """è´¨æ£€è®°å½•æ•°æ®ç»“æ„"""
    task_name: str
    annotator: str
    autoscene_id: str
    result: Union[str, List[str]]
    description: List[List[float]]  # [[start, end], ...]
    other_scenario: Union[str, List[str]]

@dataclass
class SegmentedTrajectory:
    """åˆ†æ®µè½¨è¿¹è¾“å‡ºæ•°æ®ç»“æ„"""
    task_name: str
    annotator: str
    scene_id: str
    dataset_name: str
    segment_count: int
    merged_results: List[str]
    geometry: MultiLineString
    total_duration: float
    start_time: int
    end_time: int
    total_points: int

@dataclass
class QualityCheckConfig:
    """è´¨æ£€è½¨è¿¹æŸ¥è¯¢é…ç½®"""
    local_dsn: str = LOCAL_DSN
    point_table: str = POINT_TABLE
    
    # Excelå¤„ç†é…ç½®
    excel_batch_size: int = 2000
    required_columns: List[str] = None
    filter_invalid_records: bool = True  # è¿‡æ»¤æ— resultå’Œother_scenarioçš„è®°å½•
    
    # è½¨è¿¹åˆ†æ®µé…ç½®
    min_points_per_segment: int = 2
    time_tolerance: float = 0.1  # æ—¶é—´åŒ¹é…å®¹å·®ï¼ˆç§’ï¼‰
    
    # å‡ ä½•é…ç½®
    force_multilinestring: bool = True
    simplify_geometry: bool = False
    simplify_tolerance: float = 0.00001
    
    # æ‰¹é‡å¤„ç†é…ç½®
    scene_id_batch_size: int = 200  # å¢å¤§æ‰¹é‡å¤§å°
    trajectory_batch_size: int = 100  # å¢å¤§æ‰¹é‡å¤§å°
    batch_insert_size: int = 2000  # å¢å¤§æ’å…¥æ‰¹é‡
    
    # æŸ¥è¯¢ä¼˜åŒ–é…ç½®
    query_timeout: int = 300
    cache_scene_mappings: bool = True
    
    # å¹¶è¡Œå¤„ç†é…ç½®
    max_workers: int = 4  # æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
    enable_parallel_trajectory_query: bool = True  # å¯ç”¨å¹¶è¡Œè½¨è¿¹æŸ¥è¯¢
    enable_parallel_processing: bool = True  # å¯ç”¨å¹¶è¡Œè®°å½•å¤„ç†
    memory_optimization: bool = True  # å¯ç”¨å†…å­˜ä¼˜åŒ–
    
    # ä¸‡çº§æ•°æ®å¤„ç†é…ç½®
    large_data_threshold: int = 5000  # å¤§æ•°æ®é˜ˆå€¼
    chunk_processing_size: int = 1000  # åˆ†å—å¤„ç†å¤§å°
    progress_report_interval: int = 500  # è¿›åº¦æŠ¥å‘Šé—´éš”
    
    def __post_init__(self):
        if self.required_columns is None:
            self.required_columns = [
                'task_name', 'annotator', 'autoscene_id', 
                'result', 'description', 'other_scenario'
            ]

class ExcelDataParser:
    """Excelæ•°æ®è§£æå™¨"""
    
    def __init__(self, config: QualityCheckConfig):
        self.config = config
    
    def load_multiple_excel_files(self, file_paths: List[str]) -> List[QualityCheckRecord]:
        """æ‰¹é‡åŠ è½½å¹¶è§£æå¤šä¸ªExcelæ–‡ä»¶
        
        Args:
            file_paths: Excelæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            è´¨æ£€è®°å½•åˆ—è¡¨
        """
        all_records = []
        total_files = len(file_paths)
        
        logger.info(f"ğŸ“– å¼€å§‹æ‰¹é‡åŠ è½½ {total_files} ä¸ªExcelæ–‡ä»¶")
        
        for i, file_path in enumerate(file_paths, 1):
            try:
                logger.info(f"ğŸ“Š å¤„ç†æ–‡ä»¶ {i}/{total_files}: {Path(file_path).name}")
                records = self.load_excel_data(file_path)
                all_records.extend(records)
                
                # å†…å­˜ä¼˜åŒ–ï¼šå®šæœŸæ¸…ç†
                if self.config.memory_optimization and i % 5 == 0:
                    gc.collect()
                    
            except Exception as e:
                logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {str(e)}")
                continue
        
        logger.info(f"âœ… æ‰¹é‡åŠ è½½å®Œæˆ: æ€»è®¡ {len(all_records)} æ¡æœ‰æ•ˆè®°å½•")
        return all_records
    
    def load_excel_data(self, file_path: str) -> List[QualityCheckRecord]:
        """åŠ è½½å¹¶è§£æExcelæ–‡ä»¶
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            
        Returns:
            è´¨æ£€è®°å½•åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ“– åŠ è½½Excelæ–‡ä»¶: {Path(file_path).name}")
            
            # è¯»å–Excelæ–‡ä»¶ï¼ˆæŒ‡å®šç¼–ç ï¼‰
            df = pd.read_excel(file_path, engine='openpyxl')
            logger.info(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
            
            # æ£€æŸ¥å¿…éœ€åˆ—
            missing_cols = [col for col in self.config.required_columns if col not in df.columns]
            if missing_cols:
                raise ValueError(f"Excelæ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
            
            # æ¸…ç†æ•°æ®
            df = self._clean_dataframe(df)
            
            # è¿‡æ»¤æ— æ•ˆè®°å½•ï¼ˆæé«˜æ•ˆç‡ï¼‰
            if self.config.filter_invalid_records:
                df = self._filter_valid_records(df)
                logger.info(f"ğŸ“‹ è¿‡æ»¤åæœ‰æ•ˆæ•°æ®: {len(df)} è¡Œ")
            
            # è½¬æ¢ä¸ºQualityCheckRecordåˆ—è¡¨
            records = []
            for index, row in df.iterrows():
                try:
                    record = self._parse_record(row)
                    if record:
                        records.append(record)
                except Exception as e:
                    logger.debug(f"è§£æç¬¬ {index+1} è¡Œæ•°æ®å¤±è´¥: {str(e)}")
                    continue
            
            logger.info(f"âœ… è§£æå®Œæˆ: {len(records)} æ¡æœ‰æ•ˆè´¨æ£€è®°å½•")
            return records
            
        except Exception as e:
            logger.error(f"åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise
    
    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†DataFrameæ•°æ®"""
        # ç§»é™¤ç©ºè¡Œ
        df = df.dropna(subset=['autoscene_id'])
        
        # æ¸…ç†å­—ç¬¦ä¸²å­—æ®µï¼ˆç¡®ä¿UTF-8ç¼–ç ï¼‰
        string_cols = ['task_name', 'annotator', 'autoscene_id', 'result', 'other_scenario']
        for col in string_cols:
            if col in df.columns:
                # ç¡®ä¿å­—ç¬¦ä¸²ç¼–ç æ­£ç¡®
                df[col] = df[col].astype(str).str.strip()
                # å¤„ç†ç¼–ç é—®é¢˜ï¼šå¦‚æœå­˜åœ¨ç¼–ç é”™è¯¯ï¼Œå°è¯•ä¿®å¤
                df[col] = df[col].apply(self._fix_encoding)
                df[col] = df[col].replace(['nan', 'None', ''], None)
        
        # å¤„ç†descriptionå­—æ®µ
        if 'description' in df.columns:
            df['description'] = df['description'].astype(str).str.strip()
            df['description'] = df['description'].replace(['nan', 'None', ''], None)
        
        logger.debug(f"æ•°æ®æ¸…ç†å: {len(df)} è¡Œæœ‰æ•ˆæ•°æ®")
        return df
    
    def _filter_valid_records(self, df: pd.DataFrame) -> pd.DataFrame:
        """è¿‡æ»¤æœ‰æ•ˆè®°å½•ï¼šè‡³å°‘æœ‰resultæˆ–other_scenarioçš„æ•°æ®ï¼Œä¸”æ’é™¤resultä¸º'good'çš„è®°å½•"""
        # æ£€æŸ¥resultå­—æ®µæ˜¯å¦æœ‰æ•ˆ
        result_valid = df['result'].notna() & (df['result'] != '') & (df['result'] != 'nan')
        
        # æ£€æŸ¥other_scenarioå­—æ®µæ˜¯å¦æœ‰æ•ˆ
        other_scenario_valid = df['other_scenario'].notna() & (df['other_scenario'] != '') & (df['other_scenario'] != 'nan')
        
        # è‡³å°‘æœ‰ä¸€ä¸ªå­—æ®µæœ‰æ•ˆ
        valid_mask = result_valid | other_scenario_valid
        
        # è¿‡æ»¤æ‰resultä¸º'good'çš„è®°å½•
        def is_good_result(value):
            """æ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºgoodï¼ˆå¤„ç†ç¼–ç é—®é¢˜ï¼‰"""
            if pd.isna(value) or value == '' or value == 'nan':
                return False
            
            # å…ˆä¿®å¤ç¼–ç 
            fixed_value = self._fix_encoding(value)
            
            # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„åˆ—è¡¨
            if isinstance(fixed_value, str):
                if fixed_value.lower().strip() == 'good':
                    return True
                # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„åˆ—è¡¨ï¼Œå¦‚ "['good']"
                try:
                    if fixed_value.startswith('[') and fixed_value.endswith(']'):
                        import ast
                        parsed_list = ast.literal_eval(fixed_value)
                        if isinstance(parsed_list, list):
                            # æ£€æŸ¥åˆ—è¡¨ä¸­æ˜¯å¦åªæœ‰'good'
                            cleaned_list = []
                            for item in parsed_list:
                                if str(item).strip():
                                    fixed_item = self._fix_encoding(str(item)).lower().strip()
                                    cleaned_list.append(fixed_item)
                            return len(cleaned_list) == 1 and cleaned_list[0] == 'good'
                except:
                    pass
            
            # å¤„ç†åˆ—è¡¨ç±»å‹
            elif isinstance(fixed_value, list):
                cleaned_list = []
                for item in fixed_value:
                    if str(item).strip():
                        fixed_item = self._fix_encoding(str(item)).lower().strip()
                        cleaned_list.append(fixed_item)
                return len(cleaned_list) == 1 and cleaned_list[0] == 'good'
            
            return False
        
        # è¿‡æ»¤æ‰resultä¸ºgoodçš„è®°å½•
        good_mask = df['result'].apply(is_good_result)
        logger.debug(f"å‘ç° {good_mask.sum()} æ¡resultä¸º'good'çš„è®°å½•ï¼Œå°†è¢«è¿‡æ»¤")
        
        # æœ€ç»ˆè¿‡æ»¤æ¡ä»¶ï¼šæœ‰æ•ˆè®°å½• ä¸” ä¸æ˜¯goodç»“æœ
        final_mask = valid_mask & ~good_mask
        
        filtered_df = df[final_mask].copy()
        
        logger.info(f"æ•°æ®è¿‡æ»¤: {len(df)} -> {len(filtered_df)} è¡Œ")
        logger.info(f"  è¿‡æ»¤æ‰æ— æ•ˆæ•°æ®: {(~valid_mask).sum()} è¡Œ")
        logger.info(f"  è¿‡æ»¤æ‰'good'ç»“æœ: {good_mask.sum()} è¡Œ")
        logger.info(f"  æœ€ç»ˆæœ‰æ•ˆæ•°æ®: {len(filtered_df)} è¡Œ")
        
        return filtered_df
    
    def _fix_encoding(self, text):
        """ä¿®å¤å­—ç¬¦ä¸²ç¼–ç é—®é¢˜"""
        if pd.isna(text) or text in ['nan', 'None', '']:
            return text
        
        try:
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            text_str = str(text)
            
            # å¦‚æœæ˜¯æ­£å¸¸çš„å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
            if all(ord(char) < 128 for char in text_str if char.isprintable()):
                # çº¯ASCIIå­—ç¬¦ï¼Œå¯èƒ½åŒ…å«ä¸­æ–‡çš„ç¼–ç è¡¨ç¤º
                pass
            elif any('\u4e00' <= char <= '\u9fff' for char in text_str):
                # åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œç›´æ¥è¿”å›
                return text_str
            
            # å°è¯•å„ç§ç¼–ç ä¿®å¤
            encodings_to_try = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'latin1']
            
            for encoding in encodings_to_try:
                try:
                    # å¦‚æœæ–‡æœ¬å·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•å…ˆç¼–ç å†è§£ç 
                    if isinstance(text_str, str):
                        # å°è¯•ä¸åŒçš„è§£ç æ–¹å¼
                        fixed_text = text_str.encode('latin1').decode(encoding)
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
                        if any('\u4e00' <= char <= '\u9fff' for char in fixed_text):
                            logger.debug(f"ç¼–ç ä¿®å¤æˆåŠŸ: '{text_str}' -> '{fixed_text}' (ä½¿ç”¨ {encoding})")
                            return fixed_text
                except:
                    continue
            
            # å¦‚æœæ— æ³•ä¿®å¤ï¼Œè¿”å›åŸæ–‡æœ¬
            return text_str
            
        except Exception as e:
            logger.debug(f"ç¼–ç ä¿®å¤å¤±è´¥: {text}, é”™è¯¯: {e}")
            return str(text)
    
    def _parse_record(self, row: pd.Series) -> Optional[QualityCheckRecord]:
        """è§£æå•æ¡è®°å½•"""
        try:
            # åŸºç¡€å­—æ®µ
            task_name = row.get('task_name', '').strip()
            annotator = row.get('annotator', '').strip()
            autoscene_id = row.get('autoscene_id', '').strip()
            
            if not autoscene_id:
                return None
            
            # è§£æresultå­—æ®µ
            result = self._parse_result_field(row.get('result'))
            
            # è§£æother_scenarioå­—æ®µ
            other_scenario = self._parse_result_field(row.get('other_scenario'))
            
            # è§£ædescriptionå­—æ®µ
            description = self._parse_description_field(row.get('description'))
            
            return QualityCheckRecord(
                task_name=task_name,
                annotator=annotator,
                autoscene_id=autoscene_id,
                result=result,
                description=description,
                other_scenario=other_scenario
            )
            
        except Exception as e:
            logger.debug(f"è§£æè®°å½•å¤±è´¥: {str(e)}")
            return None
    
    def _parse_result_field(self, value) -> Union[str, List[str]]:
        """è§£æresultæˆ–other_scenarioå­—æ®µï¼ˆå¤„ç†ç¼–ç é—®é¢˜ï¼‰"""
        if pd.isna(value) or value in ['', 'nan', 'None']:
            return []
        
        # å…ˆä¿®å¤ç¼–ç é—®é¢˜
        value_fixed = self._fix_encoding(value)
        value_str = str(value_fixed).strip()
        
        # å°è¯•è§£æä¸ºåˆ—è¡¨
        if value_str.startswith('[') and value_str.endswith(']'):
            try:
                parsed = ast.literal_eval(value_str)
                if isinstance(parsed, list):
                    # å¯¹åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ ä¹Ÿè¿›è¡Œç¼–ç ä¿®å¤
                    result = []
                    for item in parsed:
                        if str(item).strip():
                            fixed_item = self._fix_encoding(str(item).strip().strip("'\""))
                            result.append(fixed_item)
                    return result
                else:
                    fixed_item = self._fix_encoding(str(parsed).strip().strip("'\""))
                    return [fixed_item]
            except:
                # è§£æå¤±è´¥ï¼Œå½“ä½œå­—ç¬¦ä¸²å¤„ç†
                fixed_item = self._fix_encoding(value_str.strip().strip("'\""))
                return [fixed_item]
        else:
            # å•ä¸ªå­—ç¬¦ä¸²
            fixed_item = self._fix_encoding(value_str.strip().strip("'\""))
            return [fixed_item]
    
    def _parse_description_field(self, value) -> List[List[float]]:
        """è§£ædescriptionæ—¶é—´åŒºé—´å­—æ®µ"""
        logger.debug(f"ğŸ” è§£ædescriptionå­—æ®µ: {value} (ç±»å‹: {type(value)})")
        
        if pd.isna(value) or value in ['', 'nan', 'None']:
            logger.debug("   ç»“æœ: ç©ºå€¼ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return []
        
        value_str = str(value).strip()
        logger.debug(f"   å­—ç¬¦ä¸²åŒ–å: '{value_str}'")
        
        try:
            # å°è¯•è§£æä¸ºåµŒå¥—åˆ—è¡¨
            parsed = ast.literal_eval(value_str)
            logger.debug(f"   astè§£æç»“æœ: {parsed} (ç±»å‹: {type(parsed)})")
            
            if isinstance(parsed, list):
                result = []
                for i, item in enumerate(parsed):
                    logger.debug(f"   å¤„ç†ç¬¬{i+1}ä¸ªåŒºé—´: {item} (ç±»å‹: {type(item)})")
                    
                    if isinstance(item, list) and len(item) == 2:
                        try:
                            start_time = float(item[0])
                            end_time = float(item[1])
                            
                            if start_time < end_time:  # éªŒè¯æ—¶é—´åŒºé—´æœ‰æ•ˆæ€§
                                result.append([start_time, end_time])
                                logger.debug(f"     âœ… æœ‰æ•ˆåŒºé—´: [{start_time}, {end_time}]")
                            else:
                                logger.debug(f"     âŒ æ— æ•ˆåŒºé—´: [{start_time}, {end_time}] (å¼€å§‹>=ç»“æŸ)")
                        except Exception as e:
                            logger.debug(f"     âŒ åŒºé—´è½¬æ¢å¤±è´¥: {e}")
                            continue
                    else:
                        logger.debug(f"     âŒ åŒºé—´æ ¼å¼é”™è¯¯: ä¸æ˜¯é•¿åº¦ä¸º2çš„åˆ—è¡¨")
                
                logger.debug(f"   è§£æç»“æœ: {result} ({len(result)} ä¸ªæœ‰æ•ˆåŒºé—´)")
                return result
            else:
                logger.debug(f"   âŒ è§£æç»“æœä¸æ˜¯åˆ—è¡¨: {type(parsed)}")
        except Exception as e:
            logger.debug(f"   âŒ astè§£æå¤±è´¥: {e}")
        
        logger.debug("   è¿”å›ç©ºåˆ—è¡¨")
        return []

class ResultFieldProcessor:
    """ç»“æœå­—æ®µå¤„ç†å™¨"""
    
    @staticmethod
    def merge_and_clean_results(result: Union[str, List[str]], 
                               other_scenario: Union[str, List[str]]) -> List[str]:
        """åˆå¹¶å¹¶æ¸…ç†ç»“æœå­—æ®µ
        
        Args:
            result: ä¸»è¦ç»“æœå­—æ®µ
            other_scenario: å…¶ä»–åœºæ™¯å­—æ®µ
            
        Returns:
            åˆå¹¶å»é‡åçš„ç»“æœåˆ—è¡¨
        """
        all_results = []
        
        # å¤„ç†resultå­—æ®µ
        if isinstance(result, str):
            if result.strip():
                all_results.append(result.strip())
        elif isinstance(result, list):
            for item in result:
                if isinstance(item, str) and item.strip():
                    all_results.append(item.strip())
        
        # å¤„ç†other_scenarioå­—æ®µ
        if isinstance(other_scenario, str):
            if other_scenario.strip():
                all_results.append(other_scenario.strip())
        elif isinstance(other_scenario, list):
            for item in other_scenario:
                if isinstance(item, str) and item.strip():
                    all_results.append(item.strip())
        
        # å»é‡å¹¶æ’åº
        unique_results = list(set(all_results))
        unique_results.sort()
        
        return unique_results

class SceneIdMapper:
    """åœºæ™¯IDæ˜ å°„å™¨"""
    
    def __init__(self, config: QualityCheckConfig):
        self.config = config
        self._cache = {} if config.cache_scene_mappings else None
    
    def batch_query_scene_mappings(self, autoscene_ids: List[str]) -> Dict[str, Dict]:
        """é«˜æ•ˆæ‰¹é‡æŸ¥è¯¢autoscene_idåˆ°æ•°æ®é›†ä¿¡æ¯çš„æ˜ å°„
        
        Args:
            autoscene_ids: autoscene_idåˆ—è¡¨
            
        Returns:
            æ˜ å°„å­—å…¸ {autoscene_id: {'dataset_name': str, 'event_id': int, 'event_name': str}}
        """
        if not autoscene_ids:
            return {}
        
        # å»é‡
        unique_ids = list(set(autoscene_ids))
        
        # æ£€æŸ¥ç¼“å­˜
        if self._cache:
            uncached_ids = [aid for aid in unique_ids if aid not in self._cache]
            cached_results = {aid: self._cache[aid] for aid in unique_ids if aid in self._cache}
        else:
            uncached_ids = unique_ids
            cached_results = {}
        
        if not uncached_ids:
            logger.info(f"âœ… ä»ç¼“å­˜è·å– {len(cached_results)} ä¸ªsceneæ˜ å°„")
            return cached_results
        
        logger.info(f"ğŸ” æ‰¹é‡æŸ¥è¯¢sceneæ˜ å°„: {len(uncached_ids)} ä¸ªæ–°autoscene_idï¼ˆç¼“å­˜: {len(cached_results)} ä¸ªï¼‰")
        
        # åˆ†æ‰¹æŸ¥è¯¢ï¼ˆé¿å…å•æ¬¡æŸ¥è¯¢è¿‡å¤§ï¼‰
        new_mappings = {}
        batch_size = self.config.scene_id_batch_size
        
        for i in range(0, len(uncached_ids), batch_size):
            batch_ids = uncached_ids[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(uncached_ids) + batch_size - 1) // batch_size
            
            logger.info(f"ğŸ” æŸ¥è¯¢æ‰¹æ¬¡ {batch_num}/{total_batches}: {len(batch_ids)} ä¸ªID")
            
            batch_mappings = self._query_scene_mapping_batch(batch_ids)
            new_mappings.update(batch_mappings)
            
            # å†…å­˜ä¼˜åŒ–ï¼šå®šæœŸæ¸…ç†
            if self.config.memory_optimization and batch_num % 10 == 0:
                gc.collect()
        
        # åˆå¹¶ç»“æœ
        all_mappings = {**cached_results, **new_mappings}
        
        logger.info(f"âœ… æŸ¥è¯¢å®Œæˆ: {len(new_mappings)} ä¸ªæ–°æ˜ å°„, æ€»è®¡ {len(all_mappings)} ä¸ªæ˜ å°„")
        
        return all_mappings
    
    def _query_scene_mapping_batch(self, scene_ids: List[str]) -> Dict[str, Dict]:
        """æŸ¥è¯¢å•æ‰¹sceneæ˜ å°„"""
        try:
            sql = """
                SELECT id AS scene_id,
                       origin_name AS dataset_name,
                       event_id,
                       event_name
                FROM (
                    SELECT id, 
                           origin_name,
                           event_id,
                           event_name,
                           ROW_NUMBER() OVER (PARTITION BY id ORDER BY updated_at DESC) as rn
                    FROM transform.ods_t_data_fragment_datalake 
                    WHERE id IN %(scene_ids)s
                ) ranked
                WHERE rn = 1
            """
            
            mappings = {}
            
            with hive_cursor() as cur:
                cur.execute(sql, {"scene_ids": tuple(scene_ids)})
                cols = [d[0] for d in cur.description]
                results = cur.fetchall()
                
                for row in results:
                    row_dict = dict(zip(cols, row))
                    scene_id = row_dict['scene_id']
                    
                    # å¤„ç†event_idï¼ˆé¿å…æµ®ç‚¹æ•°é—®é¢˜ï¼‰
                    event_id = row_dict.get('event_id')
                    if event_id is not None and event_id != '':
                        try:
                            event_id = int(float(event_id))
                        except:
                            event_id = None
                    else:
                        event_id = None
                    
                    mapping = {
                        'dataset_name': row_dict.get('dataset_name', ''),
                        'event_id': event_id,
                        'event_name': row_dict.get('event_name', '')
                    }
                    
                    mappings[scene_id] = mapping
                    
                    # æ›´æ–°ç¼“å­˜
                    if self._cache is not None:
                        self._cache[scene_id] = mapping
            
            return mappings
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢sceneæ˜ å°„æ‰¹æ¬¡å¤±è´¥: {str(e)}")
            return {}

class TrajectorySegmenter:
    """è½¨è¿¹åˆ†æ®µå™¨"""
    
    def __init__(self, config: QualityCheckConfig):
        self.config = config
    
    def query_complete_trajectory(self, dataset_name: str) -> pd.DataFrame:
        """æŸ¥è¯¢å®Œæ•´è½¨è¿¹æ•°æ®
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            
        Returns:
            è½¨è¿¹ç‚¹DataFrame
        """
        logger.debug(f"ğŸ” å¼€å§‹æŸ¥è¯¢è½¨è¿¹æ•°æ®: {dataset_name}")
        
        try:
            sql = f"""
                SELECT 
                    timestamp,
                    ST_X(point_lla) as longitude,
                    ST_Y(point_lla) as latitude,
                    twist_linear,
                    avp_flag,
                    workstage
                FROM {self.config.point_table}
                WHERE dataset_name = %(dataset_name)s
                AND point_lla IS NOT NULL
                AND timestamp IS NOT NULL
                ORDER BY timestamp
            """
            
            logger.debug(f"ğŸ“Š æ‰§è¡ŒSQLæŸ¥è¯¢: {self.config.point_table}")
            
            with hive_cursor("dataset_gy1") as cur:
                cur.execute(sql, {"dataset_name": dataset_name})
                cols = [d[0] for d in cur.description]
                rows = cur.fetchall()
                
                logger.debug(f"ğŸ“‹ SQLæŸ¥è¯¢ç»“æœ: {len(rows)} è¡Œæ•°æ®")
                
                if rows:
                    df = pd.DataFrame(rows, columns=cols)
                    
                    # æ£€æŸ¥æ•°æ®è´¨é‡
                    null_coords = df[['longitude', 'latitude']].isnull().any(axis=1).sum()
                    null_timestamps = df['timestamp'].isnull().sum()
                    
                    logger.debug(f"âœ… æŸ¥è¯¢æˆåŠŸ {dataset_name}: {len(df)} ä¸ªç‚¹")
                    logger.debug(f"   ç©ºåæ ‡: {null_coords} ä¸ª")
                    logger.debug(f"   ç©ºæ—¶é—´æˆ³: {null_timestamps} ä¸ª")
                    
                    if len(df) > 0:
                        logger.debug(f"   æ—¶é—´èŒƒå›´: {df['timestamp'].min()} - {df['timestamp'].max()}")
                        logger.debug(f"   åæ ‡èŒƒå›´: lon[{df['longitude'].min():.6f}, {df['longitude'].max():.6f}], "
                                   f"lat[{df['latitude'].min():.6f}, {df['latitude'].max():.6f}]")
                    
                    return df
                else:
                    logger.warning(f"âš ï¸ æœªæŸ¥è¯¢åˆ°è½¨è¿¹æ•°æ®: {dataset_name}")
                    logger.debug(f"   SQL: {sql}")
                    logger.debug(f"   å‚æ•°: dataset_name={dataset_name}")
                    return pd.DataFrame()
                    
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢è½¨è¿¹å¤±è´¥ {dataset_name}: {str(e)}")
            logger.error(f"   SQL: {sql}")
            logger.error(f"   è¡¨å: {self.config.point_table}")
            return pd.DataFrame()
    
    def segment_trajectory_by_time_ranges(self, 
                                        trajectory_df: pd.DataFrame, 
                                        time_ranges: List[List[float]]) -> Tuple[MultiLineString, int]:
        """æ ¹æ®æ—¶é—´åŒºé—´åˆ†æ®µè½¨è¿¹
        
        Args:
            trajectory_df: å®Œæ•´è½¨è¿¹ç‚¹DataFrame
            time_ranges: æ—¶é—´åŒºé—´åˆ—è¡¨ [[start, end], ...]
            
        Returns:
            (MultiLineStringå‡ ä½•, æœ‰æ•ˆåˆ†æ®µæ•°é‡)
        """
        logger.debug(f"ğŸ”§ å¼€å§‹è½¨è¿¹åˆ†æ®µ: è½¨è¿¹ç‚¹æ•°={len(trajectory_df)}, æ—¶é—´åŒºé—´æ•°={len(time_ranges)}")
        
        if trajectory_df.empty:
            logger.warning("âš ï¸ è½¨è¿¹DataFrameä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ®µ")
            return MultiLineString([]), 0
        
        # è®¡ç®—ç›¸å¯¹æ—¶é—´
        start_timestamp = trajectory_df['timestamp'].min()
        end_timestamp = trajectory_df['timestamp'].max()
        total_duration = end_timestamp - start_timestamp
        
        logger.debug(f"ğŸ“Š è½¨è¿¹æ—¶é—´èŒƒå›´: {start_timestamp} - {end_timestamp} (æ—¶é•¿: {total_duration}s)")
        
        trajectory_df = trajectory_df.copy()
        trajectory_df['relative_time'] = trajectory_df['timestamp'] - start_timestamp
        
        segments = []
        valid_segments = 0
        skipped_segments = 0
        
        for i, time_range in enumerate(time_ranges):
            start_time, end_time = time_range
            logger.debug(f"ğŸ” å¤„ç†æ—¶é—´åŒºé—´ {i+1}/{len(time_ranges)}: [{start_time}, {end_time}]s")
            
            # æ£€æŸ¥æ—¶é—´åŒºé—´æ˜¯å¦åˆç†
            if start_time >= end_time:
                logger.warning(f"âš ï¸ æ— æ•ˆæ—¶é—´åŒºé—´: [{start_time}, {end_time}] (å¼€å§‹æ—¶é—´ >= ç»“æŸæ—¶é—´)")
                skipped_segments += 1
                continue
            
            if end_time < 0 or start_time > total_duration:
                logger.warning(f"âš ï¸ æ—¶é—´åŒºé—´è¶…å‡ºè½¨è¿¹èŒƒå›´: [{start_time}, {end_time}]s, è½¨è¿¹æ—¶é•¿: {total_duration}s")
                skipped_segments += 1
                continue
            
            # ç­›é€‰æ—¶é—´åŒºé—´å†…çš„ç‚¹
            mask = (
                (trajectory_df['relative_time'] >= start_time - self.config.time_tolerance) &
                (trajectory_df['relative_time'] <= end_time + self.config.time_tolerance)
            )
            
            segment_points = trajectory_df[mask]
            logger.debug(f"ğŸ“ æ—¶é—´åŒºé—´ [{start_time}, {end_time}]s ç­›é€‰åˆ° {len(segment_points)} ä¸ªç‚¹")
            
            if len(segment_points) < self.config.min_points_per_segment:
                logger.warning(f"âš ï¸ åˆ†æ®µç‚¹æ•°ä¸è¶³: {len(segment_points)} < {self.config.min_points_per_segment}")
                skipped_segments += 1
                continue
            
            try:
                coordinates = list(zip(segment_points['longitude'], segment_points['latitude']))
                
                # æ£€æŸ¥åæ ‡æœ‰æ•ˆæ€§
                valid_coords = [(lon, lat) for lon, lat in coordinates if pd.notna(lon) and pd.notna(lat)]
                if len(valid_coords) < self.config.min_points_per_segment:
                    logger.warning(f"âš ï¸ æœ‰æ•ˆåæ ‡ä¸è¶³: {len(valid_coords)} < {self.config.min_points_per_segment}")
                    skipped_segments += 1
                    continue
                
                segment_geom = LineString(valid_coords)
                
                # å¯é€‰çš„å‡ ä½•ç®€åŒ–
                if self.config.simplify_geometry:
                    original_coords = len(segment_geom.coords)
                    segment_geom = segment_geom.simplify(self.config.simplify_tolerance)
                    simplified_coords = len(segment_geom.coords)
                    logger.debug(f"ğŸ”§ å‡ ä½•ç®€åŒ–: {original_coords} -> {simplified_coords} ä¸ªåæ ‡ç‚¹")
                
                segments.append(segment_geom)
                valid_segments += 1
                logger.debug(f"âœ… æˆåŠŸåˆ›å»ºåˆ†æ®µ {valid_segments}: {start_time}-{end_time}s, {len(segment_points)} ä¸ªç‚¹")
                
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºåˆ†æ®µå‡ ä½•å¤±è´¥: {str(e)}")
                skipped_segments += 1
                continue
        
        logger.info(f"ğŸ“Š åˆ†æ®µç»“æœ: æˆåŠŸ={valid_segments}, è·³è¿‡={skipped_segments}, æ€»æ•°={len(time_ranges)}")
        
        if segments:
            try:
                multi_geom = MultiLineString(segments)
                logger.debug(f"âœ… æˆåŠŸåˆ›å»ºMultiLineString: {len(segments)} ä¸ªåˆ†æ®µ")
                return multi_geom, len(segments)
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºMultiLineStringå¤±è´¥: {str(e)}")
                return MultiLineString([]), 0
        else:
            logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æ®µï¼Œè¿”å›ç©ºå‡ ä½•")
            return MultiLineString([]), 0
    
    def create_complete_trajectory(self, trajectory_df: pd.DataFrame) -> Tuple[MultiLineString, float]:
        """åˆ›å»ºå®Œæ•´è½¨è¿¹ï¼ˆæ— åˆ†æ®µæƒ…å†µï¼‰
        
        Args:
            trajectory_df: è½¨è¿¹ç‚¹DataFrame
            
        Returns:
            (MultiLineStringå‡ ä½•, æ€»æ—¶é•¿)
        """
        logger.debug(f"ğŸ”§ å¼€å§‹åˆ›å»ºå®Œæ•´è½¨è¿¹: è½¨è¿¹ç‚¹æ•°={len(trajectory_df)}")
        
        if trajectory_df.empty:
            logger.warning("âš ï¸ è½¨è¿¹DataFrameä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºå®Œæ•´è½¨è¿¹")
            return MultiLineString([]), 0.0
        
        if len(trajectory_df) < self.config.min_points_per_segment:
            logger.warning(f"âš ï¸ è½¨è¿¹ç‚¹æ•°ä¸è¶³: {len(trajectory_df)} < {self.config.min_points_per_segment}")
            return MultiLineString([]), 0.0
        
        try:
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            required_columns = ['longitude', 'latitude', 'timestamp']
            missing_columns = [col for col in required_columns if col not in trajectory_df.columns]
            if missing_columns:
                logger.error(f"âŒ ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_columns}")
                return MultiLineString([]), 0.0
            
            # è¿‡æ»¤æœ‰æ•ˆåæ ‡
            coordinates = list(zip(trajectory_df['longitude'], trajectory_df['latitude']))
            valid_coords = [(lon, lat) for lon, lat in coordinates if pd.notna(lon) and pd.notna(lat)]
            
            logger.debug(f"ğŸ“ æœ‰æ•ˆåæ ‡æ•°é‡: {len(valid_coords)}/{len(coordinates)}")
            
            if len(valid_coords) < self.config.min_points_per_segment:
                logger.warning(f"âš ï¸ æœ‰æ•ˆåæ ‡ä¸è¶³: {len(valid_coords)} < {self.config.min_points_per_segment}")
                return MultiLineString([]), 0.0
            
            # åˆ›å»ºè½¨è¿¹å‡ ä½•
            trajectory_geom = LineString(valid_coords)
            logger.debug(f"âœ… æˆåŠŸåˆ›å»ºLineString: {len(trajectory_geom.coords)} ä¸ªåæ ‡ç‚¹")
            
            # å¯é€‰çš„å‡ ä½•ç®€åŒ–
            if self.config.simplify_geometry:
                original_coords = len(trajectory_geom.coords)
                trajectory_geom = trajectory_geom.simplify(self.config.simplify_tolerance)
                simplified_coords = len(trajectory_geom.coords)
                logger.debug(f"ğŸ”§ å‡ ä½•ç®€åŒ–: {original_coords} -> {simplified_coords} ä¸ªåæ ‡ç‚¹")
            
            # è®¡ç®—æ€»æ—¶é•¿
            min_timestamp = trajectory_df['timestamp'].min()
            max_timestamp = trajectory_df['timestamp'].max()
            duration = float(max_timestamp - min_timestamp)
            
            logger.debug(f"ğŸ“Š è½¨è¿¹æ—¶é•¿: {duration}s ({min_timestamp} - {max_timestamp})")
            
            # è½¬æ¢ä¸ºMultiLineString
            multi_geom = MultiLineString([trajectory_geom])
            logger.debug(f"âœ… æˆåŠŸåˆ›å»ºå®Œæ•´è½¨è¿¹MultiLineString")
            
            return multi_geom, duration
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå®Œæ•´è½¨è¿¹å¤±è´¥: {str(e)}")
            logger.error(f"   è½¨è¿¹æ•°æ®æ ·æœ¬: {trajectory_df.head() if not trajectory_df.empty else 'Empty'}")
            return MultiLineString([]), 0.0

class QualityCheckTrajectoryQuery:
    """è´¨æ£€è½¨è¿¹æŸ¥è¯¢ä¸»ç±»"""
    
    def __init__(self, config: Optional[QualityCheckConfig] = None):
        self.config = config or QualityCheckConfig()
        self.engine = create_engine(
            self.config.local_dsn, 
            future=True,
            connect_args={
                "client_encoding": "utf8",
                "options": "-c timezone=UTC"
            }
        )
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.excel_parser = ExcelDataParser(self.config)
        self.result_processor = ResultFieldProcessor()
        self.scene_mapper = SceneIdMapper(self.config)
        self.trajectory_segmenter = TrajectorySegmenter(self.config)
        
        logger.info("ğŸ”§ è´¨æ£€è½¨è¿¹æŸ¥è¯¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_excel_files(self, 
                           excel_files: Union[str, List[str]],
                           output_table: Optional[str] = None,
                           output_geojson: Optional[str] = None) -> Dict:
        """å¤„ç†Excelè´¨æ£€æ–‡ä»¶å®Œæ•´å·¥ä½œæµï¼ˆæ”¯æŒå¤šæ–‡ä»¶å’Œå¹¶è¡Œå¤„ç†ï¼‰
        
        Args:
            excel_files: Excelæ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_table: è¾“å‡ºæ•°æ®åº“è¡¨åï¼ˆå¯é€‰ï¼‰
            output_geojson: è¾“å‡ºGeoJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        workflow_start = time.time()
        
        # æ ‡å‡†åŒ–æ–‡ä»¶è·¯å¾„
        if isinstance(excel_files, str):
            file_paths = [excel_files]
        else:
            file_paths = excel_files
        
        stats = {
            'start_time': datetime.now(),
            'excel_files': file_paths,
            'file_count': len(file_paths),
            'output_table': output_table,
            'output_geojson': output_geojson,
            'total_records': 0,
            'valid_trajectories': 0,
            'failed_records': 0,
            'processing_mode': 'parallel' if self.config.enable_parallel_processing else 'sequential'
        }
        
        try:
            logger.info("=" * 60)
            logger.info("ğŸš€ å¼€å§‹è´¨æ£€è½¨è¿¹æŸ¥è¯¢å·¥ä½œæµï¼ˆä¸‡çº§æ•°æ®ä¼˜åŒ–ç‰ˆï¼‰")
            logger.info("=" * 60)
            
            # é˜¶æ®µ1: è§£æExcelæ–‡ä»¶
            logger.info(f"ğŸ“– é˜¶æ®µ1: è§£æ {len(file_paths)} ä¸ªExcelæ–‡ä»¶")
            if len(file_paths) == 1:
                records = self.excel_parser.load_excel_data(file_paths[0])
            else:
                records = self.excel_parser.load_multiple_excel_files(file_paths)
            
            stats['total_records'] = len(records)
            
            if not records:
                logger.error("âŒ æœªè§£æåˆ°ä»»ä½•æœ‰æ•ˆè®°å½•")
                stats['error'] = "No valid records parsed"
                return stats
            
            # é˜¶æ®µ2: æ‰¹é‡æŸ¥è¯¢åœºæ™¯æ˜ å°„
            logger.info(f"ğŸ” é˜¶æ®µ2: æŸ¥è¯¢åœºæ™¯æ˜ å°„ä¿¡æ¯")
            autoscene_ids = [record.autoscene_id for record in records]
            scene_mappings = self.scene_mapper.batch_query_scene_mappings(autoscene_ids)
            
            # é˜¶æ®µ3: é«˜æ•ˆæ‰¹é‡å¤„ç†è½¨è¿¹
            logger.info(f"ğŸ”§ é˜¶æ®µ3: é«˜æ•ˆå¤„ç† {len(records)} æ¡è½¨è¿¹æ•°æ®")
            
            if len(records) >= self.config.large_data_threshold:
                logger.info(f"ğŸ“Š å¤§æ•°æ®æ¨¡å¼ï¼šå¯ç”¨åˆ†å—å¹¶è¡Œå¤„ç†ï¼ˆé˜ˆå€¼: {self.config.large_data_threshold}ï¼‰")
                trajectories, failed_count = self._process_large_dataset(records, scene_mappings)
            else:
                logger.info(f"ğŸ“Š æ ‡å‡†æ¨¡å¼ï¼šé¡ºåºå¤„ç†")
                trajectories, failed_count = self._process_standard_dataset(records, scene_mappings)
            
            stats['valid_trajectories'] = len(trajectories)
            stats['failed_records'] = failed_count
            
            if not trajectories:
                logger.warning("âš ï¸ æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆè½¨è¿¹")
                stats['warning'] = "No valid trajectories generated"
                return stats
            
            logger.info(f"âœ… æˆåŠŸå¤„ç† {len(trajectories)} æ¡è½¨è¿¹")
            
            # é˜¶æ®µ4: ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
            if output_table:
                logger.info(f"ğŸ’¾ é˜¶æ®µ4: ä¿å­˜åˆ°æ•°æ®åº“è¡¨: {output_table}")
                inserted_count, save_stats = self._save_trajectories_to_table(trajectories, output_table)
                stats['save_stats'] = save_stats
                logger.info(f"âœ… æˆåŠŸä¿å­˜ {inserted_count} æ¡è½¨è¿¹åˆ°æ•°æ®åº“")
            
            # é˜¶æ®µ5: å¯¼å‡ºåˆ°GeoJSONï¼ˆå¯é€‰ï¼‰
            if output_geojson:
                logger.info(f"ğŸ“„ é˜¶æ®µ5: å¯¼å‡ºåˆ°GeoJSONæ–‡ä»¶: {output_geojson}")
                if self._export_trajectories_to_geojson(trajectories, output_geojson):
                    stats['geojson_exported'] = True
                    logger.info(f"âœ… æˆåŠŸå¯¼å‡ºè½¨è¿¹åˆ°GeoJSONæ–‡ä»¶")
                else:
                    stats['geojson_export_failed'] = True
                    logger.warning("âš ï¸ GeoJSONå¯¼å‡ºå¤±è´¥")
            
            # æœ€ç»ˆç»Ÿè®¡
            stats['workflow_duration'] = time.time() - workflow_start
            stats['end_time'] = datetime.now()
            stats['success'] = True
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ è´¨æ£€è½¨è¿¹æŸ¥è¯¢å·¥ä½œæµå®Œæˆ!")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {stats['workflow_duration']:.2f}s")
            logger.info(f"ğŸ“Š æˆåŠŸå¤„ç†: {stats['valid_trajectories']}/{stats['total_records']} æ¡è®°å½•")
            logger.info("=" * 60)
            
            return stats
            
        except Exception as e:
            stats['error'] = str(e)
            stats['workflow_duration'] = time.time() - workflow_start
            stats['end_time'] = datetime.now()
            stats['success'] = False
            logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
            return stats
    
    def _process_large_dataset(self, 
                              records: List[QualityCheckRecord], 
                              scene_mappings: Dict[str, Dict]) -> Tuple[List[SegmentedTrajectory], int]:
        """å¤§æ•°æ®é›†å¹¶è¡Œå¤„ç†"""
        trajectories = []
        failed_count = 0
        total_records = len(records)
        
        # åˆ†å—å¤„ç†
        chunk_size = self.config.chunk_processing_size
        total_chunks = (total_records + chunk_size - 1) // chunk_size
        
        logger.info(f"ğŸ”„ åˆ†å—å¹¶è¡Œå¤„ç†: {total_chunks} ä¸ªå—ï¼Œæ¯å— {chunk_size} æ¡è®°å½•")
        
        for chunk_idx in range(total_chunks):
            start_idx = chunk_idx * chunk_size
            end_idx = min(start_idx + chunk_size, total_records)
            chunk_records = records[start_idx:end_idx]
            
            logger.info(f"ğŸ”„ å¤„ç†å— {chunk_idx + 1}/{total_chunks}: {len(chunk_records)} æ¡è®°å½•")
            
            if self.config.enable_parallel_processing:
                # å¹¶è¡Œå¤„ç†å½“å‰å—
                chunk_trajectories, chunk_failed = self._process_chunk_parallel(chunk_records, scene_mappings)
            else:
                # é¡ºåºå¤„ç†å½“å‰å—
                chunk_trajectories, chunk_failed = self._process_chunk_sequential(chunk_records, scene_mappings)
            
            trajectories.extend(chunk_trajectories)
            failed_count += chunk_failed
            
            # è¿›åº¦æŠ¥å‘Š
            processed_count = end_idx
            logger.info(f"ğŸ“Š è¿›åº¦: {processed_count}/{total_records} ({processed_count/total_records*100:.1f}%), "
                       f"æˆåŠŸ: {len(chunk_trajectories)}, å¤±è´¥: {chunk_failed}")
            
            # å†…å­˜ä¼˜åŒ–
            if self.config.memory_optimization and chunk_idx % 5 == 0:
                gc.collect()
        
        return trajectories, failed_count
    
    def _process_standard_dataset(self, 
                                 records: List[QualityCheckRecord], 
                                 scene_mappings: Dict[str, Dict]) -> Tuple[List[SegmentedTrajectory], int]:
        """æ ‡å‡†æ•°æ®é›†å¤„ç†"""
        if self.config.enable_parallel_processing and len(records) > 50:
            logger.info("ğŸ”„ å¯ç”¨å¹¶è¡Œå¤„ç†")
            return self._process_chunk_parallel(records, scene_mappings)
        else:
            logger.info("ğŸ”„ ä½¿ç”¨é¡ºåºå¤„ç†")
            return self._process_chunk_sequential(records, scene_mappings)
    
    def _process_chunk_parallel(self, 
                               chunk_records: List[QualityCheckRecord], 
                               scene_mappings: Dict[str, Dict]) -> Tuple[List[SegmentedTrajectory], int]:
        """å¹¶è¡Œå¤„ç†è®°å½•å—"""
        trajectories = []
        failed_count = 0
        
        def process_record_wrapper(record):
            """è®°å½•å¤„ç†åŒ…è£…å‡½æ•°"""
            try:
                return self._process_single_record(record, scene_mappings)
            except Exception as e:
                logger.debug(f"å¤„ç†è®°å½•å¤±è´¥ {record.autoscene_id}: {str(e)}")
                return None
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_record = {
                executor.submit(process_record_wrapper, record): record 
                for record in chunk_records
            }
            
            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_record):
                record = future_to_record[future]
                try:
                    trajectory = future.result()
                    if trajectory:
                        trajectories.append(trajectory)
                    else:
                        failed_count += 1
                except Exception as e:
                    logger.debug(f"å¹¶è¡Œå¤„ç†å¼‚å¸¸ {record.autoscene_id}: {str(e)}")
                    failed_count += 1
        
        return trajectories, failed_count
    
    def _process_chunk_sequential(self, 
                                 chunk_records: List[QualityCheckRecord], 
                                 scene_mappings: Dict[str, Dict]) -> Tuple[List[SegmentedTrajectory], int]:
        """é¡ºåºå¤„ç†è®°å½•å—"""
        trajectories = []
        failed_count = 0
        
        for i, record in enumerate(chunk_records, 1):
            try:
                # è¿›åº¦æŠ¥å‘Š
                if i % self.config.progress_report_interval == 0:
                    logger.info(f"ğŸ“Š å—å†…è¿›åº¦: {i}/{len(chunk_records)} ({i/len(chunk_records)*100:.1f}%)")
                
                trajectory = self._process_single_record(record, scene_mappings)
                if trajectory:
                    trajectories.append(trajectory)
                else:
                    failed_count += 1
                    
            except Exception as e:
                logger.debug(f"å¤„ç†è®°å½•å¤±è´¥ {record.autoscene_id}: {str(e)}")
                failed_count += 1
                continue
        
        return trajectories, failed_count
    
    def _process_single_record(self, 
                             record: QualityCheckRecord, 
                             scene_mappings: Dict[str, Dict]) -> Optional[SegmentedTrajectory]:
        """å¤„ç†å•æ¡è´¨æ£€è®°å½•
        
        Args:
            record: è´¨æ£€è®°å½•
            scene_mappings: åœºæ™¯æ˜ å°„å­—å…¸
            
        Returns:
            åˆ†æ®µè½¨è¿¹å¯¹è±¡æˆ–None
        """
        logger.debug(f"ğŸ”§ å¼€å§‹å¤„ç†è®°å½•: {record.autoscene_id}")
        logger.debug(f"   task_name: {record.task_name}")
        logger.debug(f"   annotator: {record.annotator}")
        logger.debug(f"   result: {record.result}")
        logger.debug(f"   other_scenario: {record.other_scenario}")
        logger.debug(f"   description: {record.description}")
        
        # è·å–åœºæ™¯æ˜ å°„ä¿¡æ¯
        scene_info = scene_mappings.get(record.autoscene_id)
        if not scene_info:
            logger.warning(f"âŒ æœªæ‰¾åˆ°åœºæ™¯æ˜ å°„: {record.autoscene_id}")
            logger.debug(f"   å¯ç”¨çš„åœºæ™¯ID: {list(scene_mappings.keys())[:5]}...")
            return None
        
        dataset_name = scene_info.get('dataset_name')
        if not dataset_name:
            logger.warning(f"âŒ åœºæ™¯æ˜ å°„ç¼ºå°‘dataset_name: {record.autoscene_id}")
            logger.debug(f"   åœºæ™¯ä¿¡æ¯: {scene_info}")
            return None
        
        logger.debug(f"âœ… è·å¾—åœºæ™¯æ˜ å°„: {record.autoscene_id} -> {dataset_name}")
        
        # æŸ¥è¯¢å®Œæ•´è½¨è¿¹
        logger.debug(f"ğŸ” æŸ¥è¯¢è½¨è¿¹æ•°æ®: {dataset_name}")
        trajectory_df = self.trajectory_segmenter.query_complete_trajectory(dataset_name)
        if trajectory_df.empty:
            logger.warning(f"âŒ æœªæŸ¥è¯¢åˆ°è½¨è¿¹æ•°æ®: {dataset_name}")
            return None
        
        logger.debug(f"âœ… æŸ¥è¯¢åˆ°è½¨è¿¹æ•°æ®: {len(trajectory_df)} ä¸ªç‚¹")
        
        # åˆå¹¶ç»“æœå­—æ®µ
        merged_results = self.result_processor.merge_and_clean_results(
            record.result, record.other_scenario
        )
        logger.debug(f"ğŸ“‹ åˆå¹¶ç»“æœå­—æ®µ: {merged_results}")
        
        # è®¡ç®—åŸºç¡€ç»Ÿè®¡
        start_time = int(trajectory_df['timestamp'].min())
        end_time = int(trajectory_df['timestamp'].max())
        total_duration = float(end_time - start_time)
        total_points = len(trajectory_df)
        
        logger.debug(f"ğŸ“Š è½¨è¿¹åŸºç¡€ç»Ÿè®¡: ç‚¹æ•°={total_points}, æ—¶é•¿={total_duration}s, æ—¶é—´èŒƒå›´=[{start_time}, {end_time}]")
        
        # å¤„ç†è½¨è¿¹åˆ†æ®µ
        if record.description and len(record.description) > 0:
            # æœ‰æ—¶é—´åŒºé—´æè¿°ï¼Œè¿›è¡Œåˆ†æ®µ
            logger.debug(f"ğŸ”„ å¼€å§‹æ—¶é—´åˆ†æ®µå¤„ç†: {len(record.description)} ä¸ªæ—¶é—´åŒºé—´")
            logger.debug(f"   æ—¶é—´åŒºé—´è¯¦æƒ…: {record.description}")
            logger.debug(f"   è½¨è¿¹æ—¶é•¿: {total_duration}s")
            
            geometry, segment_count = self.trajectory_segmenter.segment_trajectory_by_time_ranges(
                trajectory_df, record.description
            )
            
            if geometry.is_empty:
                logger.warning(f"âŒ åˆ†æ®µè½¨è¿¹ç”Ÿæˆå¤±è´¥: {record.autoscene_id}")
                logger.warning(f"   æ—¶é—´åŒºé—´: {record.description}")
                logger.warning(f"   è½¨è¿¹æ—¶é•¿: {total_duration}s")
                # å¦‚æœåˆ†æ®µå¤±è´¥ï¼Œå°è¯•åˆ›å»ºå®Œæ•´è½¨è¿¹ä½œä¸ºå¤‡é€‰
                logger.info(f"ğŸ”„ åˆ†æ®µå¤±è´¥ï¼Œå°è¯•åˆ›å»ºå®Œæ•´è½¨è¿¹ä½œä¸ºå¤‡é€‰...")
                geometry, _ = self.trajectory_segmenter.create_complete_trajectory(trajectory_df)
                segment_count = 0
                if geometry.is_empty:
                    logger.warning(f"âŒ å¤‡é€‰å®Œæ•´è½¨è¿¹ä¹Ÿå¤±è´¥: {record.autoscene_id}")
                    return None
                else:
                    logger.info(f"âœ… å¤‡é€‰å®Œæ•´è½¨è¿¹åˆ›å»ºæˆåŠŸ")
            else:
                logger.debug(f"âœ… åˆ†æ®µè½¨è¿¹åˆ›å»ºæˆåŠŸ: {segment_count} ä¸ªåˆ†æ®µ")
        else:
            # æ— æè¿°ï¼Œä¿ç•™å®Œæ•´è½¨è¿¹
            logger.debug(f"ğŸ”„ åˆ›å»ºå®Œæ•´è½¨è¿¹ï¼ˆæ— æ—¶é—´åŒºé—´æè¿°ï¼‰")
            geometry, _ = self.trajectory_segmenter.create_complete_trajectory(trajectory_df)
            segment_count = 0
            
            if geometry.is_empty:
                logger.warning(f"âŒ å®Œæ•´è½¨è¿¹ç”Ÿæˆå¤±è´¥: {record.autoscene_id}")
                logger.warning(f"   è½¨è¿¹ç‚¹æ•°: {total_points}")
                logger.warning(f"   æœ€å°ç‚¹æ•°è¦æ±‚: {self.config.min_points_per_segment}")
                return None
                
            logger.debug(f"âœ… å®Œæ•´è½¨è¿¹åˆ›å»ºæˆåŠŸ")
        
        return SegmentedTrajectory(
            task_name=record.task_name,
            annotator=record.annotator,
            scene_id=record.autoscene_id,
            dataset_name=dataset_name,
            segment_count=segment_count,
            merged_results=merged_results,
            geometry=geometry,
            total_duration=total_duration,
            start_time=start_time,
            end_time=end_time,
            total_points=total_points
        )
    
    def _save_trajectories_to_table(self, 
                                   trajectories: List[SegmentedTrajectory], 
                                   table_name: str) -> Tuple[int, Dict]:
        """ä¿å­˜è½¨è¿¹æ•°æ®åˆ°æ•°æ®åº“è¡¨
        
        Args:
            trajectories: è½¨è¿¹æ•°æ®åˆ—è¡¨
            table_name: ç›®æ ‡è¡¨å
            
        Returns:
            (ä¿å­˜æˆåŠŸçš„è®°å½•æ•°, ä¿å­˜ç»Ÿè®¡)
        """
        start_time = time.time()
        
        save_stats = {
            'total_trajectories': len(trajectories),
            'saved_records': 0,
            'save_time': 0,
            'table_created': False,
            'batch_count': 0
        }
        
        if not trajectories:
            logger.warning("æ²¡æœ‰è½¨è¿¹æ•°æ®éœ€è¦ä¿å­˜")
            return 0, save_stats
        
        try:
            # åˆ›å»ºè¡¨
            if not self._create_trajectory_table(table_name):
                logger.error("åˆ›å»ºè¡¨å¤±è´¥")
                return 0, save_stats
            
            save_stats['table_created'] = True
            
            # æ‰¹é‡æ’å…¥
            total_saved = 0
            for i in range(0, len(trajectories), self.config.batch_insert_size):
                batch = trajectories[i:i+self.config.batch_insert_size]
                batch_num = i // self.config.batch_insert_size + 1
                
                logger.info(f"ä¿å­˜ç¬¬ {batch_num} æ‰¹: {len(batch)} æ¡è½¨è¿¹")
                
                # å‡†å¤‡GeoDataFrameæ•°æ®
                gdf_data = []
                geometries = []
                
                for j, traj in enumerate(batch):
                    # è½¬æ¢PostgreSQLæ•°ç»„æ ¼å¼ï¼ˆç¡®ä¿UTF-8ç¼–ç ï¼‰
                    try:
                        # ç¡®ä¿æ¯ä¸ªç»“æœéƒ½æ˜¯æ­£ç¡®ç¼–ç çš„å­—ç¬¦ä¸²
                        cleaned_results = []
                        for result in traj.merged_results:
                            if isinstance(result, str):
                                # ç¡®ä¿å­—ç¬¦ä¸²æ˜¯UTF-8ç¼–ç 
                                cleaned_result = result.encode('utf-8', errors='ignore').decode('utf-8')
                                # è½¬ä¹‰åŒå¼•å·
                                cleaned_result = cleaned_result.replace('"', '""')
                                cleaned_results.append(cleaned_result)
                            else:
                                cleaned_results.append(str(result))
                        
                        merged_results_pg = '{' + ','.join(f'"{result}"' for result in cleaned_results) + '}'
                    except Exception as e:
                        logger.warning(f"PostgreSQLæ•°ç»„æ ¼å¼è½¬æ¢å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤æ ¼å¼")
                        merged_results_pg = '{' + ','.join(f'"{str(result)}"' for result in traj.merged_results) + '}'
                    
                    row = {
                        'task_name': traj.task_name,
                        'annotator': traj.annotator,
                        'scene_id': traj.scene_id,
                        'dataset_name': traj.dataset_name,
                        'segment_count': traj.segment_count,
                        'merged_results': merged_results_pg,
                        'total_duration': traj.total_duration,
                        'start_time': traj.start_time,
                        'end_time': traj.end_time,
                        'total_points': traj.total_points
                    }
                    
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºç¬¬ä¸€æ¡è®°å½•çš„è¯¦ç»†ä¿¡æ¯
                    if batch_num == 1 and j == 0:
                        logger.debug(f"ğŸ’¾ ç¬¬ä¸€æ¡è®°å½•ä¿å­˜ä¿¡æ¯:")
                        logger.debug(f"   task_name: '{traj.task_name}'")
                        logger.debug(f"   annotator: '{traj.annotator}'")
                        logger.debug(f"   scene_id: '{traj.scene_id}'")
                        logger.debug(f"   dataset_name: '{traj.dataset_name}'")
                        logger.debug(f"   segment_count: {traj.segment_count}")
                        logger.debug(f"   merged_results: {traj.merged_results}")
                        logger.debug(f"   merged_results_pg: '{merged_results_pg}'")
                        # æ˜¾ç¤ºç¼–ç ä¿¡æ¯
                        for idx, result in enumerate(traj.merged_results):
                            try:
                                encoded_check = result.encode('utf-8').decode('utf-8')
                                logger.debug(f"   ç»“æœ{idx+1} ç¼–ç æ£€æŸ¥: '{result}' -> UTF-8æ­£å¸¸")
                            except:
                                logger.debug(f"   ç»“æœ{idx+1} ç¼–ç æ£€æŸ¥: '{result}' -> å¯èƒ½æœ‰ç¼–ç é—®é¢˜")
                        logger.debug(f"   total_duration: {traj.total_duration}")
                        logger.debug(f"   å‡ ä½•ç±»å‹: {traj.geometry.geom_type}")
                        logger.debug(f"   å‡ ä½•æ˜¯å¦ä¸ºç©º: {traj.geometry.is_empty}")
                    
                    gdf_data.append(row)
                    geometries.append(traj.geometry)
                
                # åˆ›å»ºGeoDataFrame
                gdf = gpd.GeoDataFrame(gdf_data, geometry=geometries, crs=4326)
                
                # æ‰¹é‡æ’å…¥åˆ°æ•°æ®åº“
                gdf.to_postgis(
                    table_name, 
                    self.engine, 
                    if_exists='append', 
                    index=False
                )
                
                total_saved += len(gdf)
                save_stats['batch_count'] += 1
                
                logger.debug(f"æ‰¹æ¬¡ {batch_num} ä¿å­˜å®Œæˆ: {len(gdf)} æ¡è®°å½•")
            
            save_stats['saved_records'] = total_saved
            save_stats['save_time'] = time.time() - start_time
            
            logger.info(f"âœ… æ•°æ®åº“ä¿å­˜å®Œæˆ: {save_stats['saved_records']} æ¡è½¨è¿¹è®°å½•, "
                       f"{save_stats['batch_count']} ä¸ªæ‰¹æ¬¡, "
                       f"è¡¨: {table_name}, "
                       f"ç”¨æ—¶: {save_stats['save_time']:.2f}s")
            
            return total_saved, save_stats
            
        except Exception as e:
            logger.error(f"ä¿å­˜è½¨è¿¹æ•°æ®å¤±è´¥: {str(e)}")
            return 0, save_stats
    
    def _create_trajectory_table(self, table_name: str) -> bool:
        """åˆ›å»ºè´¨æ£€è½¨è¿¹ç»“æœè¡¨"""
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
            check_table_sql = text(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = '{table_name}'
                );
            """)
            
            with self.engine.connect() as conn:
                result = conn.execute(check_table_sql)
                table_exists = result.scalar()
                
                if table_exists:
                    logger.info(f"è¡¨ {table_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                    return True
            
            logger.info(f"åˆ›å»ºè´¨æ£€è½¨è¿¹è¡¨: {table_name}")
            
            # åˆ›å»ºè¡¨ç»“æ„
            create_sql = text(f"""
                CREATE TABLE {table_name} (
                    id serial PRIMARY KEY,
                    task_name text NOT NULL,
                    annotator text NOT NULL,
                    scene_id text NOT NULL,
                    dataset_name text NOT NULL,
                    segment_count integer DEFAULT 0,
                    merged_results text[],
                    total_duration numeric(10,2),
                    start_time bigint,
                    end_time bigint,
                    total_points integer,
                    created_at timestamp DEFAULT CURRENT_TIMESTAMP
                );
            """)
            
            # æ·»åŠ å‡ ä½•åˆ—
            add_geom_sql = text(f"""
                SELECT AddGeometryColumn('public', '{table_name}', 'geometry', 4326, 'MULTILINESTRING', 2);
            """)
            
            # åˆ›å»ºç´¢å¼•
            index_sql = text(f"""
                CREATE INDEX idx_{table_name}_geometry ON {table_name} USING GIST(geometry);
                CREATE INDEX idx_{table_name}_scene_id ON {table_name}(scene_id);
                CREATE INDEX idx_{table_name}_task_name ON {table_name}(task_name);
                CREATE INDEX idx_{table_name}_annotator ON {table_name}(annotator);
                CREATE INDEX idx_{table_name}_dataset_name ON {table_name}(dataset_name);
                CREATE INDEX idx_{table_name}_merged_results ON {table_name} USING GIN(merged_results);
                CREATE INDEX idx_{table_name}_created_at ON {table_name}(created_at);
            """)
            
            # åˆ†æ­¥æ‰§è¡ŒSQL
            with self.engine.connect() as conn:
                conn.execute(create_sql)
                conn.commit()
            logger.debug("âœ… è¡¨ç»“æ„åˆ›å»ºå®Œæˆ")
            
            with self.engine.connect() as conn:
                conn.execute(add_geom_sql)
                conn.commit()
            logger.debug("âœ… å‡ ä½•åˆ—æ·»åŠ å®Œæˆ")
            
            with self.engine.connect() as conn:
                conn.execute(index_sql)
                conn.commit()
            logger.debug("âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ")
            
            logger.info(f"âœ… è´¨æ£€è½¨è¿¹è¡¨åˆ›å»ºæˆåŠŸ: {table_name}")
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºè´¨æ£€è½¨è¿¹è¡¨å¤±è´¥: {table_name}, é”™è¯¯: {str(e)}")
            return False
    
    def _export_trajectories_to_geojson(self, 
                                       trajectories: List[SegmentedTrajectory], 
                                       output_file: str) -> bool:
        """å¯¼å‡ºè½¨è¿¹æ•°æ®åˆ°GeoJSONæ–‡ä»¶
        
        Args:
            trajectories: è½¨è¿¹æ•°æ®åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¯¼å‡ºæ˜¯å¦æˆåŠŸ
        """
        if not trajectories:
            logger.warning("æ²¡æœ‰è½¨è¿¹æ•°æ®éœ€è¦å¯¼å‡º")
            return False
        
        try:
            # å‡†å¤‡GeoDataFrameæ•°æ®
            gdf_data = []
            geometries = []
            
            for traj in trajectories:
                # ç¡®ä¿å­—ç¬¦ä¸²å­—æ®µç¼–ç æ­£ç¡®
                try:
                    merged_results_str = ','.join(
                        result.encode('utf-8', errors='ignore').decode('utf-8') 
                        if isinstance(result, str) else str(result)
                        for result in traj.merged_results
                    )
                except:
                    merged_results_str = ','.join(str(result) for result in traj.merged_results)
                
                row = {
                    'task_name': traj.task_name,
                    'annotator': traj.annotator,
                    'scene_id': traj.scene_id,
                    'dataset_name': traj.dataset_name,
                    'segment_count': traj.segment_count,
                    'merged_results': merged_results_str,
                    'total_duration': traj.total_duration,
                    'start_time': traj.start_time,
                    'end_time': traj.end_time,
                    'total_points': traj.total_points
                }
                gdf_data.append(row)
                geometries.append(traj.geometry)
            
            # åˆ›å»ºGeoDataFrame
            gdf = gpd.GeoDataFrame(gdf_data, geometry=geometries, crs=4326)
            
            # å¯¼å‡ºåˆ°GeoJSON
            gdf.to_file(output_file, driver='GeoJSON', encoding='utf-8')
            
            logger.info(f"æˆåŠŸå¯¼å‡º {len(gdf)} æ¡è½¨è¿¹åˆ°æ–‡ä»¶: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºè½¨è¿¹æ•°æ®å¤±è´¥: {str(e)}")
            return False

# ä¾¿æ·å‡½æ•°
def process_quality_check_excel(
    excel_files: Union[str, List[str]],
    output_table: Optional[str] = None,
    output_geojson: Optional[str] = None,
    config: Optional[QualityCheckConfig] = None
) -> Dict:
    """å¤„ç†è´¨æ£€Excelæ–‡ä»¶å®Œæ•´æµç¨‹ï¼ˆæ”¯æŒå¤šæ–‡ä»¶å’Œä¸‡çº§æ•°æ®ï¼‰
    
    Args:
        excel_files: Excelæ–‡ä»¶è·¯å¾„æˆ–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        output_table: è¾“å‡ºæ•°æ®åº“è¡¨åï¼ˆå¯é€‰ï¼‰
        output_geojson: è¾“å‡ºGeoJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        config: è‡ªå®šä¹‰é…ç½®ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    query_config = config or QualityCheckConfig()
    processor = QualityCheckTrajectoryQuery(query_config)
    
    return processor.process_excel_files(
        excel_files=excel_files,
        output_table=output_table,
        output_geojson=output_geojson
    )

def main():
    """ä¸»å‡½æ•°ï¼ŒCLIå…¥å£ç‚¹"""
    parser = argparse.ArgumentParser(
        description='è´¨æ£€è½¨è¿¹æŸ¥è¯¢æ¨¡å— - åŸºäºExcelè´¨æ£€ç»“æœæŸ¥è¯¢å’Œåˆ†æ®µè½¨è¿¹æ•°æ®ï¼ˆä¸‡çº§æ•°æ®ä¼˜åŒ–ç‰ˆï¼‰',
        epilog="""
æ ¸å¿ƒåŠŸèƒ½:
  â€¢ å¤šExcelæ–‡ä»¶æ‰¹é‡å¤„ç†ï¼šæ”¯æŒåŒæ—¶å¤„ç†å¤šä¸ªExcelæ–‡ä»¶
  â€¢ æ™ºèƒ½æ•°æ®è¿‡æ»¤ï¼šè‡ªåŠ¨è¿‡æ»¤æ— resultå’Œother_scenarioçš„æ— æ•ˆè®°å½•
  â€¢ ä¸‡çº§æ•°æ®å¹¶è¡Œå¤„ç†ï¼šæ”¯æŒä¸‡çº§æ•°æ®é‡çš„é«˜æ•ˆå¹¶è¡Œå¤„ç†
  â€¢ æ™ºèƒ½å­—æ®µåˆå¹¶ï¼šè‡ªåŠ¨åˆå¹¶resultå’Œother_scenarioå­—æ®µï¼Œå»é‡å¤„ç†
  â€¢ æ—¶é—´åˆ†æ®µè½¨è¿¹ï¼šæ ¹æ®descriptionæ—¶é—´åŒºé—´è¿›è¡Œè½¨è¿¹æ™ºèƒ½åˆ†æ®µ
  â€¢ ç»Ÿä¸€å‡ ä½•æ ¼å¼ï¼šç»Ÿä¸€è¾“å‡ºMultiLineStringæ ¼å¼ï¼Œæ”¯æŒå®Œæ•´è½¨è¿¹å’Œåˆ†æ®µè½¨è¿¹
  â€¢ é«˜æ€§èƒ½ä¼˜åŒ–ï¼šåˆ†å—å¤„ç†ã€å†…å­˜ä¼˜åŒ–ã€åœºæ™¯æ˜ å°„ç¼“å­˜

ç¤ºä¾‹:
  # åŸºç¡€ç”¨æ³•ï¼šå¤„ç†å•ä¸ªExcelæ–‡ä»¶
  python -m spdatalab.dataset.quality_check_trajectory_query --input quality_check.xlsx --table qc_trajectories
  
  # å¤šæ–‡ä»¶æ‰¹é‡å¤„ç†
  python -m spdatalab.dataset.quality_check_trajectory_query --input file1.xlsx file2.xlsx file3.xlsx --table qc_trajectories
  
  # ä¸‡çº§æ•°æ®é«˜æ€§èƒ½å¤„ç†
  python -m spdatalab.dataset.quality_check_trajectory_query --input large_data.xlsx \\
    --table qc_trajectories --max-workers 8 --chunk-size 2000 --batch-size 3000
  
  # è‡ªå®šä¹‰é…ç½®å’Œç¦ç”¨å¹¶è¡Œå¤„ç†
  python -m spdatalab.dataset.quality_check_trajectory_query --input quality_check.xlsx \\
    --table qc_trajectories --disable-parallel --large-data-threshold 10000 --verbose
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--input', required=True, nargs='+', help='è¾“å…¥Excelæ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒå¤šä¸ªæ–‡ä»¶ï¼‰')
    parser.add_argument('--table', help='è¾“å‡ºæ•°æ®åº“è¡¨åï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--output', help='è¾“å‡ºGeoJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    
    # å¤„ç†é…ç½®å‚æ•°
    parser.add_argument('--batch-size', type=int, default=2000,
                       help='æ‰¹é‡æ’å…¥æ•°æ®åº“çš„æ‰¹æ¬¡å¤§å° (é»˜è®¤: 2000)')
    parser.add_argument('--min-points', type=int, default=2,
                       help='æ„å»ºè½¨è¿¹åˆ†æ®µçš„æœ€å°ç‚¹æ•° (é»˜è®¤: 2)')
    parser.add_argument('--time-tolerance', type=float, default=0.1,
                       help='æ—¶é—´åŒ¹é…å®¹å·®ï¼ˆç§’ï¼‰(é»˜è®¤: 0.1)')
    parser.add_argument('--simplify', action='store_true',
                       help='å¯ç”¨å‡ ä½•ç®€åŒ–')
    parser.add_argument('--simplify-tolerance', type=float, default=0.00001,
                       help='å‡ ä½•ç®€åŒ–å®¹å·® (é»˜è®¤: 0.00001)')
    
    # å¹¶è¡Œå¤„ç†å’Œå¤§æ•°æ®é…ç½®
    parser.add_argument('--max-workers', type=int, default=4,
                       help='æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 4)')
    parser.add_argument('--disable-parallel', action='store_true',
                       help='ç¦ç”¨å¹¶è¡Œå¤„ç†')
    parser.add_argument('--large-data-threshold', type=int, default=5000,
                       help='å¤§æ•°æ®å¤„ç†é˜ˆå€¼ (é»˜è®¤: 5000)')
    parser.add_argument('--chunk-size', type=int, default=1000,
                       help='åˆ†å—å¤„ç†å¤§å° (é»˜è®¤: 1000)')
    parser.add_argument('--disable-filter', action='store_true',
                       help='ç¦ç”¨æ— æ•ˆæ•°æ®è¿‡æ»¤')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†æ—¥å¿—')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if not args.table and not args.output:
        parser.error("å¿…é¡»æŒ‡å®š --table æˆ– --output ä¸­çš„è‡³å°‘ä¸€ä¸ª")
    
    # é…ç½®æ—¥å¿—
    level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        input_files = args.input
        for file_path in input_files:
            if not Path(file_path).exists():
                logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return 1
        
        # æ„å»ºé…ç½®
        config = QualityCheckConfig(
            batch_insert_size=args.batch_size,
            min_points_per_segment=args.min_points,
            time_tolerance=args.time_tolerance,
            simplify_geometry=args.simplify,
            simplify_tolerance=args.simplify_tolerance,
            max_workers=args.max_workers,
            enable_parallel_processing=not args.disable_parallel,
            large_data_threshold=args.large_data_threshold,
            chunk_processing_size=args.chunk_size,
            filter_invalid_records=not args.disable_filter
        )
        
        # è¾“å‡ºé…ç½®ä¿¡æ¯
        logger.info("ğŸ”§ é…ç½®å‚æ•°:")
        logger.info(f"   â€¢ è¾“å…¥æ–‡ä»¶æ•°é‡: {len(input_files)}")
        logger.info(f"   â€¢ æ‰¹é‡æ’å…¥å¤§å°: {config.batch_insert_size}")
        logger.info(f"   â€¢ æœ€å°åˆ†æ®µç‚¹æ•°: {config.min_points_per_segment}")
        logger.info(f"   â€¢ æ—¶é—´åŒ¹é…å®¹å·®: {config.time_tolerance}s")
        logger.info(f"   â€¢ å‡ ä½•ç®€åŒ–: {'å¯ç”¨' if config.simplify_geometry else 'ç¦ç”¨'}")
        logger.info(f"   â€¢ å¹¶è¡Œå¤„ç†: {'å¯ç”¨' if config.enable_parallel_processing else 'ç¦ç”¨'}")
        logger.info(f"   â€¢ æœ€å¤§å·¥ä½œçº¿ç¨‹: {config.max_workers}")
        logger.info(f"   â€¢ å¤§æ•°æ®é˜ˆå€¼: {config.large_data_threshold}")
        logger.info(f"   â€¢ åˆ†å—å¤§å°: {config.chunk_processing_size}")
        logger.info(f"   â€¢ æ•°æ®è¿‡æ»¤: {'å¯ç”¨' if config.filter_invalid_records else 'ç¦ç”¨'}")
        
        # æ‰§è¡Œå¤„ç†
        stats = process_quality_check_excel(
            excel_files=input_files,
            output_table=args.table,
            output_geojson=args.output,
            config=config
        )
        
        # æ£€æŸ¥å¤„ç†ç»“æœ
        if 'error' in stats:
            logger.error(f"âŒ å¤„ç†é”™è¯¯: {stats['error']}")
            return 1
        
        if not stats.get('success', False):
            logger.error("âŒ å¤„ç†æœªæˆåŠŸå®Œæˆ")
            return 1
        
        # æˆåŠŸå®Œæˆ
        logger.info("ğŸ‰ æ‰€æœ‰å¤„ç†æˆåŠŸå®Œæˆï¼")
        
        # ç¡®å®šè¿”å›ä»£ç 
        has_results = stats.get('valid_trajectories', 0) > 0
        return 0 if has_results else 1
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        return 1

if __name__ == '__main__':
    sys.exit(main()) 