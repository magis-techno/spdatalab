"""é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢æ¨¡å—

åŸºäºspatial_join_production.pyçš„ä¼˜åŒ–ç­–ç•¥ï¼š
- å°è§„æ¨¡ï¼ˆâ‰¤50ä¸ªpolygonï¼‰ï¼šæ‰¹é‡æŸ¥è¯¢ï¼ˆUNION ALLï¼‰- æœ€å¿«
- å¤§è§„æ¨¡ï¼ˆ>50ä¸ªpolygonï¼‰ï¼šåˆ†å—æ‰¹é‡æŸ¥è¯¢ - æœ€ç¨³å®š
- é«˜æ•ˆæ•°æ®åº“å†™å…¥å’Œè½¨è¿¹æ„å»º

åŠŸèƒ½ï¼š
1. è¯»å–GeoJSONæ–‡ä»¶ä¸­çš„polygonï¼ˆæ”¯æŒå¤šä¸ªï¼‰
2. é«˜æ•ˆæ‰¹é‡æŸ¥è¯¢ä¸polygonç›¸äº¤çš„è½¨è¿¹ç‚¹
3. æ™ºèƒ½åˆ†ç»„æ„å»ºè½¨è¿¹çº¿å’Œç»Ÿè®¡ä¿¡æ¯
4. æ‰¹é‡å†™å…¥æ•°æ®åº“ï¼Œä¼˜åŒ–æ€§èƒ½
"""

import argparse
import json
import logging
import sys
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import warnings

import geopandas as gpd
import pandas as pd
from shapely.geometry import shape, LineString, Point
from sqlalchemy import text, create_engine
from spdatalab.common.io_hive import hive_cursor

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)

# æ•°æ®åº“é…ç½®
LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"
POINT_TABLE = "public.ddi_data_points"

# æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)

@dataclass
class PolygonTrajectoryConfig:
    """Polygonè½¨è¿¹æŸ¥è¯¢é…ç½®"""
    local_dsn: str = LOCAL_DSN
    point_table: str = POINT_TABLE
    
    # æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–é…ç½®
    batch_threshold: int = 50          # æ‰¹é‡æŸ¥è¯¢vsåˆ†å—æŸ¥è¯¢çš„é˜ˆå€¼
    chunk_size: int = 20               # åˆ†å—å¤§å°
    limit_per_polygon: int = 10000     # æ¯ä¸ªpolygonçš„è½¨è¿¹ç‚¹é™åˆ¶
    batch_insert_size: int = 1000      # æ‰¹é‡æ’å…¥å¤§å°
    
    # æŸ¥è¯¢ä¼˜åŒ–é…ç½®
    enable_spatial_index: bool = True  # å¯ç”¨ç©ºé—´ç´¢å¼•ä¼˜åŒ–
    query_timeout: int = 300           # æŸ¥è¯¢è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    # è½¨è¿¹æ„å»ºé…ç½®
    min_points_per_trajectory: int = 2 # æ„å»ºè½¨è¿¹çš„æœ€å°ç‚¹æ•°
    enable_speed_stats: bool = True    # å¯ç”¨é€Ÿåº¦ç»Ÿè®¡
    enable_avp_stats: bool = True      # å¯ç”¨AVPç»Ÿè®¡
    
    # å®Œæ•´è½¨è¿¹è·å–é…ç½®
    fetch_complete_trajectories: bool = True  # æ˜¯å¦è·å–å®Œæ•´è½¨è¿¹ï¼ˆè€Œéä»…å¤šè¾¹å½¢å†…çš„ç‰‡æ®µï¼‰

def load_polygons_from_geojson(file_path: str) -> List[Dict]:
    """ä»GeoJSONæ–‡ä»¶åŠ è½½polygon
    
    Args:
        file_path: GeoJSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        polygonåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«geometryå’Œproperties
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            geojson_data = json.load(f)
        
        polygons = []
        
        if geojson_data.get('type') == 'FeatureCollection':
            # FeatureCollectionæ ¼å¼
            for i, feature in enumerate(geojson_data.get('features', [])):
                if feature.get('geometry', {}).get('type') in ['Polygon', 'MultiPolygon']:
                    polygon_info = {
                        'id': feature.get('properties', {}).get('id', f'polygon_{i}'),
                        'geometry': shape(feature['geometry']),
                        'properties': feature.get('properties', {})
                    }
                    polygons.append(polygon_info)
        elif geojson_data.get('type') in ['Polygon', 'MultiPolygon']:
            # å•ä¸ªå‡ ä½•å¯¹è±¡
            polygon_info = {
                'id': 'polygon_0',
                'geometry': shape(geojson_data),
                'properties': {}
            }
            polygons.append(polygon_info)
        else:
            logger.error(f"ä¸æ”¯æŒçš„GeoJSONæ ¼å¼: {geojson_data.get('type')}")
            return []
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(polygons)} ä¸ªpolygon")
        for polygon in polygons:
            logger.debug(f"Polygon {polygon['id']}: {polygon['geometry'].geom_type}")
        
        return polygons
        
    except Exception as e:
        logger.error(f"åŠ è½½GeoJSONæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
        raise

class HighPerformancePolygonTrajectoryQuery:
    """é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢å™¨"""
    
    def __init__(self, config: Optional[PolygonTrajectoryConfig] = None):
        self.config = config or PolygonTrajectoryConfig()
        self.engine = create_engine(
            self.config.local_dsn, 
            future=True,
            connect_args={"client_encoding": "utf8"}
        )
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºç±»çš„å¯ç”¨æ–¹æ³•
        logger.debug(f"ğŸ”§ HighPerformancePolygonTrajectoryQuery åˆå§‹åŒ–å®Œæˆ")
        logger.debug(f"ğŸ”§ å¯ç”¨æ–¹æ³•: {[method for method in dir(self) if not method.startswith('_')]}")
        logger.debug(f"ğŸ”§ process_complete_workflow æ–¹æ³•å­˜åœ¨: {hasattr(self, 'process_complete_workflow')}")
    
    def query_intersecting_trajectory_points(self, polygons: List[Dict]) -> Tuple[pd.DataFrame, Dict]:
        """é«˜æ•ˆæ‰¹é‡æŸ¥è¯¢ä¸polygonç›¸äº¤çš„è½¨è¿¹ç‚¹
        
        Args:
            polygons: polygonåˆ—è¡¨
            
        Returns:
            (è½¨è¿¹ç‚¹DataFrame, æ€§èƒ½ç»Ÿè®¡)
        """
        start_time = time.time()
        
        # æ€§èƒ½ç»Ÿè®¡
        stats = {
            'polygon_count': len(polygons),
            'strategy': None,
            'chunk_size': None,
            'query_time': 0,
            'total_points': 0,
            'unique_datasets': 0,
            'points_per_polygon': 0
        }
        
        if not polygons:
            logger.warning("æ²¡æœ‰polygonæ•°æ®")
            return pd.DataFrame(), stats
        
        logger.info(f"å¼€å§‹æ‰¹é‡æŸ¥è¯¢ {len(polygons)} ä¸ªpolygonçš„è½¨è¿¹ç‚¹")
        
        # é€‰æ‹©æœ€ä¼˜æŸ¥è¯¢ç­–ç•¥
        if len(polygons) <= self.config.batch_threshold:
            stats['strategy'] = 'batch_query'
            result_df = self._batch_query_strategy(polygons)
        else:
            stats['strategy'] = 'chunked_query'
            stats['chunk_size'] = self.config.chunk_size
            result_df = self._chunked_query_strategy(polygons)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats['query_time'] = time.time() - start_time
        stats['total_points'] = len(result_df)
        
        if not result_df.empty:
            stats['unique_datasets'] = result_df['dataset_name'].nunique()
            stats['points_per_polygon'] = stats['total_points'] / stats['polygon_count']
            
            # å¦‚æœå¯ç”¨å®Œæ•´è½¨è¿¹è·å–ï¼Œåˆ™è·å–å®Œæ•´è½¨è¿¹æ•°æ®
            if self.config.fetch_complete_trajectories:
                logger.info(f"ğŸ”„ è·å–å®Œæ•´è½¨è¿¹æ•°æ®...")
                complete_result_df, complete_stats = self._fetch_complete_trajectories(result_df)
                
                if not complete_result_df.empty:
                    result_df = complete_result_df
                    stats.update(complete_stats)
                    stats['total_points'] = len(result_df)
                    stats['complete_trajectories_fetched'] = True
                    logger.info(f"âœ… å®Œæ•´è½¨è¿¹è·å–å®Œæˆ: {stats['total_points']} ä¸ªè½¨è¿¹ç‚¹")
                else:
                    stats['complete_trajectories_fetched'] = False
                    logger.warning("âš ï¸ å®Œæ•´è½¨è¿¹è·å–å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®")
            
            logger.info(f"âœ… æŸ¥è¯¢å®Œæˆ: {stats['total_points']} ä¸ªè½¨è¿¹ç‚¹, "
                       f"{stats['unique_datasets']} ä¸ªæ•°æ®é›†, "
                       f"ç­–ç•¥: {stats['strategy']}, "
                       f"ç”¨æ—¶: {stats['query_time']:.2f}s")
        else:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•ç›¸äº¤çš„è½¨è¿¹ç‚¹")
        
        return result_df, stats
    
    def _fetch_complete_trajectories(self, intersection_result_df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """è·å–å®Œæ•´è½¨è¿¹æ•°æ®ï¼ˆåŸºäºç›¸äº¤ç»“æœä¸­çš„data_nameï¼‰
        
        Args:
            intersection_result_df: å¤šè¾¹å½¢ç›¸äº¤ç»“æœDataFrame
            
        Returns:
            (å®Œæ•´è½¨è¿¹DataFrame, ç»Ÿè®¡ä¿¡æ¯)
        """
        start_time = time.time()
        
        # ç»Ÿè®¡ä¿¡æ¯
        complete_stats = {
            'complete_query_time': 0,
            'original_datasets': 0,
            'original_points': 0,
            'complete_datasets': 0,
            'complete_points': 0
        }
        
        if intersection_result_df.empty:
            logger.warning("ç›¸äº¤ç»“æœä¸ºç©ºï¼Œæ— æ³•è·å–å®Œæ•´è½¨è¿¹")
            return pd.DataFrame(), complete_stats
        
        # è·å–æ‰€æœ‰æ¶‰åŠçš„data_name
        unique_data_names = intersection_result_df['dataset_name'].unique()
        complete_stats['original_datasets'] = len(unique_data_names)
        complete_stats['original_points'] = len(intersection_result_df)
        
        logger.info(f"ğŸ“‹ éœ€è¦è·å–å®Œæ•´è½¨è¿¹çš„æ•°æ®é›†: {len(unique_data_names)} ä¸ª")
        
        try:
            # æ„å»ºæŸ¥è¯¢æ‰€æœ‰å®Œæ•´è½¨è¿¹çš„SQL
            data_names_tuple = tuple(unique_data_names)
            
            complete_trajectory_sql = f"""
                SELECT 
                    dataset_name,
                    timestamp,
                    point_lla,
                    twist_linear,
                    avp_flag,
                    workstage,
                    ST_X(point_lla) as longitude,
                    ST_Y(point_lla) as latitude
                FROM {self.config.point_table}
                WHERE dataset_name IN %(data_names)s
                AND point_lla IS NOT NULL
                AND timestamp IS NOT NULL
                ORDER BY dataset_name, timestamp
            """
            
            logger.info(f"ğŸš€ æ‰§è¡Œå®Œæ•´è½¨è¿¹æŸ¥è¯¢...")
            
            with hive_cursor("dataset_gy1") as cur:
                cur.execute(complete_trajectory_sql, {"data_names": data_names_tuple})
                
                # è·å–åˆ—åå’Œæ•°æ®
                cols = [d[0] for d in cur.description]
                rows = cur.fetchall()
                
                if rows:
                    complete_df = pd.DataFrame(rows, columns=cols)
                    
                    # ä¸ºå®Œæ•´è½¨è¿¹æ•°æ®æ·»åŠ polygon_idä¿¡æ¯
                    # åŸºäºåŸå§‹ç›¸äº¤ç»“æœåˆ›å»ºdata_nameåˆ°polygon_idçš„æ˜ å°„
                    dataset_polygon_mapping = {}
                    for _, row in intersection_result_df.iterrows():
                        dataset_name = row['dataset_name']
                        polygon_id = row.get('polygon_id', 'unknown')
                        
                        if dataset_name not in dataset_polygon_mapping:
                            dataset_polygon_mapping[dataset_name] = []
                        if polygon_id not in dataset_polygon_mapping[dataset_name]:
                            dataset_polygon_mapping[dataset_name].append(polygon_id)
                    
                    # ä¸ºå®Œæ•´è½¨è¿¹æ·»åŠ polygon_idä¿¡æ¯
                    complete_df['polygon_id'] = complete_df['dataset_name'].map(
                        lambda x: dataset_polygon_mapping.get(x, ['unknown'])[0]
                    )
                    
                    complete_stats['complete_datasets'] = complete_df['dataset_name'].nunique()
                    complete_stats['complete_points'] = len(complete_df)
                    complete_stats['complete_query_time'] = time.time() - start_time
                    
                    logger.info(f"âœ… å®Œæ•´è½¨è¿¹æŸ¥è¯¢æˆåŠŸ: {len(complete_df)} ä¸ªç‚¹, "
                               f"{complete_df['dataset_name'].nunique()} ä¸ªæ•°æ®é›†, "
                               f"ç”¨æ—¶: {complete_stats['complete_query_time']:.2f}s")
                    
                    return complete_df, complete_stats
                else:
                    logger.warning("å®Œæ•´è½¨è¿¹æŸ¥è¯¢æ— ç»“æœ")
                    return pd.DataFrame(), complete_stats
                    
        except Exception as e:
            logger.error(f"è·å–å®Œæ•´è½¨è¿¹å¤±è´¥: {str(e)}")
            return pd.DataFrame(), complete_stats

    def _fetch_scene_ids_from_data_names(self, data_names: List[str]) -> pd.DataFrame:
        """æ ¹æ®data_nameæ‰¹é‡æŸ¥è¯¢å¯¹åº”çš„scene_idã€event_idã€event_nameï¼ˆæ¸è¿›å¼æŸ¥è¯¢ï¼‰
        
        é‡‡ç”¨ä¸¤é˜¶æ®µæŸ¥è¯¢ç­–ç•¥ï¼š
        1. ä¸»æŸ¥è¯¢ï¼šç›´æ¥é€šè¿‡origin_nameæŸ¥è¯¢
        2. å¤‡é€‰æŸ¥è¯¢ï¼šé€šè¿‡data_name->defect_id->origin_source_idæŸ¥è¯¢
        
        Args:
            data_names: æ•°æ®åç§°åˆ—è¡¨
            
        Returns:
            åŒ…å«data_nameã€scene_idã€event_idã€event_nameæ˜ å°„çš„DataFrame
        """
        if not data_names:
            return pd.DataFrame()
        
        # ç¬¬ä¸€é˜¶æ®µï¼šä¸»æŸ¥è¯¢ï¼ˆç›´æ¥é€šè¿‡origin_nameæŸ¥è¯¢ï¼‰
        primary_result_df = self._primary_query_by_origin_name(data_names)
        
        # æ£€æŸ¥å“ªäº›data_nameæ²¡æœ‰æŸ¥åˆ°
        found_data_names = set(primary_result_df['data_name'].tolist()) if not primary_result_df.empty else set()
        missing_data_names = [name for name in data_names if name not in found_data_names]
        
        logger.info(f"ä¸»æŸ¥è¯¢æˆåŠŸ: {len(found_data_names)}/{len(data_names)}, éœ€è¦å¤‡é€‰æŸ¥è¯¢: {len(missing_data_names)}")
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¤‡é€‰æŸ¥è¯¢ï¼ˆé€šè¿‡defect_idæŸ¥è¯¢ï¼‰
        if missing_data_names:
            logger.info(f"å¼€å§‹å¤‡é€‰æŸ¥è¯¢ï¼Œå¤„ç† {len(missing_data_names)} ä¸ªç¼ºå¤±çš„data_name")
            fallback_result_df = self._fallback_query_by_defect_id(missing_data_names)
            
            # åˆå¹¶ä¸»æŸ¥è¯¢å’Œå¤‡é€‰æŸ¥è¯¢çš„ç»“æœ
            if not fallback_result_df.empty:
                result_df = pd.concat([primary_result_df, fallback_result_df], ignore_index=True)
                logger.info(f"å¤‡é€‰æŸ¥è¯¢æˆåŠŸ: {len(fallback_result_df)} ä¸ªï¼Œæ€»è®¡: {len(result_df)} ä¸ª")
            else:
                result_df = primary_result_df
                logger.warning("å¤‡é€‰æŸ¥è¯¢æœªæ‰¾åˆ°ä»»ä½•ç»“æœ")
        else:
            result_df = primary_result_df
            logger.info("ä¸»æŸ¥è¯¢å·²æ»¡è¶³æ‰€æœ‰éœ€æ±‚ï¼Œæ— éœ€å¤‡é€‰æŸ¥è¯¢")
        
        return result_df
    
    def _primary_query_by_origin_name(self, data_names: List[str]) -> pd.DataFrame:
        """ä¸»æŸ¥è¯¢ï¼šç›´æ¥é€šè¿‡origin_nameæŸ¥è¯¢scene_idã€event_idã€event_name"""
        try:
            sql = """
                SELECT origin_name AS data_name, 
                       id AS scene_id,
                       event_id,
                       event_name
                FROM (
                    SELECT origin_name, 
                           id, 
                           event_id,
                           event_name,
                           ROW_NUMBER() OVER (PARTITION BY origin_name ORDER BY updated_at DESC) as rn
                    FROM transform.ods_t_data_fragment_datalake 
                    WHERE origin_name IN %(tok)s
                ) ranked
                WHERE rn = 1
            """
            
            with hive_cursor() as cur:
                cur.execute(sql, {"tok": tuple(data_names)})
                cols = [d[0] for d in cur.description]
                result_df = pd.DataFrame(cur.fetchall(), columns=cols)
                
            logger.debug(f"ä¸»æŸ¥è¯¢å®Œæˆ: {len(result_df)} æ¡è®°å½•")
            return result_df
            
        except Exception as e:
            logger.error(f"ä¸»æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def _fallback_query_by_defect_id(self, missing_data_names: List[str]) -> pd.DataFrame:
        """å¤‡é€‰æŸ¥è¯¢ï¼šé€šè¿‡data_name->defect_id->origin_source_idæŸ¥è¯¢"""
        try:
            # ç¬¬ä¸€æ­¥ï¼šé€šè¿‡data_nameæŸ¥è¯¢defect_id
            defect_mapping = self._query_defect_ids(missing_data_names)
            
            if defect_mapping.empty:
                logger.warning("æœªæŸ¥è¯¢åˆ°ä»»ä½•defect_id")
                return pd.DataFrame()
            
            # ç¬¬äºŒæ­¥ï¼šé€šè¿‡defect_idæŸ¥è¯¢scene_idã€event_idã€event_name
            defect_ids = defect_mapping['defect_id'].tolist()
            result_df = self._query_by_origin_source_id(defect_ids, defect_mapping)
            
            logger.debug(f"å¤‡é€‰æŸ¥è¯¢å®Œæˆ: {len(result_df)} æ¡è®°å½•")
            return result_df
            
        except Exception as e:
            logger.error(f"å¤‡é€‰æŸ¥è¯¢å¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def _query_defect_ids(self, data_names: List[str]) -> pd.DataFrame:
        """æŸ¥è¯¢data_nameå¯¹åº”çš„defect_id"""
        try:
            sql = "SELECT id AS data_name, defect_id FROM elasticsearch_ros.ods_ddi_index002_datalake WHERE id IN %(tok)s"
            
            with hive_cursor() as cur:
                cur.execute(sql, {"tok": tuple(data_names)})
                cols = [d[0] for d in cur.description]
                result_df = pd.DataFrame(cur.fetchall(), columns=cols)
                
            logger.debug(f"defect_idæŸ¥è¯¢: {len(result_df)}/{len(data_names)} æˆåŠŸ")
            return result_df
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢defect_idå¤±è´¥: {str(e)}")
            return pd.DataFrame()
    
    def _query_by_origin_source_id(self, defect_ids: List[str], defect_mapping: pd.DataFrame) -> pd.DataFrame:
        """é€šè¿‡origin_source_idæŸ¥è¯¢scene_idã€event_idã€event_name"""
        try:
            sql = """
                SELECT origin_source_id AS defect_id,
                       id AS scene_id,
                       event_id,
                       event_name
                FROM (
                    SELECT origin_source_id, 
                           id, 
                           event_id,
                           event_name,
                           ROW_NUMBER() OVER (PARTITION BY origin_source_id ORDER BY updated_at DESC) as rn
                    FROM transform.ods_t_data_fragment_datalake 
                    WHERE origin_source_id IN %(tok)s
                ) ranked
                WHERE rn = 1
            """
            
            with hive_cursor() as cur:
                cur.execute(sql, {"tok": tuple(defect_ids)})
                cols = [d[0] for d in cur.description]
                result_df = pd.DataFrame(cur.fetchall(), columns=cols)
            
            # åˆå¹¶defect_mappingå’ŒæŸ¥è¯¢ç»“æœï¼Œè·å¾—data_name
            final_result = pd.merge(
                defect_mapping, 
                result_df, 
                on='defect_id', 
                how='inner'
            ).drop('defect_id', axis=1)  # åˆ é™¤ä¸­é—´å­—æ®µdefect_id
            
            logger.debug(f"origin_source_idæŸ¥è¯¢: {len(final_result)} æ¡æœ€ç»ˆè®°å½•")
            return final_result
            
        except Exception as e:
            logger.error(f"é€šè¿‡origin_source_idæŸ¥è¯¢å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def _batch_query_strategy(self, polygons: List[Dict]) -> pd.DataFrame:
        """æ‰¹é‡æŸ¥è¯¢ç­–ç•¥ - ä½¿ç”¨hive_cursorè¿æ¥ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰"""
        logger.info(f"ğŸ” ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢ç­–ç•¥å¤„ç† {len(polygons)} ä¸ªpolygon")
        logger.info(f"âš¡ æ¯polygonç‚¹æ•°é™åˆ¶: {self.config.limit_per_polygon:,}")
        
        # å…ˆæµ‹è¯•æ•°æ®åº“è¿æ¥
        logger.info("ğŸ”— æµ‹è¯•æ•°æ®åº“è¿æ¥...")
        try:
            with hive_cursor("dataset_gy1") as cur:
                cur.execute("SELECT 1 as test_connection")
                result = cur.fetchone()
                if result and result[0] == 1:
                    logger.info("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
                else:
                    logger.error("âŒ æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥")
                    return pd.DataFrame()
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return pd.DataFrame()
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šæ£€æŸ¥polygonæ•°é‡
        if len(polygons) > 10:
            logger.info(f"âš ï¸ polygonæ•°é‡è¾ƒå¤š({len(polygons)})ï¼Œåˆ‡æ¢åˆ°åˆ†å—ç­–ç•¥")
            return self._chunked_query_strategy(polygons)
        
        # æ„å»ºä¼˜åŒ–çš„æŸ¥è¯¢
        subqueries = []
        total_estimated_points = len(polygons) * self.config.limit_per_polygon
        
        logger.info(f"ğŸ“ˆ é¢„ä¼°æœ€å¤§æ•°æ®é‡: {total_estimated_points:,} ä¸ªç‚¹")
        
        for i, polygon in enumerate(polygons, 1):
            polygon_id = polygon['id']
            polygon_wkt = polygon['geometry'].wkt
            
            logger.info(f"ğŸ”¸ æ„å»ºæŸ¥è¯¢ {i}/{len(polygons)}: {polygon_id}")
            
            # å»æ‰ORDER BYï¼Œç®€åŒ–å­æŸ¥è¯¢
            subquery = f"""
                (SELECT 
                    dataset_name,
                    timestamp,
                    point_lla,
                    twist_linear,
                    avp_flag,
                    workstage,
                    ST_X(point_lla) as longitude,
                    ST_Y(point_lla) as latitude,
                    '{polygon_id}' as polygon_id
                FROM {self.config.point_table}
                WHERE point_lla IS NOT NULL
                AND timestamp IS NOT NULL
                AND dataset_name IS NOT NULL
                AND ST_Intersects(
                    point_lla,
                    ST_SetSRID(ST_GeomFromText('{polygon_wkt}'), 4326)
                )
                LIMIT {self.config.limit_per_polygon})
            """
            subqueries.append(subquery)
        
        # æ„å»ºå®Œæ•´çš„UNIONæŸ¥è¯¢
        union_query = " UNION ALL ".join(subqueries)
        batch_sql = f"""
            SELECT * FROM (
                {union_query}
            ) AS combined_results
            ORDER BY dataset_name, timestamp
        """
        
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ‰¹é‡SQLæŸ¥è¯¢ï¼ˆä½¿ç”¨hive_cursorï¼‰...")
        logger.debug(f"ğŸ” SQLæŸ¥è¯¢ï¼ˆå‰300å­—ç¬¦ï¼‰: {batch_sql[:300]}...")
        
        start_time = time.time()
        
        try:
            with hive_cursor("dataset_gy1") as cur:
                logger.info("ğŸ“Š æ­£åœ¨æ‰§è¡ŒæŸ¥è¯¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")
                
                if logger.isEnabledFor(logging.DEBUG):
                    logger.debug("=== æ‰§è¡Œæ‰¹é‡æŸ¥è¯¢SQL (dataset_gy1) ===")
                    logger.debug(batch_sql)
                
                # æ‰§è¡ŒæŸ¥è¯¢
                cur.execute(batch_sql)
                rows = cur.fetchall()
                
                query_time = time.time() - start_time
                logger.info(f"âœ… æŸ¥è¯¢å®Œæˆï¼ç”¨æ—¶: {query_time:.2f}s, è·å¾— {len(rows):,} ä¸ªæ•°æ®ç‚¹")
                
                if not rows:
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸äº¤çš„è½¨è¿¹ç‚¹")
                    return pd.DataFrame()
                
                # æ„å»ºDataFrame
                columns = ['dataset_name', 'timestamp', 'point_lla', 'twist_linear', 
                          'avp_flag', 'workstage', 'longitude', 'latitude', 'polygon_id']
                result_df = pd.DataFrame(rows, columns=columns)
                
                logger.info(f"ğŸ“Š æ„å»ºDataFrameå®Œæˆ: {len(result_df)} è¡Œæ•°æ®")
                return result_df
                
        except Exception as sql_error:
            query_time = time.time() - start_time
            logger.error(f"âŒ SQLæ‰§è¡Œå¤±è´¥ (ç”¨æ—¶: {query_time:.2f}s): {sql_error}")
            
            if "timeout" in str(sql_error).lower() or "cancelled" in str(sql_error).lower():
                logger.error(f"â° æŸ¥è¯¢è¶…æ—¶æˆ–è¢«å–æ¶ˆï¼å»ºè®®ï¼š")
                logger.error(f"   1. å‡å°‘ limit_per_polygon (å½“å‰: {self.config.limit_per_polygon})")
                logger.error(f"   2. ç¼©å°polygonèŒƒå›´æˆ–å‡å°‘polygonæ•°é‡")
                logger.error(f"   3. ä½¿ç”¨åˆ†å—æŸ¥è¯¢ç­–ç•¥")
            
            raise
        
        return pd.DataFrame()
    
    def _chunked_query_strategy(self, polygons: List[Dict]) -> pd.DataFrame:
        """åˆ†å—æŸ¥è¯¢ç­–ç•¥ - é€‚åˆå¤§è§„æ¨¡polygon"""
        logger.info(f"ä½¿ç”¨åˆ†å—æŸ¥è¯¢ç­–ç•¥ï¼Œ{len(polygons)} ä¸ªpolygonåˆ†ä¸º {len(polygons)//self.config.chunk_size + 1} å—")
        
        all_results = []
        
        for i in range(0, len(polygons), self.config.chunk_size):
            chunk = polygons[i:i+self.config.chunk_size]
            chunk_num = i // self.config.chunk_size + 1
            logger.info(f"å¤„ç†ç¬¬ {chunk_num} å—: {len(chunk)} ä¸ªpolygon")
            
            # ä½¿ç”¨æ‰¹é‡ç­–ç•¥å¤„ç†å½“å‰å—
            chunk_result = self._batch_query_strategy(chunk)
            if not chunk_result.empty:
                all_results.append(chunk_result)
        
        return pd.concat(all_results, ignore_index=True) if all_results else pd.DataFrame()

    def build_trajectories_from_points(self, points_df: pd.DataFrame) -> Tuple[List[Dict], Dict]:
        """æ™ºèƒ½æ„å»ºè½¨è¿¹çº¿å’Œç»Ÿè®¡ä¿¡æ¯
        
        Args:
            points_df: è½¨è¿¹ç‚¹DataFrame
            
        Returns:
            (è½¨è¿¹åˆ—è¡¨, æ„å»ºç»Ÿè®¡)
        """
        start_time = time.time()
        
        build_stats = {
            'total_points': len(points_df),
            'total_datasets': 0,
            'valid_trajectories': 0,
            'skipped_trajectories': 0,
            'build_time': 0
        }
        
        if points_df.empty:
            logger.warning("æ²¡æœ‰è½¨è¿¹ç‚¹æ•°æ®")
            return [], build_stats
        
        trajectories = []
        
        try:
            # è·å–æ‰€æœ‰æ¶‰åŠçš„data_nameï¼Œå¹¶æŸ¥è¯¢å¯¹åº”çš„scene_id
            unique_data_names = points_df['dataset_name'].unique()
            logger.info(f"æŸ¥è¯¢ {len(unique_data_names)} ä¸ªdata_nameå¯¹åº”çš„scene_id...")
            
            # æŸ¥è¯¢scene_idæ˜ å°„
            scene_id_mappings = self._fetch_scene_ids_from_data_names(unique_data_names.tolist())
            
            # åˆ›å»ºdata_nameåˆ°å„å­—æ®µçš„æ˜ å°„å­—å…¸
            data_name_to_scene_id = {}
            data_name_to_event_id = {}
            data_name_to_event_name = {}
            
            if not scene_id_mappings.empty:
                data_name_to_scene_id = dict(zip(scene_id_mappings['data_name'], scene_id_mappings['scene_id']))
                
                # å¤„ç†event_idå­—æ®µï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
                if 'event_id' in scene_id_mappings.columns:
                    # ç¡®ä¿event_idæ˜¯æ•´æ•°ç±»å‹ï¼Œé¿å…æµ®ç‚¹æ•°æ ¼å¼é—®é¢˜
                    event_ids_cleaned = scene_id_mappings['event_id'].apply(
                        lambda x: int(float(x)) if pd.notna(x) and x != '' else None
                    )
                    data_name_to_event_id = dict(zip(scene_id_mappings['data_name'], event_ids_cleaned))
                
                # å¤„ç†event_nameå­—æ®µï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
                if 'event_name' in scene_id_mappings.columns:
                    data_name_to_event_name = dict(zip(scene_id_mappings['data_name'], scene_id_mappings['event_name']))
                
                logger.info(f"æˆåŠŸæŸ¥è¯¢åˆ° {len(data_name_to_scene_id)} ä¸ªscene_idæ˜ å°„")
                if data_name_to_event_id:
                    logger.info(f"æˆåŠŸæŸ¥è¯¢åˆ° {len(data_name_to_event_id)} ä¸ªevent_idæ˜ å°„")
                if data_name_to_event_name:
                    logger.info(f"æˆåŠŸæŸ¥è¯¢åˆ° {len(data_name_to_event_name)} ä¸ªevent_nameæ˜ å°„")
            else:
                logger.warning("æœªæŸ¥è¯¢åˆ°ä»»ä½•scene_idæ˜ å°„ï¼Œç›¸å…³å­—æ®µå°†ä¸ºç©º")
            
            # æŒ‰dataset_nameåˆ†ç»„å¤„ç†
            grouped = points_df.groupby('dataset_name')
            build_stats['total_datasets'] = len(grouped)
            
            logger.info(f"å¼€å§‹æ„å»ºè½¨è¿¹: {build_stats['total_datasets']} ä¸ªæ•°æ®é›†, {build_stats['total_points']} ä¸ªç‚¹")
            
            # ç§»é™¤event_id_counterï¼Œæ”¹ä¸ºä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢çš„å€¼
            
            for dataset_name, group in grouped:
                # æŒ‰æ—¶é—´æ’åº
                group = group.sort_values('timestamp')
                
                # æ£€æŸ¥ç‚¹æ•°é‡
                if len(group) < self.config.min_points_per_trajectory:
                    build_stats['skipped_trajectories'] += 1
                    logger.debug(f"æ•°æ®é›† {dataset_name} ç‚¹æ•°é‡ä¸è¶³({len(group)})ï¼Œè·³è¿‡")
                    continue
                
                # æå–åæ ‡
                coordinates = list(zip(group['longitude'], group['latitude']))
                
                # æ„å»ºLineStringå‡ ä½•
                trajectory_geom = LineString(coordinates)
                
                # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
                stats = {
                    'dataset_name': dataset_name,
                    'scene_id': data_name_to_scene_id.get(dataset_name, ''),  # ä»æ•°æ®åº“æŸ¥è¯¢è·å–scene_id
                    'event_id': data_name_to_event_id.get(dataset_name, None),  # ä»æ•°æ®åº“æŸ¥è¯¢è·å–event_id
                    'event_name': data_name_to_event_name.get(dataset_name, ''),  # ä»æ•°æ®åº“æŸ¥è¯¢è·å–event_name
                    'start_time': int(group['timestamp'].min()),
                    'end_time': int(group['timestamp'].max()),
                    'duration': int(group['timestamp'].max() - group['timestamp'].min()),
                    'point_count': len(group),
                    'geometry': trajectory_geom,
                    'polygon_ids': list(group['polygon_id'].unique())
                }
                
                # ç§»é™¤event_id_counteré€’å¢ï¼Œå› ä¸ºç°åœ¨ä½¿ç”¨æ•°æ®åº“å€¼
                
                # é€Ÿåº¦ç»Ÿè®¡ï¼ˆå¯é…ç½®ï¼‰
                if self.config.enable_speed_stats and 'twist_linear' in group.columns:
                    speed_data = group['twist_linear'].dropna()
                    if len(speed_data) > 0:
                        stats.update({
                            'avg_speed': round(float(speed_data.mean()), 2),
                            'max_speed': round(float(speed_data.max()), 2),
                            'min_speed': round(float(speed_data.min()), 2),
                            'std_speed': round(float(speed_data.std()) if len(speed_data) > 1 else 0.0, 2)
                        })
                
                # AVPç»Ÿè®¡ï¼ˆå¯é…ç½®ï¼‰
                if self.config.enable_avp_stats and 'avp_flag' in group.columns:
                    avp_data = group['avp_flag'].dropna()
                    if len(avp_data) > 0:
                        stats.update({
                            'avp_ratio': round(float((avp_data == 1).mean()), 3)
                        })
                
                trajectories.append(stats)
                build_stats['valid_trajectories'] += 1
                
                logger.debug(f"æ„å»ºè½¨è¿¹: {dataset_name}, ç‚¹æ•°: {stats['point_count']}, "
                           f"æ—¶é•¿: {stats['duration']}s, polygonæ•°: {len(stats['polygon_ids'])}")
            
            build_stats['build_time'] = time.time() - start_time
            
            logger.info(f"âœ… è½¨è¿¹æ„å»ºå®Œæˆ: {build_stats['valid_trajectories']} æ¡æœ‰æ•ˆè½¨è¿¹, "
                       f"{build_stats['skipped_trajectories']} æ¡è·³è¿‡, "
                       f"ç”¨æ—¶: {build_stats['build_time']:.2f}s")
            
            return trajectories, build_stats
            
        except Exception as e:
            logger.error(f"æ„å»ºè½¨è¿¹å¤±è´¥: {str(e)}")
            return [], build_stats



    def save_trajectories_to_table(self, trajectories: List[Dict], table_name: str) -> Tuple[int, Dict]:
        """é«˜æ•ˆæ‰¹é‡ä¿å­˜è½¨è¿¹æ•°æ®åˆ°æ•°æ®åº“è¡¨
        
        Args:
            trajectories: è½¨è¿¹æ•°æ®åˆ—è¡¨
            table_name: ç›®æ ‡è¡¨å
            
        Returns:
            (ä¿å­˜æˆåŠŸçš„è®°å½•æ•°, ä¿å­˜ç»Ÿè®¡)
        """
        start_time = time.time()
        
        save_stats = {
            'total_trajectories': len(trajectories),
            'saved_records': 0,
            'save_time': 0,
            'table_created': False,
            'batch_count': 0
        }
        
        if not trajectories:
            logger.warning("æ²¡æœ‰è½¨è¿¹æ•°æ®éœ€è¦ä¿å­˜")
            return 0, save_stats
        
        try:
            # åˆ›å»ºè¡¨
            if not self._create_trajectory_table(table_name):
                logger.error("åˆ›å»ºè¡¨å¤±è´¥")
                return 0, save_stats
            
            save_stats['table_created'] = True
            
            # æ‰¹é‡æ’å…¥
            total_saved = 0
            for i in range(0, len(trajectories), self.config.batch_insert_size):
                batch = trajectories[i:i+self.config.batch_insert_size]
                batch_num = i // self.config.batch_insert_size + 1
                
                logger.info(f"ä¿å­˜ç¬¬ {batch_num} æ‰¹: {len(batch)} æ¡è½¨è¿¹")
                
                # å‡†å¤‡GeoDataFrameæ•°æ®
                gdf_data = []
                geometries = []
                
                for traj in batch:
                    # åˆ†ç¦»å‡ ä½•å’Œå±æ€§æ•°æ®
                    row = {k: v for k, v in traj.items() if k != 'geometry'}
                    gdf_data.append(row)
                    geometries.append(traj['geometry'])
                
                # åˆ›å»ºGeoDataFrame
                gdf = gpd.GeoDataFrame(gdf_data, geometry=geometries, crs=4326)
                
                # å¼ºåˆ¶è½¬æ¢event_idä¸ºæ•´æ•°ç±»å‹ï¼Œé¿å…æµ®ç‚¹æ•°æ ¼å¼é—®é¢˜
                if 'event_id' in gdf.columns:
                    # å¤„ç†pandaså°†æ•´æ•°è½¬æ¢ä¸ºæµ®ç‚¹æ•°çš„é—®é¢˜
                    valid_mask = gdf['event_id'].notna()
                    new_event_ids = pd.Series([None] * len(gdf), dtype=object)
                    
                    if valid_mask.any():
                        valid_values = gdf.loc[valid_mask, 'event_id']
                        converted_values = valid_values.apply(lambda x: int(x))
                        new_event_ids.loc[valid_mask] = converted_values
                    
                    gdf['event_id'] = new_event_ids
                
                # è½¬æ¢PostgreSQLæ•°ç»„æ ¼å¼
                if 'polygon_ids' in gdf.columns:
                    # å°†Pythonåˆ—è¡¨è½¬æ¢ä¸ºPostgreSQLæ•°ç»„æ ¼å¼ {item1,item2}
                    gdf['polygon_ids'] = gdf['polygon_ids'].apply(
                        lambda x: '{' + ','.join(str(item) for item in x) + '}' if isinstance(x, list) else x
                    )
                
                # æ‰¹é‡æ’å…¥åˆ°æ•°æ®åº“
                gdf.to_postgis(
                    table_name, 
                    self.engine, 
                    if_exists='append', 
                    index=False
                )
                
                total_saved += len(gdf)
                save_stats['batch_count'] += 1
                
                logger.debug(f"æ‰¹æ¬¡ {batch_num} ä¿å­˜å®Œæˆ: {len(gdf)} æ¡è®°å½•")
            
            save_stats['saved_records'] = total_saved
            save_stats['save_time'] = time.time() - start_time
            
            logger.info(f"âœ… æ•°æ®åº“ä¿å­˜å®Œæˆ: {save_stats['saved_records']} æ¡è½¨è¿¹è®°å½•, "
                       f"{save_stats['batch_count']} ä¸ªæ‰¹æ¬¡, "
                       f"è¡¨: {table_name}, "
                       f"ç”¨æ—¶: {save_stats['save_time']:.2f}s")
            
            return total_saved, save_stats
            
        except Exception as e:
            logger.error(f"ä¿å­˜è½¨è¿¹æ•°æ®å¤±è´¥: {str(e)}")
            return 0, save_stats
    
    def _create_trajectory_table(self, table_name: str) -> bool:
        """åˆ›å»ºè½¨è¿¹ç»“æœè¡¨ï¼ˆäº‹åŠ¡ä¿®å¤ç‰ˆï¼‰"""
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
            check_table_sql = text(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = '{table_name}'
                );
            """)
            
            with self.engine.connect() as conn:
                result = conn.execute(check_table_sql)
                table_exists = result.scalar()
                
                if table_exists:
                    logger.info(f"è¡¨ {table_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                    return True
            
            logger.info(f"åˆ›å»ºé«˜æ€§èƒ½è½¨è¿¹è¡¨: {table_name}")
            
            # åˆ›å»ºè¡¨ç»“æ„ï¼ˆåˆ†æ­¥æ‰§è¡Œé¿å…äº‹åŠ¡å†²çªï¼‰
            create_sql = text(f"""
                CREATE TABLE {table_name} (
                    id serial PRIMARY KEY,
                    dataset_name text NOT NULL,
                    scene_id text,
                    event_id integer,
                    event_name varchar(765),
                    start_time bigint,
                    end_time bigint,
                    duration bigint,
                    point_count integer,
                    avg_speed numeric(8,2),
                    max_speed numeric(8,2),
                    min_speed numeric(8,2),
                    std_speed numeric(8,2),
                    avp_ratio numeric(5,3),
                    polygon_ids text[],
                    created_at timestamp DEFAULT CURRENT_TIMESTAMP
                );
            """)
            
            # æ·»åŠ å‡ ä½•åˆ—
            add_geom_sql = text(f"""
                SELECT AddGeometryColumn('public', '{table_name}', 'geometry', 4326, 'LINESTRING', 2);
            """)
            
            # åˆ›å»ºä¼˜åŒ–ç´¢å¼•
            index_sql = text(f"""
                CREATE INDEX idx_{table_name}_geometry ON {table_name} USING GIST(geometry);
                CREATE INDEX idx_{table_name}_dataset_name ON {table_name}(dataset_name);
                CREATE INDEX idx_{table_name}_scene_id ON {table_name}(scene_id);
                CREATE INDEX idx_{table_name}_event_id ON {table_name}(event_id);
                CREATE INDEX idx_{table_name}_event_name ON {table_name}(event_name);
                CREATE INDEX idx_{table_name}_start_time ON {table_name}(start_time);
                CREATE INDEX idx_{table_name}_point_count ON {table_name}(point_count);
                CREATE INDEX idx_{table_name}_polygon_ids ON {table_name} USING GIN(polygon_ids);
                CREATE INDEX idx_{table_name}_created_at ON {table_name}(created_at);
            """)
            
            # åˆ†æ­¥æ‰§è¡ŒSQLï¼ˆé¿å…äº‹åŠ¡å†²çªï¼‰
            try:
                # æ­¥éª¤1ï¼šåˆ›å»ºè¡¨
                with self.engine.connect() as conn:
                    conn.execute(create_sql)
                    conn.commit()
                logger.debug("âœ… è¡¨ç»“æ„åˆ›å»ºå®Œæˆ")
                
                # æ­¥éª¤2ï¼šæ·»åŠ å‡ ä½•åˆ—
                with self.engine.connect() as conn:
                    conn.execute(add_geom_sql)
                    conn.commit()
                logger.debug("âœ… å‡ ä½•åˆ—æ·»åŠ å®Œæˆ")
                
                # æ­¥éª¤3ï¼šåˆ›å»ºç´¢å¼•
                with self.engine.connect() as conn:
                    conn.execute(index_sql)
                    conn.commit()
                logger.debug("âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ")
                
                logger.info(f"âœ… è½¨è¿¹è¡¨åˆ›å»ºæˆåŠŸ: {table_name}")
                return True
                
            except Exception as e:
                logger.error(f"SQLæ‰§è¡Œå¤±è´¥: {str(e)}")
                # å°è¯•æ¸…ç†éƒ¨åˆ†åˆ›å»ºçš„è¡¨
                try:
                    with self.engine.connect() as conn:
                        conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
                        conn.commit()
                    logger.info(f"å·²æ¸…ç†éƒ¨åˆ†åˆ›å»ºçš„è¡¨: {table_name}")
                except:
                    pass
                raise e
                
        except Exception as e:
            logger.error(f"åˆ›å»ºè½¨è¿¹è¡¨å¤±è´¥: {table_name}, é”™è¯¯: {str(e)}")
            return False

    def process_complete_workflow(
        self,
        geojson_file: str,
        output_table: Optional[str] = None,
        output_geojson: Optional[str] = None
    ) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„é«˜æ€§èƒ½polygonè½¨è¿¹æŸ¥è¯¢å·¥ä½œæµ
        
        Args:
            geojson_file: è¾“å…¥çš„GeoJSONæ–‡ä»¶è·¯å¾„
            output_table: è¾“å‡ºæ•°æ®åº“è¡¨åï¼ˆå¯é€‰ï¼‰
            output_geojson: è¾“å‡ºGeoJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        logger.debug("ğŸ”§ DEBUG: process_complete_workflow æ–¹æ³•è¢«è°ƒç”¨")
        logger.debug(f"ğŸ”§ DEBUG: å®ä¾‹ç±»å‹: {type(self)}")
        logger.debug(f"ğŸ”§ DEBUG: æ–¹æ³•å­˜åœ¨æ€§: {hasattr(self, 'process_complete_workflow')}")
        
        workflow_start = time.time()
        
        # ç»¼åˆç»Ÿè®¡ä¿¡æ¯
        complete_stats = {
            'start_time': datetime.now(),
            'geojson_file': geojson_file,
            'output_table': output_table,
            'output_geojson': output_geojson,
            'config': {
                'batch_threshold': self.config.batch_threshold,
                'chunk_size': self.config.chunk_size,
                'limit_per_polygon': self.config.limit_per_polygon,
                'batch_insert_size': self.config.batch_insert_size
            }
        }
        
        try:
            # é˜¶æ®µ1: åŠ è½½polygon
            logger.info("=" * 60)
            logger.info("ğŸš€ å¼€å§‹é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢å·¥ä½œæµ")
            logger.info("=" * 60)
            
            logger.info(f"ğŸ“ é˜¶æ®µ1: åŠ è½½GeoJSONæ–‡ä»¶: {geojson_file}")
            polygons = load_polygons_from_geojson(geojson_file)
            complete_stats['polygon_count'] = len(polygons)
            
            if not polygons:
                logger.error("âŒ æœªåŠ è½½åˆ°ä»»ä½•polygon")
                complete_stats['error'] = "No polygons loaded"
                return complete_stats
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(polygons)} ä¸ªpolygon")
            
            # é˜¶æ®µ2: é«˜æ•ˆæŸ¥è¯¢è½¨è¿¹ç‚¹
            logger.info(f"ğŸ” é˜¶æ®µ2: æ‰§è¡Œé«˜æ€§èƒ½è½¨è¿¹ç‚¹æŸ¥è¯¢")
            points_df, query_stats = self.query_intersecting_trajectory_points(polygons)
            complete_stats['query_stats'] = query_stats
            
            if points_df.empty:
                logger.warning("âš ï¸ æœªæŸ¥è¯¢åˆ°ä»»ä½•è½¨è¿¹ç‚¹")
                complete_stats['warning'] = "No trajectory points found"
                return complete_stats
            
            logger.info(f"âœ… æŸ¥è¯¢åˆ° {len(points_df)} ä¸ªè½¨è¿¹ç‚¹")
            
            # é˜¶æ®µ3: æ„å»ºè½¨è¿¹
            logger.info(f"ğŸ”§ é˜¶æ®µ3: æ„å»ºè½¨è¿¹çº¿å’Œç»Ÿè®¡ä¿¡æ¯")
            trajectories, build_stats = self.build_trajectories_from_points(points_df)
            complete_stats['build_stats'] = build_stats
            
            if not trajectories:
                logger.warning("âš ï¸ æœªæ„å»ºåˆ°ä»»ä½•è½¨è¿¹")
                complete_stats['warning'] = "No trajectories built"
                return complete_stats
            
            logger.info(f"âœ… æ„å»ºäº† {len(trajectories)} æ¡è½¨è¿¹")
            
            # é˜¶æ®µ4: ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
            if output_table:
                logger.info(f"ğŸ’¾ é˜¶æ®µ4: ä¿å­˜åˆ°æ•°æ®åº“è¡¨: {output_table}")
                inserted_count, save_stats = self.save_trajectories_to_table(trajectories, output_table)
                complete_stats['save_stats'] = save_stats
                logger.info(f"âœ… æˆåŠŸä¿å­˜ {inserted_count} æ¡è½¨è¿¹åˆ°æ•°æ®åº“")
            
            # é˜¶æ®µ5: å¯¼å‡ºåˆ°GeoJSONï¼ˆå¯é€‰ï¼‰
            if output_geojson:
                logger.info(f"ğŸ“„ é˜¶æ®µ5: å¯¼å‡ºåˆ°GeoJSONæ–‡ä»¶: {output_geojson}")
                if export_trajectories_to_geojson(trajectories, output_geojson):
                    complete_stats['geojson_exported'] = True
                    logger.info(f"âœ… æˆåŠŸå¯¼å‡ºè½¨è¿¹åˆ°GeoJSONæ–‡ä»¶")
                else:
                    complete_stats['geojson_export_failed'] = True
                    logger.warning("âš ï¸ GeoJSONå¯¼å‡ºå¤±è´¥")
            
            # æœ€ç»ˆç»Ÿè®¡
            complete_stats['total_trajectories'] = len(trajectories)
            complete_stats['workflow_duration'] = time.time() - workflow_start
            complete_stats['end_time'] = datetime.now()
            complete_stats['success'] = True
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢å·¥ä½œæµå®Œæˆ!")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {complete_stats['workflow_duration']:.2f}s")
            logger.info(f"ğŸ“Š è¾“å‡ºè½¨è¿¹: {complete_stats['total_trajectories']} æ¡")
            logger.info("=" * 60)
            
            return complete_stats
            
        except Exception as e:
            complete_stats['error'] = str(e)
            complete_stats['workflow_duration'] = time.time() - workflow_start
            complete_stats['end_time'] = datetime.now()
            complete_stats['success'] = False
            logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
            return complete_stats

def export_trajectories_to_geojson(trajectories: List[Dict], output_file: str) -> bool:
    """å¯¼å‡ºè½¨è¿¹æ•°æ®åˆ°GeoJSONæ–‡ä»¶
    
    Args:
        trajectories: è½¨è¿¹æ•°æ®åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        å¯¼å‡ºæ˜¯å¦æˆåŠŸ
    """
    if not trajectories:
        logger.warning("æ²¡æœ‰è½¨è¿¹æ•°æ®éœ€è¦å¯¼å‡º")
        return False
    
    try:
        # å‡†å¤‡GeoDataFrameæ•°æ®
        gdf_data = []
        geometries = []
        
        for traj in trajectories:
            # åˆ†ç¦»å‡ ä½•å’Œå±æ€§æ•°æ®
            row = {k: v for k, v in traj.items() if k != 'geometry'}
            # è½¬æ¢polygon_idsä¸ºå­—ç¬¦ä¸²
            if 'polygon_ids' in row:
                row['polygon_ids'] = ','.join(row['polygon_ids'])
            gdf_data.append(row)
            geometries.append(traj['geometry'])
        
        # åˆ›å»ºGeoDataFrame
        gdf = gpd.GeoDataFrame(gdf_data, geometry=geometries, crs=4326)
        
        # å¯¼å‡ºåˆ°GeoJSON
        gdf.to_file(output_file, driver='GeoJSON', encoding='utf-8')
        
        logger.info(f"æˆåŠŸå¯¼å‡º {len(gdf)} æ¡è½¨è¿¹åˆ°æ–‡ä»¶: {output_file}")
        return True
        
    except Exception as e:
        logger.error(f"å¯¼å‡ºè½¨è¿¹æ•°æ®å¤±è´¥: {str(e)}")
        return False

# ä¾¿æ·å‡½æ•°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
def process_polygon_trajectory_query(
    geojson_file: str,
    output_table: Optional[str] = None,
    output_geojson: Optional[str] = None,
    config: Optional[PolygonTrajectoryConfig] = None
) -> Dict:
    """é«˜æ€§èƒ½polygonè½¨è¿¹æŸ¥è¯¢å®Œæ•´æµç¨‹
    
    Args:
        geojson_file: è¾“å…¥çš„GeoJSONæ–‡ä»¶è·¯å¾„
        output_table: è¾“å‡ºæ•°æ®åº“è¡¨åï¼ˆå¯é€‰ï¼‰
        output_geojson: è¾“å‡ºGeoJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        config: è‡ªå®šä¹‰é…ç½®ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    # ä½¿ç”¨é«˜æ€§èƒ½æŸ¥è¯¢å™¨
    query_config = config or PolygonTrajectoryConfig()
    processor = HighPerformancePolygonTrajectoryQuery(query_config)
    
    # è°ƒè¯•ä¿¡æ¯ï¼šéªŒè¯å®ä¾‹å’Œæ–¹æ³•
    logger.debug(f"ğŸ”§ DEBUG: åˆ›å»ºçš„å¤„ç†å™¨ç±»å‹: {type(processor)}")
    logger.debug(f"ğŸ”§ DEBUG: å¤„ç†å™¨å¯ç”¨æ–¹æ³•: {[method for method in dir(processor) if not method.startswith('_')]}")
    logger.debug(f"ğŸ”§ DEBUG: process_complete_workflow æ–¹æ³•æ˜¯å¦å­˜åœ¨: {hasattr(processor, 'process_complete_workflow')}")
    
    if not hasattr(processor, 'process_complete_workflow'):
        logger.error("âŒ CRITICAL: process_complete_workflow æ–¹æ³•ä¸å­˜åœ¨!")
        logger.error(f"âŒ å¯ç”¨æ–¹æ³•: {[method for method in dir(processor) if callable(getattr(processor, method)) and not method.startswith('_')]}")
        raise AttributeError("HighPerformancePolygonTrajectoryQuery object has no attribute 'process_complete_workflow'")
    
    logger.debug("ğŸ”§ DEBUG: å³å°†è°ƒç”¨ process_complete_workflow æ–¹æ³•")
    
    return processor.process_complete_workflow(
        geojson_file=geojson_file,
        output_table=output_table,
        output_geojson=output_geojson
    )

def main():
    """ä¸»å‡½æ•°ï¼ŒCLIå…¥å£ç‚¹"""
    parser = argparse.ArgumentParser(
        description='é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢æ¨¡å— - æ‰¹é‡æŸ¥æ‰¾ä¸polygonç›¸äº¤çš„è½¨è¿¹æ•°æ®',
        epilog="""
é«˜æ€§èƒ½ç‰¹æ€§:
  â€¢ æ™ºèƒ½æ‰¹é‡æŸ¥è¯¢ç­–ç•¥ï¼šâ‰¤50ä¸ªpolygonä½¿ç”¨UNION ALLï¼Œ>50ä¸ªpolygonä½¿ç”¨åˆ†å—æŸ¥è¯¢
  â€¢ ä¼˜åŒ–çš„æ•°æ®åº“å†™å…¥ï¼šæ‰¹é‡æ’å…¥ï¼Œäº‹åŠ¡ä¿æŠ¤ï¼Œå¤šé‡ç´¢å¼•
  â€¢ è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡ï¼šæŸ¥è¯¢æ—¶é—´ã€æ„å»ºæ—¶é—´ã€å¤„ç†é€Ÿåº¦ç­‰

ç¤ºä¾‹:
  # åŸºç¡€ç”¨æ³•ï¼šæŸ¥è¯¢è½¨è¿¹å¹¶ä¿å­˜åˆ°æ•°æ®åº“è¡¨
  python -m spdatalab.dataset.polygon_trajectory_query --input polygons.geojson --table my_trajectories
  
  # é«˜æ€§èƒ½æ¨¡å¼ï¼šè‡ªå®šä¹‰æ‰¹é‡æŸ¥è¯¢å‚æ•°
  python -m spdatalab.dataset.polygon_trajectory_query --input polygons.geojson --table my_trajectories \\
    --batch-threshold 30 --chunk-size 15 --batch-insert 500
  
  # åŒæ—¶ä¿å­˜åˆ°æ•°æ®åº“å’Œæ–‡ä»¶
  python -m spdatalab.dataset.polygon_trajectory_query --input polygons.geojson \\
    --table my_trajectories --output trajectories.geojson
  
  # è°ƒæ•´è½¨è¿¹ç‚¹é™åˆ¶å’Œç»Ÿè®¡é€‰é¡¹
  python -m spdatalab.dataset.polygon_trajectory_query --input polygons.geojson \\
    --table my_trajectories --limit 20000 --no-speed-stats --verbose
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--input', required=True, help='è¾“å…¥GeoJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--table', help='è¾“å‡ºæ•°æ®åº“è¡¨åï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--output', help='è¾“å‡ºGeoJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    
    # æ€§èƒ½ä¼˜åŒ–å‚æ•°
    parser.add_argument('--batch-threshold', type=int, default=50, 
                       help='æ‰¹é‡æŸ¥è¯¢vsåˆ†å—æŸ¥è¯¢çš„é˜ˆå€¼ (é»˜è®¤: 50)')
    parser.add_argument('--chunk-size', type=int, default=20,
                       help='åˆ†å—æŸ¥è¯¢çš„å—å¤§å° (é»˜è®¤: 20)')
    parser.add_argument('--limit', type=int, default=10000, 
                       help='æ¯ä¸ªpolygonçš„è½¨è¿¹ç‚¹é™åˆ¶æ•°é‡ (é»˜è®¤: 10000)')
    parser.add_argument('--batch-insert', type=int, default=1000,
                       help='æ‰¹é‡æ’å…¥æ•°æ®åº“çš„æ‰¹æ¬¡å¤§å° (é»˜è®¤: 1000)')
    parser.add_argument('--timeout', type=int, default=300,
                       help='æŸ¥è¯¢è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰(é»˜è®¤: 300)')
    
    # åŠŸèƒ½é€‰é¡¹
    parser.add_argument('--min-points', type=int, default=2,
                       help='æ„å»ºè½¨è¿¹çš„æœ€å°ç‚¹æ•° (é»˜è®¤: 2)')
    parser.add_argument('--no-speed-stats', action='store_true',
                       help='ç¦ç”¨é€Ÿåº¦ç»Ÿè®¡è®¡ç®—')
    parser.add_argument('--no-avp-stats', action='store_true',
                       help='ç¦ç”¨AVPç»Ÿè®¡è®¡ç®—')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†æ—¥å¿—')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if not args.table and not args.output:
        parser.error("å¿…é¡»æŒ‡å®š --table æˆ– --output ä¸­çš„è‡³å°‘ä¸€ä¸ª")
    
    # é…ç½®æ—¥å¿—
    level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not Path(args.input).exists():
            logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
            return 1
        
        # æ„å»ºé…ç½®
        config = PolygonTrajectoryConfig(
            batch_threshold=args.batch_threshold,
            chunk_size=args.chunk_size,
            limit_per_polygon=args.limit,
            batch_insert_size=args.batch_insert,
            min_points_per_trajectory=args.min_points,
            enable_speed_stats=not args.no_speed_stats,
            enable_avp_stats=not args.no_avp_stats
        )
        
        # è¾“å‡ºé…ç½®ä¿¡æ¯
        logger.info("ğŸ”§ é…ç½®å‚æ•°:")
        logger.info(f"   â€¢ æ‰¹é‡æŸ¥è¯¢é˜ˆå€¼: {config.batch_threshold}")
        logger.info(f"   â€¢ åˆ†å—å¤§å°: {config.chunk_size}")
        logger.info(f"   â€¢ æ¯polygonè½¨è¿¹ç‚¹é™åˆ¶: {config.limit_per_polygon:,}")
        logger.info(f"   â€¢ æ‰¹é‡æ’å…¥å¤§å°: {config.batch_insert_size}")
        logger.info(f"   â€¢ æœ€å°è½¨è¿¹ç‚¹æ•°: {config.min_points_per_trajectory}")
        logger.info(f"   â€¢ é€Ÿåº¦ç»Ÿè®¡: {'å¯ç”¨' if config.enable_speed_stats else 'ç¦ç”¨'}")
        logger.info(f"   â€¢ AVPç»Ÿè®¡: {'å¯ç”¨' if config.enable_avp_stats else 'ç¦ç”¨'}")
        
        # æ‰§è¡Œå¤„ç†
        stats = process_polygon_trajectory_query(
            geojson_file=args.input,
            output_table=args.table,
            output_geojson=args.output,
            config=config
        )
        
        # æ£€æŸ¥å¤„ç†ç»“æœ
        if 'error' in stats:
            logger.error(f"âŒ å¤„ç†é”™è¯¯: {stats['error']}")
            return 1
        
        if not stats.get('success', False):
            logger.error("âŒ å¤„ç†æœªæˆåŠŸå®Œæˆ")
            return 1
        
        # æˆåŠŸå®Œæˆ
        logger.info("ğŸ‰ æ‰€æœ‰å¤„ç†æˆåŠŸå®Œæˆï¼")
        
        # ç¡®å®šè¿”å›ä»£ç 
        query_stats = stats.get('query_stats', {})
        build_stats = stats.get('build_stats', {})
        
        has_results = (
            query_stats.get('total_points', 0) > 0 and 
            build_stats.get('valid_trajectories', 0) > 0
        )
        
        return 0 if has_results else 1
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        return 1

if __name__ == '__main__':
    sys.exit(main()) 