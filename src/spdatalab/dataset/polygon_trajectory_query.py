"""é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢æ¨¡å—

åŸºäºspatial_join_production.pyçš„ä¼˜åŒ–ç­–ç•¥ï¼š
- å°è§„æ¨¡ï¼ˆâ‰¤50ä¸ªpolygonï¼‰ï¼šæ‰¹é‡æŸ¥è¯¢ï¼ˆUNION ALLï¼‰- æœ€å¿«
- å¤§è§„æ¨¡ï¼ˆ>50ä¸ªpolygonï¼‰ï¼šåˆ†å—æ‰¹é‡æŸ¥è¯¢ - æœ€ç¨³å®š
- é«˜æ•ˆæ•°æ®åº“å†™å…¥å’Œè½¨è¿¹æ„å»º

åŠŸèƒ½ï¼š
1. è¯»å–GeoJSONæ–‡ä»¶ä¸­çš„polygonï¼ˆæ”¯æŒå¤šä¸ªï¼‰
2. é«˜æ•ˆæ‰¹é‡æŸ¥è¯¢ä¸polygonç›¸äº¤çš„è½¨è¿¹ç‚¹
3. æ™ºèƒ½åˆ†ç»„æ„å»ºè½¨è¿¹çº¿å’Œç»Ÿè®¡ä¿¡æ¯
4. æ‰¹é‡å†™å…¥æ•°æ®åº“ï¼Œä¼˜åŒ–æ€§èƒ½
"""

import argparse
import json
import logging
import sys
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import warnings

import geopandas as gpd
import pandas as pd
from shapely.geometry import shape, LineString, Point
from sqlalchemy import text, create_engine
from spdatalab.common.io_hive import hive_cursor

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)

# æ•°æ®åº“é…ç½®
LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"
POINT_TABLE = "public.ddi_data_points"

# æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)

@dataclass
class PolygonTrajectoryConfig:
    """Polygonè½¨è¿¹æŸ¥è¯¢é…ç½®"""
    local_dsn: str = LOCAL_DSN
    point_table: str = POINT_TABLE
    
    # æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–é…ç½®
    batch_threshold: int = 50          # æ‰¹é‡æŸ¥è¯¢vsåˆ†å—æŸ¥è¯¢çš„é˜ˆå€¼
    chunk_size: int = 20               # åˆ†å—å¤§å°
    limit_per_polygon: int = 10000     # æ¯ä¸ªpolygonçš„è½¨è¿¹ç‚¹é™åˆ¶
    batch_insert_size: int = 1000      # æ‰¹é‡æ’å…¥å¤§å°
    
    # æŸ¥è¯¢ä¼˜åŒ–é…ç½®
    enable_spatial_index: bool = True  # å¯ç”¨ç©ºé—´ç´¢å¼•ä¼˜åŒ–
    query_timeout: int = 300           # æŸ¥è¯¢è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    # è½¨è¿¹æ„å»ºé…ç½®
    min_points_per_trajectory: int = 2 # æ„å»ºè½¨è¿¹çš„æœ€å°ç‚¹æ•°
    enable_speed_stats: bool = True    # å¯ç”¨é€Ÿåº¦ç»Ÿè®¡
    enable_avp_stats: bool = True      # å¯ç”¨AVPç»Ÿè®¡
    
    # å®Œæ•´è½¨è¿¹è·å–é…ç½®
    fetch_complete_trajectories: bool = True  # æ˜¯å¦è·å–å®Œæ•´è½¨è¿¹ï¼ˆè€Œéä»…å¤šè¾¹å½¢å†…çš„ç‰‡æ®µï¼‰

def load_polygons_from_geojson(file_path: str) -> List[Dict]:
    """ä»GeoJSONæ–‡ä»¶åŠ è½½polygon
    
    Args:
        file_path: GeoJSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        polygonåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«geometryå’Œproperties
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            geojson_data = json.load(f)
        
        polygons = []
        
        if geojson_data.get('type') == 'FeatureCollection':
            # FeatureCollectionæ ¼å¼
            for i, feature in enumerate(geojson_data.get('features', [])):
                if feature.get('geometry', {}).get('type') in ['Polygon', 'MultiPolygon']:
                    polygon_info = {
                        'id': feature.get('properties', {}).get('id', f'polygon_{i}'),
                        'geometry': shape(feature['geometry']),
                        'properties': feature.get('properties', {})
                    }
                    polygons.append(polygon_info)
        elif geojson_data.get('type') in ['Polygon', 'MultiPolygon']:
            # å•ä¸ªå‡ ä½•å¯¹è±¡
            polygon_info = {
                'id': 'polygon_0',
                'geometry': shape(geojson_data),
                'properties': {}
            }
            polygons.append(polygon_info)
        else:
            logger.error(f"ä¸æ”¯æŒçš„GeoJSONæ ¼å¼: {geojson_data.get('type')}")
            return []
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(polygons)} ä¸ªpolygon")
        for polygon in polygons:
            logger.debug(f"Polygon {polygon['id']}: {polygon['geometry'].geom_type}")
        
        return polygons
        
    except Exception as e:
        logger.error(f"åŠ è½½GeoJSONæ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
        raise

class HighPerformancePolygonTrajectoryQuery:
    """é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢å™¨"""
    
    def __init__(self, config: Optional[PolygonTrajectoryConfig] = None):
        self.config = config or PolygonTrajectoryConfig()
        self.engine = create_engine(
            self.config.local_dsn, 
            future=True,
            connect_args={"client_encoding": "utf8"}
        )
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºç±»çš„å¯ç”¨æ–¹æ³•
        logger.debug(f"ğŸ”§ HighPerformancePolygonTrajectoryQuery åˆå§‹åŒ–å®Œæˆ")
        logger.debug(f"ğŸ”§ å¯ç”¨æ–¹æ³•: {[method for method in dir(self) if not method.startswith('_')]}")
        logger.debug(f"ğŸ”§ process_complete_workflow æ–¹æ³•å­˜åœ¨: {hasattr(self, 'process_complete_workflow')}")
    
    def query_intersecting_trajectory_points(self, polygons: List[Dict]) -> Tuple[pd.DataFrame, Dict]:
        """é«˜æ•ˆæ‰¹é‡æŸ¥è¯¢ä¸polygonç›¸äº¤çš„è½¨è¿¹ç‚¹
        
        Args:
            polygons: polygonåˆ—è¡¨
            
        Returns:
            (è½¨è¿¹ç‚¹DataFrame, æ€§èƒ½ç»Ÿè®¡)
        """
        start_time = time.time()
        
        # æ€§èƒ½ç»Ÿè®¡
        stats = {
            'polygon_count': len(polygons),
            'strategy': None,
            'chunk_size': None,
            'query_time': 0,
            'total_points': 0,
            'unique_datasets': 0,
            'points_per_polygon': 0
        }
        
        if not polygons:
            logger.warning("æ²¡æœ‰polygonæ•°æ®")
            return pd.DataFrame(), stats
        
        logger.info(f"å¼€å§‹æ‰¹é‡æŸ¥è¯¢ {len(polygons)} ä¸ªpolygonçš„è½¨è¿¹ç‚¹")
        
        # é€‰æ‹©æœ€ä¼˜æŸ¥è¯¢ç­–ç•¥
        if len(polygons) <= self.config.batch_threshold:
            stats['strategy'] = 'batch_query'
            result_df = self._batch_query_strategy(polygons)
        else:
            stats['strategy'] = 'chunked_query'
            stats['chunk_size'] = self.config.chunk_size
            result_df = self._chunked_query_strategy(polygons)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats['query_time'] = time.time() - start_time
        stats['total_points'] = len(result_df)
        
        if not result_df.empty:
            stats['unique_datasets'] = result_df['dataset_name'].nunique()
            stats['points_per_polygon'] = stats['total_points'] / stats['polygon_count']
            
            # å¦‚æœå¯ç”¨å®Œæ•´è½¨è¿¹è·å–ï¼Œåˆ™è·å–å®Œæ•´è½¨è¿¹æ•°æ®
            if self.config.fetch_complete_trajectories:
                logger.info(f"ğŸ”„ è·å–å®Œæ•´è½¨è¿¹æ•°æ®...")
                complete_result_df, complete_stats = self._fetch_complete_trajectories(result_df)
                
                if not complete_result_df.empty:
                    result_df = complete_result_df
                    stats.update(complete_stats)
                    stats['total_points'] = len(result_df)
                    stats['complete_trajectories_fetched'] = True
                    logger.info(f"âœ… å®Œæ•´è½¨è¿¹è·å–å®Œæˆ: {stats['total_points']} ä¸ªè½¨è¿¹ç‚¹")
                else:
                    stats['complete_trajectories_fetched'] = False
                    logger.warning("âš ï¸ å®Œæ•´è½¨è¿¹è·å–å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®")
            
            logger.info(f"âœ… æŸ¥è¯¢å®Œæˆ: {stats['total_points']} ä¸ªè½¨è¿¹ç‚¹, "
                       f"{stats['unique_datasets']} ä¸ªæ•°æ®é›†, "
                       f"ç­–ç•¥: {stats['strategy']}, "
                       f"ç”¨æ—¶: {stats['query_time']:.2f}s")
        else:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½•ç›¸äº¤çš„è½¨è¿¹ç‚¹")
        
        return result_df, stats
    
    def _fetch_complete_trajectories(self, intersection_result_df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
        """è·å–å®Œæ•´è½¨è¿¹æ•°æ®ï¼ˆåŸºäºç›¸äº¤ç»“æœä¸­çš„data_nameï¼‰
        
        Args:
            intersection_result_df: å¤šè¾¹å½¢ç›¸äº¤ç»“æœDataFrame
            
        Returns:
            (å®Œæ•´è½¨è¿¹DataFrame, ç»Ÿè®¡ä¿¡æ¯)
        """
        start_time = time.time()
        
        # ç»Ÿè®¡ä¿¡æ¯
        complete_stats = {
            'complete_query_time': 0,
            'original_datasets': 0,
            'original_points': 0,
            'complete_datasets': 0,
            'complete_points': 0
        }
        
        if intersection_result_df.empty:
            logger.warning("ç›¸äº¤ç»“æœä¸ºç©ºï¼Œæ— æ³•è·å–å®Œæ•´è½¨è¿¹")
            return pd.DataFrame(), complete_stats
        
        # è·å–æ‰€æœ‰æ¶‰åŠçš„data_name
        unique_data_names = intersection_result_df['dataset_name'].unique()
        complete_stats['original_datasets'] = len(unique_data_names)
        complete_stats['original_points'] = len(intersection_result_df)
        
        logger.info(f"ğŸ“‹ éœ€è¦è·å–å®Œæ•´è½¨è¿¹çš„æ•°æ®é›†: {len(unique_data_names)} ä¸ª")
        
        try:
            # æ„å»ºæŸ¥è¯¢æ‰€æœ‰å®Œæ•´è½¨è¿¹çš„SQL
            data_names_tuple = tuple(unique_data_names)
            
            complete_trajectory_sql = f"""
                SELECT 
                    dataset_name,
                    timestamp,
                    point_lla,
                    twist_linear,
                    avp_flag,
                    workstage,
                    ST_X(point_lla) as longitude,
                    ST_Y(point_lla) as latitude
                FROM {self.config.point_table}
                WHERE dataset_name IN %(data_names)s
                AND point_lla IS NOT NULL
                AND timestamp IS NOT NULL
                ORDER BY dataset_name, timestamp
            """
            
            logger.info(f"ğŸš€ æ‰§è¡Œå®Œæ•´è½¨è¿¹æŸ¥è¯¢...")
            
            with hive_cursor("dataset_gy1") as cur:
                cur.execute(complete_trajectory_sql, {"data_names": data_names_tuple})
                
                # è·å–åˆ—åå’Œæ•°æ®
                cols = [d[0] for d in cur.description]
                rows = cur.fetchall()
                
                if rows:
                    complete_df = pd.DataFrame(rows, columns=cols)
                    
                    # ä¸ºå®Œæ•´è½¨è¿¹æ•°æ®æ·»åŠ polygon_idä¿¡æ¯
                    # åŸºäºåŸå§‹ç›¸äº¤ç»“æœåˆ›å»ºdata_nameåˆ°polygon_idçš„æ˜ å°„
                    dataset_polygon_mapping = {}
                    for _, row in intersection_result_df.iterrows():
                        dataset_name = row['dataset_name']
                        polygon_id = row.get('polygon_id', 'unknown')
                        
                        if dataset_name not in dataset_polygon_mapping:
                            dataset_polygon_mapping[dataset_name] = []
                        if polygon_id not in dataset_polygon_mapping[dataset_name]:
                            dataset_polygon_mapping[dataset_name].append(polygon_id)
                    
                    # ä¸ºå®Œæ•´è½¨è¿¹æ·»åŠ polygon_idä¿¡æ¯
                    complete_df['polygon_id'] = complete_df['dataset_name'].map(
                        lambda x: dataset_polygon_mapping.get(x, ['unknown'])[0]
                    )
                    
                    # æ·»åŠ scene_idä¿¡æ¯ï¼ˆå‚è€ƒbboxæ¨¡å—é€»è¾‘ï¼‰
                    logger.info(f"ğŸ” è·å–scene_idå…ƒæ•°æ®...")
                    unique_data_names_list = complete_df['dataset_name'].unique().tolist()
                    scene_meta_df = self._fetch_scene_meta(unique_data_names_list)
                    
                    if not scene_meta_df.empty:
                        # åˆ›å»ºdata_nameåˆ°scene_idçš„æ˜ å°„
                        data_name_to_scene_id = dict(zip(scene_meta_df['data_name'], scene_meta_df['scene_id']))
                        
                        # ä¸ºå®Œæ•´è½¨è¿¹æ·»åŠ scene_idä¿¡æ¯
                        complete_df['scene_id'] = complete_df['dataset_name'].map(
                            lambda x: data_name_to_scene_id.get(x, 'unknown')
                        )
                        
                        logger.info(f"âœ… æˆåŠŸæ·»åŠ scene_idä¿¡æ¯: {len(scene_meta_df)} ä¸ªæ˜ å°„")
                    else:
                        # å¦‚æœæ— æ³•è·å–scene_idå…ƒæ•°æ®ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
                        complete_df['scene_id'] = 'unknown'
                        logger.warning("âš ï¸ æ— æ³•è·å–scene_idå…ƒæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    
                    complete_stats['complete_datasets'] = complete_df['dataset_name'].nunique()
                    complete_stats['complete_points'] = len(complete_df)
                    complete_stats['complete_query_time'] = time.time() - start_time
                    
                    logger.info(f"âœ… å®Œæ•´è½¨è¿¹æŸ¥è¯¢æˆåŠŸ: {len(complete_df)} ä¸ªç‚¹, "
                               f"{complete_df['dataset_name'].nunique()} ä¸ªæ•°æ®é›†, "
                               f"ç”¨æ—¶: {complete_stats['complete_query_time']:.2f}s")
                    
                    return complete_df, complete_stats
                else:
                    logger.warning("å®Œæ•´è½¨è¿¹æŸ¥è¯¢æ— ç»“æœ")
                    return pd.DataFrame(), complete_stats
                    
        except Exception as e:
            logger.error(f"è·å–å®Œæ•´è½¨è¿¹å¤±è´¥: {str(e)}")
            return pd.DataFrame(), complete_stats

    def _fetch_scene_meta(self, data_names: List[str]) -> pd.DataFrame:
        """è·å–data_nameå¯¹åº”çš„scene_idå…ƒæ•°æ®ï¼ˆå‚è€ƒbboxæ¨¡å—é€»è¾‘ï¼‰
        
        Args:
            data_names: data_nameåˆ—è¡¨
            
        Returns:
            åŒ…å«scene_idå’Œdata_nameæ˜ å°„çš„DataFrame
        """
        if not data_names:
            return pd.DataFrame()
        
        try:
            # å‚è€ƒbboxæ¨¡å—çš„fetch_metaé€»è¾‘ï¼Œé€šè¿‡data_nameåå‘æŸ¥è¯¢scene_id
            sql = ("SELECT id AS scene_id, origin_name AS data_name, event_id, city_id, timestamp "
                   "FROM transform.ods_t_data_fragment_datalake WHERE origin_name IN %(data_names)s")
            
            with hive_cursor() as cur:
                cur.execute(sql, {"data_names": tuple(data_names)})
                cols = [d[0] for d in cur.description]
                meta_df = pd.DataFrame(cur.fetchall(), columns=cols)
                
            logger.debug(f"è·å–åˆ° {len(meta_df)} ä¸ªdata_nameçš„scene_idæ˜ å°„")
            return meta_df
            
        except Exception as e:
            logger.error(f"è·å–scene_idå…ƒæ•°æ®å¤±è´¥: {str(e)}")
            return pd.DataFrame()

    def build_trajectories_from_points(self, points_df: pd.DataFrame) -> Tuple[List[Dict], Dict]:
        """æ™ºèƒ½æ„å»ºè½¨è¿¹çº¿å’Œç»Ÿè®¡ä¿¡æ¯
        
        Args:
            points_df: è½¨è¿¹ç‚¹DataFrame
            
        Returns:
            (è½¨è¿¹åˆ—è¡¨, æ„å»ºç»Ÿè®¡)
        """
        start_time = time.time()
        
        build_stats = {
            'total_points': len(points_df),
            'total_datasets': 0,
            'valid_trajectories': 0,
            'skipped_trajectories': 0,
            'build_time': 0
        }
        
        if points_df.empty:
            logger.warning("æ²¡æœ‰è½¨è¿¹ç‚¹æ•°æ®")
            return [], build_stats
        
        trajectories = []
        
        try:
            # æŒ‰dataset_nameåˆ†ç»„å¤„ç†
            grouped = points_df.groupby('dataset_name')
            build_stats['total_datasets'] = len(grouped)
            
            logger.info(f"å¼€å§‹æ„å»ºè½¨è¿¹: {build_stats['total_datasets']} ä¸ªæ•°æ®é›†, {build_stats['total_points']} ä¸ªç‚¹")
            
            for dataset_name, group in grouped:
                # æŒ‰æ—¶é—´æ’åº
                group = group.sort_values('timestamp')
                
                # æ£€æŸ¥ç‚¹æ•°é‡
                if len(group) < self.config.min_points_per_trajectory:
                    build_stats['skipped_trajectories'] += 1
                    logger.debug(f"æ•°æ®é›† {dataset_name} ç‚¹æ•°é‡ä¸è¶³({len(group)})ï¼Œè·³è¿‡")
                    continue
                
                # æå–åæ ‡
                coordinates = list(zip(group['longitude'], group['latitude']))
                
                # æ„å»ºLineStringå‡ ä½•
                trajectory_geom = LineString(coordinates)
                
                # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
                stats = {
                    'dataset_name': dataset_name,
                    'start_time': int(group['timestamp'].min()),
                    'end_time': int(group['timestamp'].max()),
                    'duration': int(group['timestamp'].max() - group['timestamp'].min()),
                    'point_count': len(group),
                    'geometry': trajectory_geom,
                    'polygon_ids': list(group['polygon_id'].unique())
                }
                
                # æ·»åŠ scene_idä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if 'scene_id' in group.columns:
                    unique_scene_ids = group['scene_id'].unique()
                    # å¦‚æœä¸€ä¸ªè½¨è¿¹æœ‰å¤šä¸ªscene_idï¼Œå–ç¬¬ä¸€ä¸ªï¼ˆé€šå¸¸åº”è¯¥åªæœ‰ä¸€ä¸ªï¼‰
                    stats['scene_id'] = unique_scene_ids[0] if len(unique_scene_ids) > 0 else 'unknown'
                else:
                    stats['scene_id'] = 'unknown'
                
                # é€Ÿåº¦ç»Ÿè®¡ï¼ˆå¯é…ç½®ï¼‰
                if self.config.enable_speed_stats and 'twist_linear' in group.columns:
                    speed_data = group['twist_linear'].dropna()
                    if len(speed_data) > 0:
                        stats.update({
                            'avg_speed': round(float(speed_data.mean()), 2),
                            'max_speed': round(float(speed_data.max()), 2),
                            'min_speed': round(float(speed_data.min()), 2),
                            'std_speed': round(float(speed_data.std()) if len(speed_data) > 1 else 0.0, 2)
                        })
                
                # AVPç»Ÿè®¡ï¼ˆå¯é…ç½®ï¼‰
                if self.config.enable_avp_stats and 'avp_flag' in group.columns:
                    avp_data = group['avp_flag'].dropna()
                    if len(avp_data) > 0:
                        stats.update({
                            'avp_ratio': round(float((avp_data == 1).mean()), 3)
                        })
                
                trajectories.append(stats)
                build_stats['valid_trajectories'] += 1
                
                logger.debug(f"æ„å»ºè½¨è¿¹: {dataset_name}, ç‚¹æ•°: {stats['point_count']}, "
                           f"æ—¶é•¿: {stats['duration']}s, polygonæ•°: {len(stats['polygon_ids'])}")
            
            build_stats['build_time'] = time.time() - start_time
            
            logger.info(f"âœ… è½¨è¿¹æ„å»ºå®Œæˆ: {build_stats['valid_trajectories']} æ¡æœ‰æ•ˆè½¨è¿¹, "
                       f"{build_stats['skipped_trajectories']} æ¡è·³è¿‡, "
                       f"ç”¨æ—¶: {build_stats['build_time']:.2f}s")
            
            return trajectories, build_stats
            
        except Exception as e:
            logger.error(f"æ„å»ºè½¨è¿¹å¤±è´¥: {str(e)}")
            return [], build_stats



    def save_trajectories_to_table(self, trajectories: List[Dict], table_name: str) -> Tuple[int, Dict]:
        """é«˜æ•ˆæ‰¹é‡ä¿å­˜è½¨è¿¹æ•°æ®åˆ°æ•°æ®åº“è¡¨
        
        Args:
            trajectories: è½¨è¿¹æ•°æ®åˆ—è¡¨
            table_name: ç›®æ ‡è¡¨å
            
        Returns:
            (ä¿å­˜æˆåŠŸçš„è®°å½•æ•°, ä¿å­˜ç»Ÿè®¡)
        """
        start_time = time.time()
        
        save_stats = {
            'total_trajectories': len(trajectories),
            'saved_records': 0,
            'save_time': 0,
            'table_created': False,
            'batch_count': 0
        }
        
        if not trajectories:
            logger.warning("æ²¡æœ‰è½¨è¿¹æ•°æ®éœ€è¦ä¿å­˜")
            return 0, save_stats
        
        try:
            # åˆ›å»ºè¡¨
            if not self._create_trajectory_table(table_name):
                logger.error("åˆ›å»ºè¡¨å¤±è´¥")
                return 0, save_stats
            
            save_stats['table_created'] = True
            
            # æ‰¹é‡æ’å…¥
            total_saved = 0
            for i in range(0, len(trajectories), self.config.batch_insert_size):
                batch = trajectories[i:i+self.config.batch_insert_size]
                batch_num = i // self.config.batch_insert_size + 1
                
                logger.info(f"ä¿å­˜ç¬¬ {batch_num} æ‰¹: {len(batch)} æ¡è½¨è¿¹")
                
                # å‡†å¤‡GeoDataFrameæ•°æ®
                gdf_data = []
                geometries = []
                
                for traj in batch:
                    # åˆ†ç¦»å‡ ä½•å’Œå±æ€§æ•°æ®
                    row = {k: v for k, v in traj.items() if k != 'geometry'}
                    gdf_data.append(row)
                    geometries.append(traj['geometry'])
                
                # åˆ›å»ºGeoDataFrame
                gdf = gpd.GeoDataFrame(gdf_data, geometry=geometries, crs=4326)
                
                # è½¬æ¢PostgreSQLæ•°ç»„æ ¼å¼
                if 'polygon_ids' in gdf.columns:
                    # å°†Pythonåˆ—è¡¨è½¬æ¢ä¸ºPostgreSQLæ•°ç»„æ ¼å¼ {item1,item2}
                    gdf['polygon_ids'] = gdf['polygon_ids'].apply(
                        lambda x: '{' + ','.join(str(item) for item in x) + '}' if isinstance(x, list) else x
                    )
                
                # æ‰¹é‡æ’å…¥åˆ°æ•°æ®åº“
                gdf.to_postgis(
                    table_name, 
                    self.engine, 
                    if_exists='append', 
                    index=False
                )
                
                total_saved += len(gdf)
                save_stats['batch_count'] += 1
                
                logger.debug(f"æ‰¹æ¬¡ {batch_num} ä¿å­˜å®Œæˆ: {len(gdf)} æ¡è®°å½•")
            
            save_stats['saved_records'] = total_saved
            save_stats['save_time'] = time.time() - start_time
            
            logger.info(f"âœ… æ•°æ®åº“ä¿å­˜å®Œæˆ: {save_stats['saved_records']} æ¡è½¨è¿¹è®°å½•, "
                       f"{save_stats['batch_count']} ä¸ªæ‰¹æ¬¡, "
                       f"è¡¨: {table_name}, "
                       f"ç”¨æ—¶: {save_stats['save_time']:.2f}s")
            
            return total_saved, save_stats
            
        except Exception as e:
            logger.error(f"ä¿å­˜è½¨è¿¹æ•°æ®å¤±è´¥: {str(e)}")
            return 0, save_stats
    
    def _create_trajectory_table(self, table_name: str) -> bool:
        """åˆ›å»ºè½¨è¿¹ç»“æœè¡¨ï¼ˆäº‹åŠ¡ä¿®å¤ç‰ˆï¼‰"""
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
            check_table_sql = text(f"""
                SELECT EXISTS (
                    SELECT FROM information_schema.tables 
                    WHERE table_schema = 'public' 
                    AND table_name = '{table_name}'
                );
            """)
            
            with self.engine.connect() as conn:
                result = conn.execute(check_table_sql)
                table_exists = result.scalar()
                
                if table_exists:
                    logger.info(f"è¡¨ {table_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                    return True
            
            logger.info(f"åˆ›å»ºé«˜æ€§èƒ½è½¨è¿¹è¡¨: {table_name}")
            
            # åˆ›å»ºè¡¨ç»“æ„ï¼ˆåˆ†æ­¥æ‰§è¡Œé¿å…äº‹åŠ¡å†²çªï¼‰
            create_sql = text(f"""
                CREATE TABLE {table_name} (
                    id serial PRIMARY KEY,
                    dataset_name text NOT NULL,
                    scene_id text,
                    start_time bigint,
                    end_time bigint,
                    duration bigint,
                    point_count integer,
                    avg_speed numeric(8,2),
                    max_speed numeric(8,2),
                    min_speed numeric(8,2),
                    std_speed numeric(8,2),
                    avp_ratio numeric(5,3),
                    polygon_ids text[],
                    created_at timestamp DEFAULT CURRENT_TIMESTAMP
                );
            """)
            
            # æ·»åŠ å‡ ä½•åˆ—
            add_geom_sql = text(f"""
                SELECT AddGeometryColumn('public', '{table_name}', 'geometry', 4326, 'LINESTRING', 2);
            """)
            
            # åˆ›å»ºä¼˜åŒ–ç´¢å¼•
            index_sql = text(f"""
                CREATE INDEX idx_{table_name}_geometry ON {table_name} USING GIST(geometry);
                CREATE INDEX idx_{table_name}_dataset_name ON {table_name}(dataset_name);
                CREATE INDEX idx_{table_name}_scene_id ON {table_name}(scene_id);
                CREATE INDEX idx_{table_name}_start_time ON {table_name}(start_time);
                CREATE INDEX idx_{table_name}_point_count ON {table_name}(point_count);
                CREATE INDEX idx_{table_name}_polygon_ids ON {table_name} USING GIN(polygon_ids);
                CREATE INDEX idx_{table_name}_created_at ON {table_name}(created_at);
            """)
            
            # åˆ†æ­¥æ‰§è¡ŒSQLï¼ˆé¿å…äº‹åŠ¡å†²çªï¼‰
            try:
                # æ­¥éª¤1ï¼šåˆ›å»ºè¡¨
                with self.engine.connect() as conn:
                    conn.execute(create_sql)
                    conn.commit()
                logger.debug("âœ… è¡¨ç»“æ„åˆ›å»ºå®Œæˆ")
                
                # æ­¥éª¤2ï¼šæ·»åŠ å‡ ä½•åˆ—
                with self.engine.connect() as conn:
                    conn.execute(add_geom_sql)
                    conn.commit()
                logger.debug("âœ… å‡ ä½•åˆ—æ·»åŠ å®Œæˆ")
                
                # æ­¥éª¤3ï¼šåˆ›å»ºç´¢å¼•
                with self.engine.connect() as conn:
                    conn.execute(index_sql)
                    conn.commit()
                logger.debug("âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ")
                
                logger.info(f"âœ… è½¨è¿¹è¡¨åˆ›å»ºæˆåŠŸ: {table_name}")
                return True
                
            except Exception as e:
                logger.error(f"SQLæ‰§è¡Œå¤±è´¥: {str(e)}")
                # å°è¯•æ¸…ç†éƒ¨åˆ†åˆ›å»ºçš„è¡¨
                try:
                    with self.engine.connect() as conn:
                        conn.execute(text(f"DROP TABLE IF EXISTS {table_name}"))
                        conn.commit()
                    logger.info(f"å·²æ¸…ç†éƒ¨åˆ†åˆ›å»ºçš„è¡¨: {table_name}")
                except:
                    pass
                raise e
                
        except Exception as e:
            logger.error(f"åˆ›å»ºè½¨è¿¹è¡¨å¤±è´¥: {table_name}, é”™è¯¯: {str(e)}")
            return False

    def process_complete_workflow(
        self,
        geojson_file: str,
        output_table: Optional[str] = None,
        output_geojson: Optional[str] = None
    ) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„é«˜æ€§èƒ½polygonè½¨è¿¹æŸ¥è¯¢å·¥ä½œæµ
        
        Args:
            geojson_file: è¾“å…¥çš„GeoJSONæ–‡ä»¶è·¯å¾„
            output_table: è¾“å‡ºæ•°æ®åº“è¡¨åï¼ˆå¯é€‰ï¼‰
            output_geojson: è¾“å‡ºGeoJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        logger.debug("ğŸ”§ DEBUG: process_complete_workflow æ–¹æ³•è¢«è°ƒç”¨")
        logger.debug(f"ğŸ”§ DEBUG: å®ä¾‹ç±»å‹: {type(self)}")
        logger.debug(f"ğŸ”§ DEBUG: æ–¹æ³•å­˜åœ¨æ€§: {hasattr(self, 'process_complete_workflow')}")
        
        workflow_start = time.time()
        
        # ç»¼åˆç»Ÿè®¡ä¿¡æ¯
        complete_stats = {
            'start_time': datetime.now(),
            'geojson_file': geojson_file,
            'output_table': output_table,
            'output_geojson': output_geojson,
            'config': {
                'batch_threshold': self.config.batch_threshold,
                'chunk_size': self.config.chunk_size,
                'limit_per_polygon': self.config.limit_per_polygon,
                'batch_insert_size': self.config.batch_insert_size
            }
        }
        
        try:
            # é˜¶æ®µ1: åŠ è½½polygon
            logger.info("=" * 60)
            logger.info("ğŸš€ å¼€å§‹é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢å·¥ä½œæµ")
            logger.info("=" * 60)
            
            logger.info(f"ğŸ“ é˜¶æ®µ1: åŠ è½½GeoJSONæ–‡ä»¶: {geojson_file}")
            polygons = load_polygons_from_geojson(geojson_file)
            complete_stats['polygon_count'] = len(polygons)
            
            if not polygons:
                logger.error("âŒ æœªåŠ è½½åˆ°ä»»ä½•polygon")
                complete_stats['error'] = "No polygons loaded"
                return complete_stats
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(polygons)} ä¸ªpolygon")
            
            # é˜¶æ®µ2: é«˜æ•ˆæŸ¥è¯¢è½¨è¿¹ç‚¹
            logger.info(f"ğŸ” é˜¶æ®µ2: æ‰§è¡Œé«˜æ€§èƒ½è½¨è¿¹ç‚¹æŸ¥è¯¢")
            points_df, query_stats = self.query_intersecting_trajectory_points(polygons)
            complete_stats['query_stats'] = query_stats
            
            if points_df.empty:
                logger.warning("âš ï¸ æœªæŸ¥è¯¢åˆ°ä»»ä½•è½¨è¿¹ç‚¹")
                complete_stats['warning'] = "No trajectory points found"
                return complete_stats
            
            logger.info(f"âœ… æŸ¥è¯¢åˆ° {len(points_df)} ä¸ªè½¨è¿¹ç‚¹")
            
            # é˜¶æ®µ3: æ„å»ºè½¨è¿¹
            logger.info(f"ğŸ”§ é˜¶æ®µ3: æ„å»ºè½¨è¿¹çº¿å’Œç»Ÿè®¡ä¿¡æ¯")
            trajectories, build_stats = self.build_trajectories_from_points(points_df)
            complete_stats['build_stats'] = build_stats
            
            if not trajectories:
                logger.warning("âš ï¸ æœªæ„å»ºåˆ°ä»»ä½•è½¨è¿¹")
                complete_stats['warning'] = "No trajectories built"
                return complete_stats
            
            logger.info(f"âœ… æ„å»ºäº† {len(trajectories)} æ¡è½¨è¿¹")
            
            # é˜¶æ®µ4: ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
            if output_table:
                logger.info(f"ğŸ’¾ é˜¶æ®µ4: ä¿å­˜åˆ°æ•°æ®åº“è¡¨: {output_table}")
                inserted_count, save_stats = self.save_trajectories_to_table(trajectories, output_table)
                complete_stats['save_stats'] = save_stats
                logger.info(f"âœ… æˆåŠŸä¿å­˜ {inserted_count} æ¡è½¨è¿¹åˆ°æ•°æ®åº“")
            
            # é˜¶æ®µ5: å¯¼å‡ºåˆ°GeoJSONï¼ˆå¯é€‰ï¼‰
            if output_geojson:
                logger.info(f"ğŸ“„ é˜¶æ®µ5: å¯¼å‡ºåˆ°GeoJSONæ–‡ä»¶: {output_geojson}")
                if export_trajectories_to_geojson(trajectories, output_geojson):
                    complete_stats['geojson_exported'] = True
                    logger.info(f"âœ… æˆåŠŸå¯¼å‡ºè½¨è¿¹åˆ°GeoJSONæ–‡ä»¶")
                else:
                    complete_stats['geojson_export_failed'] = True
                    logger.warning("âš ï¸ GeoJSONå¯¼å‡ºå¤±è´¥")
            
            # æœ€ç»ˆç»Ÿè®¡
            complete_stats['total_trajectories'] = len(trajectories)
            complete_stats['workflow_duration'] = time.time() - workflow_start
            complete_stats['end_time'] = datetime.now()
            complete_stats['success'] = True
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢å·¥ä½œæµå®Œæˆ!")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {complete_stats['workflow_duration']:.2f}s")
            logger.info(f"ğŸ“Š è¾“å‡ºè½¨è¿¹: {complete_stats['total_trajectories']} æ¡")
            logger.info("=" * 60)
            
            return complete_stats
            
        except Exception as e:
            complete_stats['error'] = str(e)
            complete_stats['workflow_duration'] = time.time() - workflow_start
            complete_stats['end_time'] = datetime.now()
            complete_stats['success'] = False
            logger.error(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
            return complete_stats

def export_trajectories_to_geojson(trajectories: List[Dict], output_file: str) -> bool:
    """å¯¼å‡ºè½¨è¿¹æ•°æ®åˆ°GeoJSONæ–‡ä»¶
    
    Args:
        trajectories: è½¨è¿¹æ•°æ®åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        
    Returns:
        å¯¼å‡ºæ˜¯å¦æˆåŠŸ
    """
    if not trajectories:
        logger.warning("æ²¡æœ‰è½¨è¿¹æ•°æ®éœ€è¦å¯¼å‡º")
        return False
    
    try:
        # å‡†å¤‡GeoDataFrameæ•°æ®
        gdf_data = []
        geometries = []
        
        for traj in trajectories:
            # åˆ†ç¦»å‡ ä½•å’Œå±æ€§æ•°æ®
            row = {k: v for k, v in traj.items() if k != 'geometry'}
            # è½¬æ¢polygon_idsä¸ºå­—ç¬¦ä¸²
            if 'polygon_ids' in row:
                row['polygon_ids'] = ','.join(row['polygon_ids'])
            gdf_data.append(row)
            geometries.append(traj['geometry'])
        
        # åˆ›å»ºGeoDataFrame
        gdf = gpd.GeoDataFrame(gdf_data, geometry=geometries, crs=4326)
        
        # å¯¼å‡ºåˆ°GeoJSON
        gdf.to_file(output_file, driver='GeoJSON', encoding='utf-8')
        
        logger.info(f"æˆåŠŸå¯¼å‡º {len(gdf)} æ¡è½¨è¿¹åˆ°æ–‡ä»¶: {output_file}")
        return True
        
    except Exception as e:
        logger.error(f"å¯¼å‡ºè½¨è¿¹æ•°æ®å¤±è´¥: {str(e)}")
        return False

# ä¾¿æ·å‡½æ•°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
def process_polygon_trajectory_query(
    geojson_file: str,
    output_table: Optional[str] = None,
    output_geojson: Optional[str] = None,
    config: Optional[PolygonTrajectoryConfig] = None
) -> Dict:
    """é«˜æ€§èƒ½polygonè½¨è¿¹æŸ¥è¯¢å®Œæ•´æµç¨‹
    
    Args:
        geojson_file: è¾“å…¥çš„GeoJSONæ–‡ä»¶è·¯å¾„
        output_table: è¾“å‡ºæ•°æ®åº“è¡¨åï¼ˆå¯é€‰ï¼‰
        output_geojson: è¾“å‡ºGeoJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        config: è‡ªå®šä¹‰é…ç½®ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        è¯¦ç»†çš„å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    # ä½¿ç”¨é«˜æ€§èƒ½æŸ¥è¯¢å™¨
    query_config = config or PolygonTrajectoryConfig()
    processor = HighPerformancePolygonTrajectoryQuery(query_config)
    
    # è°ƒè¯•ä¿¡æ¯ï¼šéªŒè¯å®ä¾‹å’Œæ–¹æ³•
    logger.debug(f"ğŸ”§ DEBUG: åˆ›å»ºçš„å¤„ç†å™¨ç±»å‹: {type(processor)}")
    logger.debug(f"ğŸ”§ DEBUG: å¤„ç†å™¨å¯ç”¨æ–¹æ³•: {[method for method in dir(processor) if not method.startswith('_')]}")
    logger.debug(f"ğŸ”§ DEBUG: process_complete_workflow æ–¹æ³•æ˜¯å¦å­˜åœ¨: {hasattr(processor, 'process_complete_workflow')}")
    
    if not hasattr(processor, 'process_complete_workflow'):
        logger.error("âŒ CRITICAL: process_complete_workflow æ–¹æ³•ä¸å­˜åœ¨!")
        logger.error(f"âŒ å¯ç”¨æ–¹æ³•: {[method for method in dir(processor) if callable(getattr(processor, method)) and not method.startswith('_')]}")
        raise AttributeError("HighPerformancePolygonTrajectoryQuery object has no attribute 'process_complete_workflow'")
    
    logger.debug("ğŸ”§ DEBUG: å³å°†è°ƒç”¨ process_complete_workflow æ–¹æ³•")
    
    return processor.process_complete_workflow(
        geojson_file=geojson_file,
        output_table=output_table,
        output_geojson=output_geojson
    )

def main():
    """ä¸»å‡½æ•°ï¼ŒCLIå…¥å£ç‚¹"""
    parser = argparse.ArgumentParser(
        description='é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢æ¨¡å— - æ‰¹é‡æŸ¥æ‰¾ä¸polygonç›¸äº¤çš„è½¨è¿¹æ•°æ®',
        epilog="""
é«˜æ€§èƒ½ç‰¹æ€§:
  â€¢ æ™ºèƒ½æ‰¹é‡æŸ¥è¯¢ç­–ç•¥ï¼šâ‰¤50ä¸ªpolygonä½¿ç”¨UNION ALLï¼Œ>50ä¸ªpolygonä½¿ç”¨åˆ†å—æŸ¥è¯¢
  â€¢ ä¼˜åŒ–çš„æ•°æ®åº“å†™å…¥ï¼šæ‰¹é‡æ’å…¥ï¼Œäº‹åŠ¡ä¿æŠ¤ï¼Œå¤šé‡ç´¢å¼•
  â€¢ è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡ï¼šæŸ¥è¯¢æ—¶é—´ã€æ„å»ºæ—¶é—´ã€å¤„ç†é€Ÿåº¦ç­‰

ç¤ºä¾‹:
  # åŸºç¡€ç”¨æ³•ï¼šæŸ¥è¯¢è½¨è¿¹å¹¶ä¿å­˜åˆ°æ•°æ®åº“è¡¨
  python -m spdatalab.dataset.polygon_trajectory_query --input polygons.geojson --table my_trajectories
  
  # é«˜æ€§èƒ½æ¨¡å¼ï¼šè‡ªå®šä¹‰æ‰¹é‡æŸ¥è¯¢å‚æ•°
  python -m spdatalab.dataset.polygon_trajectory_query --input polygons.geojson --table my_trajectories \\
    --batch-threshold 30 --chunk-size 15 --batch-insert 500
  
  # åŒæ—¶ä¿å­˜åˆ°æ•°æ®åº“å’Œæ–‡ä»¶
  python -m spdatalab.dataset.polygon_trajectory_query --input polygons.geojson \\
    --table my_trajectories --output trajectories.geojson
  
  # è°ƒæ•´è½¨è¿¹ç‚¹é™åˆ¶å’Œç»Ÿè®¡é€‰é¡¹
  python -m spdatalab.dataset.polygon_trajectory_query --input polygons.geojson \\
    --table my_trajectories --limit 20000 --no-speed-stats --verbose
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--input', required=True, help='è¾“å…¥GeoJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--table', help='è¾“å‡ºæ•°æ®åº“è¡¨åï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--output', help='è¾“å‡ºGeoJSONæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    
    # æ€§èƒ½ä¼˜åŒ–å‚æ•°
    parser.add_argument('--batch-threshold', type=int, default=50, 
                       help='æ‰¹é‡æŸ¥è¯¢vsåˆ†å—æŸ¥è¯¢çš„é˜ˆå€¼ (é»˜è®¤: 50)')
    parser.add_argument('--chunk-size', type=int, default=20,
                       help='åˆ†å—æŸ¥è¯¢çš„å—å¤§å° (é»˜è®¤: 20)')
    parser.add_argument('--limit', type=int, default=10000, 
                       help='æ¯ä¸ªpolygonçš„è½¨è¿¹ç‚¹é™åˆ¶æ•°é‡ (é»˜è®¤: 10000)')
    parser.add_argument('--batch-insert', type=int, default=1000,
                       help='æ‰¹é‡æ’å…¥æ•°æ®åº“çš„æ‰¹æ¬¡å¤§å° (é»˜è®¤: 1000)')
    parser.add_argument('--timeout', type=int, default=300,
                       help='æŸ¥è¯¢è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰(é»˜è®¤: 300)')
    
    # åŠŸèƒ½é€‰é¡¹
    parser.add_argument('--min-points', type=int, default=2,
                       help='æ„å»ºè½¨è¿¹çš„æœ€å°ç‚¹æ•° (é»˜è®¤: 2)')
    parser.add_argument('--no-speed-stats', action='store_true',
                       help='ç¦ç”¨é€Ÿåº¦ç»Ÿè®¡è®¡ç®—')
    parser.add_argument('--no-avp-stats', action='store_true',
                       help='ç¦ç”¨AVPç»Ÿè®¡è®¡ç®—')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†æ—¥å¿—')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if not args.table and not args.output:
        parser.error("å¿…é¡»æŒ‡å®š --table æˆ– --output ä¸­çš„è‡³å°‘ä¸€ä¸ª")
    
    # é…ç½®æ—¥å¿—
    level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
        if not Path(args.input).exists():
            logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
            return 1
        
        # æ„å»ºé…ç½®
        config = PolygonTrajectoryConfig(
            batch_threshold=args.batch_threshold,
            chunk_size=args.chunk_size,
            limit_per_polygon=args.limit,
            batch_insert_size=args.batch_insert,
            min_points_per_trajectory=args.min_points,
            enable_speed_stats=not args.no_speed_stats,
            enable_avp_stats=not args.no_avp_stats
        )
        
        # è¾“å‡ºé…ç½®ä¿¡æ¯
        logger.info("ğŸ”§ é…ç½®å‚æ•°:")
        logger.info(f"   â€¢ æ‰¹é‡æŸ¥è¯¢é˜ˆå€¼: {config.batch_threshold}")
        logger.info(f"   â€¢ åˆ†å—å¤§å°: {config.chunk_size}")
        logger.info(f"   â€¢ æ¯polygonè½¨è¿¹ç‚¹é™åˆ¶: {config.limit_per_polygon:,}")
        logger.info(f"   â€¢ æ‰¹é‡æ’å…¥å¤§å°: {config.batch_insert_size}")
        logger.info(f"   â€¢ æœ€å°è½¨è¿¹ç‚¹æ•°: {config.min_points_per_trajectory}")
        logger.info(f"   â€¢ é€Ÿåº¦ç»Ÿè®¡: {'å¯ç”¨' if config.enable_speed_stats else 'ç¦ç”¨'}")
        logger.info(f"   â€¢ AVPç»Ÿè®¡: {'å¯ç”¨' if config.enable_avp_stats else 'ç¦ç”¨'}")
        
        # æ‰§è¡Œå¤„ç†
        stats = process_polygon_trajectory_query(
            geojson_file=args.input,
            output_table=args.table,
            output_geojson=args.output,
            config=config
        )
        
        # æ£€æŸ¥å¤„ç†ç»“æœ
        if 'error' in stats:
            logger.error(f"âŒ å¤„ç†é”™è¯¯: {stats['error']}")
            return 1
        
        if not stats.get('success', False):
            logger.error("âŒ å¤„ç†æœªæˆåŠŸå®Œæˆ")
            return 1
        
        # æˆåŠŸå®Œæˆ
        logger.info("ğŸ‰ æ‰€æœ‰å¤„ç†æˆåŠŸå®Œæˆï¼")
        
        # ç¡®å®šè¿”å›ä»£ç 
        query_stats = stats.get('query_stats', {})
        build_stats = stats.get('build_stats', {})
        
        has_results = (
            query_stats.get('total_points', 0) > 0 and 
            build_stats.get('valid_trajectories', 0) > 0
        )
        
        return 0 if has_results else 1
        
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        return 1

if __name__ == '__main__':
    sys.exit(main()) 