from __future__ import annotations
import argparse
import sys
import re
from datetime import datetime
import geopandas as gpd, pandas as pd
from sqlalchemy import text, create_engine
from typing import List, Dict
import multiprocessing as mp
from multiprocessing import Pool, Manager
from concurrent.futures import ProcessPoolExecutor, as_completed
import threading
import time
import os

# å…¼å®¹æ—§å®ç°çš„åŸºç¡€é…ç½®ç”± pipeline æ¨¡å—æä¾›
from .pipeline import (
    InterruptFlag,
    LightweightProgressTracker,
    PARQUET_AVAILABLE,
    setup_interrupt_handlers,
)
from .core import chunk
from .io import load_scene_ids, fetch_meta, fetch_bbox_with_geometry

LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"
POINT_TABLE = "public.ddi_data_points"

# å…¨å±€å˜é‡ç”¨äºä¼˜é›…é€€å‡º
interrupted = False
interrupt_flag = InterruptFlag()


def _default_interrupt_message(signum: int, _frame: object | None) -> None:
    print(f"\n???????({signum})???????..")
    print("??????????????..")



def setup_signal_handlers() -> None:
    """??????????????????????"""

    global interrupted
    interrupted = False
    interrupt_flag.reset()

    def _handler(signum: int, frame: object | None) -> None:
        global interrupted
        if not interrupt_flag.is_set():
            _default_interrupt_message(signum, frame)
        interrupt_flag.set()
        interrupted = True

    setup_interrupt_handlers(interrupt_flag, on_interrupt=_handler)


def create_table_if_not_exists(eng, table_name='clips_bbox'):
    """å¦‚æœè¡¨ä¸å­˜åœ¨åˆ™åˆ›å»ºè¡¨ - ä¸cleanup_clips_bbox.sqlä¿æŒä¸€è‡´"""
    # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
    check_table_sql = text(f"""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_schema = 'public' 
            AND table_name = '{table_name}'
        );
    """)
    
    try:
        with eng.connect() as conn:
            result = conn.execute(check_table_sql)
            table_exists = result.scalar()
            
            if table_exists:
                print(f"è¡¨ {table_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return True
                
            print(f"è¡¨ {table_name} ä¸å­˜åœ¨ï¼Œå¼€å§‹åˆ›å»º...")
            
            # ä¸cleanup_clips_bbox.sqlä¿æŒä¸€è‡´çš„è¡¨ç»“æ„
            create_sql = text(f"""
                CREATE TABLE {table_name}(
                    id serial PRIMARY KEY,
                    scene_token text,
                    data_name text UNIQUE,
                    event_id text,
                    city_id text,
                    "timestamp" bigint,
                    all_good boolean
                );
            """)
            
            # ä½¿ç”¨PostGISæ·»åŠ å‡ ä½•åˆ—
            add_geom_sql = text(f"""
                SELECT AddGeometryColumn('public', '{table_name}', 'geometry', 4326, 'POLYGON', 2);
            """)
            
            # æ·»åŠ å‡ ä½•çº¦æŸ
            constraint_sql = text(f"""
                ALTER TABLE {table_name} ADD CONSTRAINT check_geom_type 
                    CHECK (ST_GeometryType(geometry) IN ('ST_Polygon', 'ST_Point'));
            """)
            
            # åˆ›å»ºç´¢å¼•
            index_sql = text(f"""
                CREATE INDEX idx_{table_name}_geometry ON {table_name} USING GIST(geometry);
                CREATE INDEX idx_{table_name}_data_name ON {table_name}(data_name);
                CREATE INDEX idx_{table_name}_scene_token ON {table_name}(scene_token);
            """)
            
            # æ‰§è¡ŒSQLè¯­å¥ï¼Œéœ€è¦åˆ†æ­¥æäº¤ä»¥ç¡®ä¿PostGISå‡½æ•°èƒ½æ‰¾åˆ°è¡¨
            conn.execute(create_sql)
            conn.commit()  # å…ˆæäº¤è¡¨åˆ›å»º
            
            # æ‰§è¡ŒPostGISç›¸å…³æ“ä½œ
            conn.execute(add_geom_sql)
            conn.execute(constraint_sql)
            conn.commit()  # æäº¤å‡ ä½•åˆ—å’Œçº¦æŸ
            
            # åˆ›å»ºç´¢å¼•
            conn.execute(index_sql)
            conn.commit()  # æœ€åæäº¤ç´¢å¼•
            
            print(f"æˆåŠŸåˆ›å»ºè¡¨ {table_name} åŠç›¸å…³ç´¢å¼•")
            return True
            
    except Exception as e:
        print(f"åˆ›å»ºè¡¨æ—¶å‡ºé”™: {str(e)}")
        print("å»ºè®®ï¼šå¦‚æœè¡¨å·²é€šè¿‡cleanup_clips_bbox.sqlåˆ›å»ºï¼Œè¯·ä½¿ç”¨ --no-create-table é€‰é¡¹")
        return False

def batch_insert_to_postgis(gdf, eng, table_name='clips_bbox', batch_size=1000, tracker=None, batch_num=None):
    """æ‰¹é‡æ’å…¥åˆ°PostGISï¼Œä¾èµ–æ•°æ®åº“çº¦æŸå¤„ç†é‡å¤æ•°æ®"""
    total_rows = len(gdf)
    inserted_rows = 0
    successful_tokens = []
    
    # åˆ†æ‰¹æ’å…¥
    for i in range(0, total_rows, batch_size):
        batch_gdf = gdf.iloc[i:i+batch_size]
        batch_tokens = batch_gdf['scene_token'].tolist()
        
        try:
            # ç›´æ¥æ’å…¥ï¼Œè®©æ•°æ®åº“å¤„ç†é‡å¤
            batch_gdf.to_postgis(
                table_name, 
                eng, 
                if_exists='append', 
                index=False
            )
            inserted_rows += len(batch_gdf)
            successful_tokens.extend(batch_tokens)
            print(f'[æ‰¹é‡æ’å…¥] å·²æ’å…¥: {inserted_rows}/{total_rows} è¡Œ')
            
        except Exception as e:
            error_str = str(e).lower()
            
            # å¦‚æœæ˜¯é‡å¤é”®è¿åçº¦æŸï¼Œå°è¯•é€è¡Œæ’å…¥ä»¥è¯†åˆ«å…·ä½“çš„é‡å¤è®°å½•
            if 'unique' in error_str or 'duplicate' in error_str or 'constraint' in error_str:
                print(f'[æ‰¹é‡æ’å…¥] æ‰¹æ¬¡ {i//batch_size + 1} é‡åˆ°é‡å¤æ•°æ®ï¼Œè¿›è¡Œé€è¡Œæ’å…¥')
                
                for idx, row in batch_gdf.iterrows():
                    scene_token = row['scene_token']
                    try:
                        # åˆ›å»ºå•è¡ŒGeoDataFrame
                        single_gdf = gpd.GeoDataFrame(
                            [row.drop('geometry')], 
                            geometry=[row.geometry], 
                            crs=4326
                        )
                        single_gdf.to_postgis(table_name, eng, if_exists='append', index=False)
                        inserted_rows += 1
                        successful_tokens.append(scene_token)
                    except Exception as row_e:
                        row_error_str = str(row_e).lower()
                        if 'unique' in row_error_str or 'duplicate' in row_error_str:
                            # é‡å¤æ•°æ®ï¼Œä¸è®°å½•ä¸ºå¤±è´¥ï¼Œåªæ˜¯è·³è¿‡
                            print(f'[è·³è¿‡é‡å¤] scene_token: {scene_token}')
                            successful_tokens.append(scene_token)  # è§†ä¸ºæˆåŠŸï¼ˆå·²å­˜åœ¨ï¼‰
                        else:
                            # å…¶ä»–ç±»å‹çš„é”™è¯¯æ‰è®°å½•ä¸ºå¤±è´¥
                            error_msg = f'æ’å…¥å¤±è´¥: {str(row_e)}'
                            print(f'[æ’å…¥é”™è¯¯] scene_token: {scene_token}: {error_msg}')
                            if tracker:
                                tracker.save_failed_record(scene_token, error_msg, batch_num, "database_insert")
            else:
                # éé‡å¤æ•°æ®é—®é¢˜ï¼Œè®°å½•ä¸ºå¤±è´¥
                print(f'[æ‰¹é‡æ’å…¥é”™è¯¯] æ‰¹æ¬¡ {i//batch_size + 1}: {str(e)}')
                for token in batch_tokens:
                    if tracker:
                        tracker.save_failed_record(token, f"æ‰¹é‡æ’å…¥å¼‚å¸¸: {str(e)}", batch_num, "database_insert")
    
    # æ‰¹é‡ä¿å­˜æˆåŠŸçš„tokensï¼ˆåŒ…æ‹¬é‡å¤è·³è¿‡çš„ï¼‰
    if tracker and successful_tokens:
        tracker.save_successful_batch(successful_tokens, batch_num)
    
    return inserted_rows

def normalize_subdataset_name(subdataset_name: str) -> str:
    """è§„èŒƒåŒ–å­æ•°æ®é›†åç§°
    
    è§„åˆ™ï¼š
    1. å»æ‰å¼€å¤´çš„ "GOD_E2E_"
    2. å¦‚æœæœ‰ "_sub_ddi_" åˆ™æˆªæ–­åˆ°è¿™é‡Œï¼ˆä¸åŒ…å«_sub_ddi_ï¼‰
    3. å¦‚æœä¸­é—´å‡ºç°ç±»ä¼¼ "_277736e2e_"ï¼ˆ277æ‰“å¤´ï¼Œe2eç»“å°¾ï¼‰çš„å­—ç¬¦ä¸²ï¼Œè¿™éƒ¨åˆ†åŠä¹‹åéƒ½ä¸è¦
    4. ç§»é™¤ç»“å°¾çš„æ—¶é—´æˆ³æ ¼å¼ "_YYYY_MM_DD_HH_MM_SS"
    
    Args:
        subdataset_name: åŸå§‹å­æ•°æ®é›†åç§°
        
    Returns:
        è§„èŒƒåŒ–åçš„åç§°
    """
    original_name = subdataset_name
    
    # 1. å»æ‰å¼€å¤´çš„ GOD_E2E_
    if subdataset_name.startswith("GOD_E2E_"):
        subdataset_name = subdataset_name[8:]  # len("GOD_E2E_") = 8
    
    # 2. å¤„ç† _sub_ddi_ æˆªæ–­
    sub_ddi_pos = subdataset_name.find("_sub_ddi_")
    if sub_ddi_pos != -1:
        subdataset_name = subdataset_name[:sub_ddi_pos]
    
    # 3. å¤„ç†ç±»ä¼¼ _277736e2e_ çš„æ¨¡å¼æˆªæ–­ï¼ˆ277æ‰“å¤´ï¼Œe2eç»“å°¾ï¼‰
    # åŒ¹é…æ¨¡å¼ï¼š_277å¼€å¤´ï¼Œä»»æ„å­—ç¬¦ï¼Œe2eç»“å°¾çš„å­—ç¬¦ä¸²
    hash_pattern = r'_277[^_]*e2e_'
    hash_match = re.search(hash_pattern, subdataset_name)
    if hash_match:
        # æˆªæ–­åˆ°åŒ¹é…ä½ç½®ä¹‹å‰
        subdataset_name = subdataset_name[:hash_match.start()]
    
    # 4. ç§»é™¤ç»“å°¾çš„æ—¶é—´æˆ³æ ¼å¼ "_YYYY_MM_DD_HH_MM_SS"
    timestamp_pattern = r'_\d{4}_\d{2}_\d{2}_\d{2}_\d{2}_\d{2}$'
    subdataset_name = re.sub(timestamp_pattern, '', subdataset_name)
    
    # 5. æ¸…ç†å’ŒéªŒè¯ç»“æœ
    subdataset_name = subdataset_name.strip('_')
    
    # ç¡®ä¿åç§°ä¸ä¸ºç©º
    if not subdataset_name:
        subdataset_name = "unnamed_dataset"
    
    print(f"åç§°è§„èŒƒåŒ–: '{original_name}' -> '{subdataset_name}'")
    return subdataset_name

def get_table_name_for_subdataset(subdataset_name: str) -> str:
    """ä¸ºå­æ•°æ®é›†ç”Ÿæˆåˆæ³•çš„PostgreSQLè¡¨å"""
    # å…ˆè§„èŒƒåŒ–åç§°
    normalized_name = normalize_subdataset_name(subdataset_name)
    
    # æ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼Œç¡®ä¿è¡¨ååˆæ³•
    clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', normalized_name)
    
    # è½¬æ¢ä¸ºå°å†™ï¼ˆPostgreSQLè¡¨åæœ€ä½³å®è·µï¼‰
    clean_name = clean_name.lower()
    
    # å¤„ç†è¿ç»­ä¸‹åˆ’çº¿é—®é¢˜
    original_underscores = len(re.findall(r'_{2,}', clean_name))
    clean_name = re.sub(r'_+', '_', clean_name)  # å¤šä¸ªä¸‹åˆ’çº¿åˆå¹¶ä¸ºä¸€ä¸ª
    clean_name = clean_name.strip('_')  # å»é™¤é¦–å°¾ä¸‹åˆ’çº¿
    
    if original_underscores > 0:
        print(f"è­¦å‘Š: å‘ç° {original_underscores} å¤„è¿ç»­ä¸‹åˆ’çº¿ï¼Œå·²è‡ªåŠ¨åˆå¹¶")
        
    # ç¡®ä¿æ²¡æœ‰ç©ºçš„è¡¨åéƒ¨åˆ†ï¼ˆç”±è¿ç»­ä¸‹åˆ’çº¿å¯¼è‡´ï¼‰
    name_parts = clean_name.split('_')
    # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²éƒ¨åˆ†
    valid_parts = [part for part in name_parts if part.strip()]
    if len(valid_parts) != len(name_parts):
        print(f"è­¦å‘Š: æ¸…ç†äº†ç©ºçš„è¡¨åæ®µï¼Œä» {len(name_parts)} æ®µå‡å°‘åˆ° {len(valid_parts)} æ®µ")
        clean_name = '_'.join(valid_parts)
    
    # ä½¿ç”¨ä¿å®ˆçš„é•¿åº¦é™åˆ¶ï¼Œä¸ºPostGISå…¼å®¹æ€§é¢„ç•™ç©ºé—´
    # clips_bbox_ = 12å­—ç¬¦ï¼Œæ‰€ä»¥ä¸»ä½“éƒ¨åˆ†é™åˆ¶åœ¨ 50-12=38 å­—ç¬¦
    max_main_length = 38
    
    if len(clean_name) > max_main_length:
        # ç›´æ¥æˆªæ–­ï¼ˆä¸éœ€è¦å¤„ç†æ—¶é—´æˆ³ï¼Œå› ä¸ºå·²ç»åœ¨normalizeé˜¶æ®µç§»é™¤äº†ï¼‰
        clean_name = clean_name[:max_main_length]
    
    # æ„å»ºæœ€ç»ˆè¡¨å
    table_name = f"clips_bbox_{clean_name}"
    
    # ç¡®ä¿è¡¨åç¬¦åˆPostgreSQLè§„èŒƒï¼ˆä»¥å­—æ¯å¼€å¤´ï¼‰
    if table_name[0].isdigit():
        table_name = "t_" + table_name
        # å¦‚æœåŠ å‰ç¼€åè¶…é•¿ï¼Œå†æ¬¡æˆªæ–­
        if len(table_name) > 50:  # ä¿å®ˆæˆªæ–­
            table_name = table_name[:50]
    
    # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
    if len(table_name) > 50:
        table_name = table_name[:50]
    
    # æœ€ç»ˆæ¸…ç†ï¼šç¡®ä¿æ²¡æœ‰è¿ç»­ä¸‹åˆ’çº¿å’Œç©ºæ®µ
    table_name = re.sub(r'_+', '_', table_name)  # åˆå¹¶è¿ç»­ä¸‹åˆ’çº¿
    table_name = table_name.strip('_')  # å»é™¤é¦–å°¾ä¸‹åˆ’çº¿
    
    # æ¸…ç†ç©ºæ®µ
    name_parts = table_name.split('_')
    valid_parts = [part for part in name_parts if part.strip()]
    if len(valid_parts) != len(name_parts):
        print(f"è­¦å‘Š: æœ€ç»ˆæ¸…ç†äº†ç©ºçš„è¡¨åæ®µï¼Œä» {len(name_parts)} æ®µå‡å°‘åˆ° {len(valid_parts)} æ®µ")
        table_name = '_'.join(valid_parts)
    
    # æœ€ç»ˆéªŒè¯è¡¨ååˆæ³•æ€§
    validation_result = validate_table_name(table_name)
    if not validation_result['valid']:
        print(f"è­¦å‘Š: è¡¨åéªŒè¯å¤±è´¥: {validation_result['issues']}")
    
    print(f"è¡¨åç”Ÿæˆ: '{subdataset_name}' -> '{table_name}' (é•¿åº¦: {len(table_name)})")
    return table_name

def validate_table_name(table_name: str) -> dict:
    """éªŒè¯è¡¨åçš„åˆæ³•æ€§
    
    Args:
        table_name: è¦éªŒè¯çš„è¡¨å
        
    Returns:
        åŒ…å«éªŒè¯ç»“æœçš„å­—å…¸
    """
    issues = []
    
    # æ£€æŸ¥é•¿åº¦
    if len(table_name) > 63:
        issues.append(f"è¡¨åè¿‡é•¿: {len(table_name)} > 63")
    
    # æ£€æŸ¥å­—ç¬¦
    if not re.match(r'^[a-zA-Z][a-zA-Z0-9_]*$', table_name):
        issues.append("è¡¨ååŒ…å«éæ³•å­—ç¬¦æˆ–ä¸ä»¥å­—æ¯å¼€å¤´")
    
    # æ£€æŸ¥å¤§å†™å­—æ¯ï¼ˆPostgreSQLæœ€ä½³å®è·µæ˜¯å°å†™ï¼‰
    if re.search(r'[A-Z]', table_name):
        issues.append("è¡¨ååŒ…å«å¤§å†™å­—æ¯ï¼Œå»ºè®®ä½¿ç”¨å°å†™")
    
    # æ£€æŸ¥è¿ç»­ä¸‹åˆ’çº¿
    if re.search(r'_{2,}', table_name):
        issues.append("è¡¨ååŒ…å«è¿ç»­ä¸‹åˆ’çº¿")
    
    # æ£€æŸ¥é¦–å°¾ä¸‹åˆ’çº¿
    if table_name.startswith('_') or table_name.endswith('_'):
        issues.append("è¡¨åä»¥ä¸‹åˆ’çº¿å¼€å¤´æˆ–ç»“å°¾")
    
    # æ£€æŸ¥ç©ºçš„æ®µ
    parts = table_name.split('_')
    empty_parts = [i for i, part in enumerate(parts) if not part.strip()]
    if empty_parts:
        issues.append(f"è¡¨ååŒ…å«ç©ºæ®µ: ä½ç½® {empty_parts}")
    
    return {
        'valid': len(issues) == 0,
        'issues': issues,
        'length': len(table_name)
    }

def create_table_for_subdataset(eng, subdataset_name, subdataset_metadata=None, base_table_name='clips_bbox'):
    """ä¸ºç‰¹å®šå­æ•°æ®é›†åˆ›å»ºåˆ†è¡¨ï¼Œæ”¯æŒæ ¹æ®metadataåŠ¨æ€æ·»åŠ å­—æ®µ"""
    table_name = get_table_name_for_subdataset(subdataset_name)
    
    # æ£€æŸ¥è¡¨æ˜¯å¦å·²å­˜åœ¨
    check_table_sql = text(f"""
        SELECT EXISTS (
            SELECT FROM information_schema.tables 
            WHERE table_schema = 'public' 
            AND table_name = '{table_name}'
        );
    """)
    
    try:
        with eng.connect() as conn:
            result = conn.execute(check_table_sql)
            table_exists = result.scalar()
            
            if table_exists:
                print(f"åˆ†è¡¨ {table_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return True, table_name
                
            print(f"åˆ›å»ºå­æ•°æ®é›†åˆ†è¡¨: {table_name}")
            
            # åŸºç¡€å­—æ®µï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            base_fields = [
                "id serial PRIMARY KEY",
                "scene_token text",
                "data_name text UNIQUE", 
                "event_id text",
                "city_id text",
                '"timestamp" bigint',
                "all_good boolean"
            ]
            
            # æ ¹æ®metadataåŠ¨æ€æ·»åŠ å­—æ®µ
            dynamic_fields = []
            if subdataset_metadata:
                # æ£€æµ‹æ˜¯å¦ä¸ºé—®é¢˜å•æ•°æ®é›†
                data_type = subdataset_metadata.get('data_type')
                if data_type == 'defect':
                    # ä¸ºé—®é¢˜å•æ•°æ®é›†æ·»åŠ ç‰¹æ®Šå­—æ®µ
                    dynamic_fields.append("data_type text DEFAULT 'defect'")
                    dynamic_fields.append("original_url text")
                    
                    # åˆ†æscene_attributesä¸­çš„æ‰€æœ‰å­—æ®µç±»å‹
                    scene_attributes = subdataset_metadata.get('scene_attributes', {})
                    all_custom_fields = set()
                    field_types = {}
                    
                    for scene_attrs in scene_attributes.values():
                        for key, value in scene_attrs.items():
                            if key not in {'original_url', 'data_name'}:
                                all_custom_fields.add(key)
                                # æ¨æ–­å­—æ®µç±»å‹ï¼ˆå–æœ€å®½æ³›çš„ç±»å‹ï¼‰
                                inferred_type = infer_field_type(value)
                                if key not in field_types:
                                    field_types[key] = inferred_type
                                else:
                                    # å¦‚æœå·²æœ‰ç±»å‹ï¼Œé€‰æ‹©æ›´å®½æ³›çš„ç±»å‹
                                    current_type = field_types[key]
                                    new_type = merge_field_types(current_type, inferred_type)
                                    field_types[key] = new_type
                    
                    # æ·»åŠ åŠ¨æ€å­—æ®µ
                    for field_name in sorted(all_custom_fields):
                        field_type = field_types.get(field_name, 'text')
                        dynamic_fields.append(f"{field_name} {field_type}")
                        print(f"  æ·»åŠ åŠ¨æ€å­—æ®µ: {field_name} ({field_type})")
                else:
                    # æ ‡å‡†æ•°æ®é›†ï¼Œæ·»åŠ data_typeæ ‡è¯†
                    dynamic_fields.append("data_type text DEFAULT 'standard'")
            else:
                # å‘åå…¼å®¹ï¼šæ²¡æœ‰metadataçš„æƒ…å†µ
                dynamic_fields.append("data_type text DEFAULT 'standard'")
            
            # ç»„åˆæ‰€æœ‰å­—æ®µ
            all_fields = base_fields + dynamic_fields
            fields_sql = ",\n                ".join(all_fields)
            
            # åˆ›å»ºè¡¨çš„SQL
            create_sql = text(f"""
                CREATE TABLE {table_name}(
                {fields_sql}
                );
            """)
            
            # ä½¿ç”¨PostGISæ·»åŠ å‡ ä½•åˆ—
            add_geom_sql = text(f"""
                SELECT AddGeometryColumn('public', '{table_name}', 'geometry', 4326, 'POLYGON', 2);
            """)
            
            # æ·»åŠ å‡ ä½•çº¦æŸ
            constraint_sql = text(f"""
                ALTER TABLE {table_name} ADD CONSTRAINT check_{table_name}_geom_type 
                    CHECK (ST_GeometryType(geometry) IN ('ST_Polygon', 'ST_Point'));
            """)
            
            # åˆ›å»ºç´¢å¼•
            index_sql = text(f"""
                CREATE INDEX idx_{table_name}_geometry ON {table_name} USING GIST(geometry);
                CREATE INDEX idx_{table_name}_data_name ON {table_name}(data_name);
                CREATE INDEX idx_{table_name}_scene_token ON {table_name}(scene_token);
                CREATE INDEX idx_{table_name}_data_type ON {table_name}(data_type);
            """)
            
            # æ‰§è¡ŒSQLè¯­å¥ï¼Œéœ€è¦åˆ†æ­¥æäº¤ä»¥ç¡®ä¿PostGISå‡½æ•°èƒ½æ‰¾åˆ°è¡¨
            conn.execute(create_sql)
            conn.commit()  # å…ˆæäº¤è¡¨åˆ›å»º
            
            # æ‰§è¡ŒPostGISç›¸å…³æ“ä½œ
            conn.execute(add_geom_sql)
            conn.execute(constraint_sql)
            conn.commit()  # æäº¤å‡ ä½•åˆ—å’Œçº¦æŸ
            
            # åˆ›å»ºç´¢å¼•
            conn.execute(index_sql)
            conn.commit()  # æœ€åæäº¤ç´¢å¼•
            
            print(f"æˆåŠŸåˆ›å»ºåˆ†è¡¨ {table_name} åŠç›¸å…³ç´¢å¼•")
            if dynamic_fields:
                print(f"  åŒ…å« {len(dynamic_fields)} ä¸ªåŠ¨æ€å­—æ®µ")
            
            return True, table_name
            
    except Exception as e:
        print(f"åˆ›å»ºåˆ†è¡¨ {table_name} æ—¶å‡ºé”™: {str(e)}")
        return False, table_name

def infer_field_type(value):
    """æ¨æ–­å­—æ®µç±»å‹"""
    if isinstance(value, bool):
        return "boolean"
    elif isinstance(value, int):
        return "integer"
    elif isinstance(value, float):
        return "numeric"
    elif isinstance(value, str):
        # å°è¯•æ£€æµ‹ç‰¹æ®Šæ ¼å¼
        if value.lower() in ('true', 'false'):
            return "boolean"
        try:
            int(value)
            return "integer"
        except ValueError:
            try:
                float(value)
                return "numeric"
            except ValueError:
                return "text"
    else:
        return "text"

def merge_field_types(type1: str, type2: str) -> str:
    """åˆå¹¶ä¸¤ä¸ªå­—æ®µç±»å‹ï¼Œé€‰æ‹©æ›´å®½æ³›çš„ç±»å‹
    
    ç±»å‹ä¼˜å…ˆçº§ï¼ˆä»çª„åˆ°å®½ï¼‰ï¼šboolean < integer < numeric < text
    
    Args:
        type1: ç¬¬ä¸€ä¸ªç±»å‹
        type2: ç¬¬äºŒä¸ªç±»å‹
        
    Returns:
        åˆå¹¶åçš„ç±»å‹
    """
    # å®šä¹‰ç±»å‹ä¼˜å…ˆçº§
    type_priority = {
        'boolean': 1,
        'integer': 2, 
        'numeric': 3,
        'text': 4
    }
    
    # è·å–ä¼˜å…ˆçº§ï¼ŒæœªçŸ¥ç±»å‹é»˜è®¤ä¸ºtext
    priority1 = type_priority.get(type1, 4)
    priority2 = type_priority.get(type2, 4)
    
    # è¿”å›ä¼˜å…ˆçº§æ›´é«˜ï¼ˆæ›´å®½æ³›ï¼‰çš„ç±»å‹
    if priority1 >= priority2:
        return type1
    else:
        return type2

def convert_value_to_expected_type(field_name: str, value):
    """æ ¹æ®å­—æ®µåå’Œå€¼ï¼Œè½¬æ¢ä¸ºåˆé€‚çš„æ•°æ®ç±»å‹
    
    Args:
        field_name: å­—æ®µåç§°
        value: åŸå§‹å€¼
        
    Returns:
        è½¬æ¢åçš„å€¼
    """
    if value is None:
        return None
    
    # ç‰¹æ®Šå­—æ®µçš„ç±»å‹å¤„ç†å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
    
    # æ¨æ–­å­—æ®µç±»å‹å¹¶è½¬æ¢
    inferred_type = infer_field_type(value)
    
    try:
        if inferred_type == "boolean":
            if isinstance(value, str):
                return value.lower() in ('true', '1', 'yes', 'on')
            else:
                return bool(value)
        elif inferred_type == "integer":
            if isinstance(value, str):
                return int(float(value))  # å¤„ç†"15.0"è¿™ç§æƒ…å†µ
            elif isinstance(value, float):
                return int(value)
            else:
                return int(value)
        elif inferred_type == "numeric":
            return float(value)
        else:
            return str(value)
    except (ValueError, TypeError) as e:
        print(f"è­¦å‘Š: å­—æ®µ {field_name} çš„å€¼ {value} ç±»å‹è½¬æ¢å¤±è´¥: {e}ï¼Œä½¿ç”¨å­—ç¬¦ä¸²ç±»å‹")
        return str(value)

def group_scenes_by_subdataset(dataset_file: str) -> Dict[str, Dict]:
    """æŒ‰å­æ•°æ®é›†åˆ†ç»„scene_idsï¼ŒåŒ…å«metadataä¿¡æ¯
    
    Args:
        dataset_file: datasetæ–‡ä»¶è·¯å¾„ï¼ˆJSON/Parquetæ ¼å¼ï¼‰
        
    Returns:
        å­—å…¸ï¼Œkeyä¸ºå­æ•°æ®é›†åç§°ï¼Œvalueä¸ºåŒ…å«scene_idså’Œmetadataçš„å­—å…¸
        æ ¼å¼ï¼š{subdataset_name: {'scene_ids': [...], 'metadata': {...}}}
    """
    from ..dataset.dataset_manager import DatasetManager
    
    try:
        dataset_manager = DatasetManager()
        dataset = dataset_manager.load_dataset(dataset_file)
        
        groups = {}
        total_scenes = 0
        
        print(f"ä»æ•°æ®é›†æ–‡ä»¶åŠ è½½: {dataset_file}")
        print(f"æ•°æ®é›†åç§°: {dataset.name}")
        print(f"å­æ•°æ®é›†æ•°é‡: {len(dataset.subdatasets)}")
        
        for subdataset in dataset.subdatasets:
            subdataset_name = subdataset.name
            scene_ids = subdataset.scene_ids
            metadata = subdataset.metadata or {}
            
            if scene_ids:
                groups[subdataset_name] = {
                    'scene_ids': scene_ids,
                    'metadata': metadata
                }
                total_scenes += len(scene_ids)
                
                # æ˜¾ç¤ºæ•°æ®é›†ç±»å‹
                data_type = metadata.get('data_type', 'standard')
                type_info = f" (ç±»å‹: {data_type})"
                if data_type == 'defect':
                    dynamic_fields = [k for k in metadata.keys() 
                                    if k not in {'data_type', 'original_url', 'data_name', 'line_number'} 
                                    and not k.startswith('data_')]
                    if dynamic_fields:
                        type_info += f", åŠ¨æ€å­—æ®µ: {len(dynamic_fields)}ä¸ª"
                
                print(f"  {subdataset_name}: {len(scene_ids)} ä¸ªåœºæ™¯{type_info}")
            else:
                print(f"  {subdataset_name}: æ— åœºæ™¯æ•°æ®ï¼Œè·³è¿‡")
        
        print(f"æ€»è®¡: {len(groups)} ä¸ªæœ‰æ•ˆå­æ•°æ®é›†ï¼Œ{total_scenes} ä¸ªåœºæ™¯")
        return groups
        
    except Exception as e:
        print(f"åˆ†ç»„scene_idså¤±è´¥: {str(e)}")
        raise

def batch_create_tables_for_subdatasets(eng, subdataset_groups: Dict[str, Dict]) -> Dict[str, str]:
    """æ‰¹é‡ä¸ºå­æ•°æ®é›†åˆ›å»ºåˆ†è¡¨ï¼Œæ”¯æŒåŠ¨æ€å­—æ®µ
    
    Args:
        eng: æ•°æ®åº“å¼•æ“
        subdataset_groups: å­æ•°æ®é›†åˆ†ç»„ä¿¡æ¯ï¼ŒåŒ…å«scene_idså’Œmetadata
                          æ ¼å¼ï¼š{subdataset_name: {'scene_ids': [...], 'metadata': {...}}}
        
    Returns:
        å­—å…¸ï¼Œkeyä¸ºåŸå§‹å­æ•°æ®é›†åç§°ï¼Œvalueä¸ºåˆ›å»ºçš„è¡¨å
    """
    table_mapping = {}
    success_count = 0
    
    print(f"å¼€å§‹æ‰¹é‡åˆ›å»º {len(subdataset_groups)} ä¸ªåˆ†è¡¨...")
    
    for i, (subdataset_name, subdataset_info) in enumerate(subdataset_groups.items(), 1):
        metadata = subdataset_info.get('metadata', {})
        data_type = metadata.get('data_type', 'standard')
        
        print(f"[{i}/{len(subdataset_groups)}] å¤„ç†: {subdataset_name} (ç±»å‹: {data_type})")
        
        success, table_name = create_table_for_subdataset(eng, subdataset_name, metadata)
        table_mapping[subdataset_name] = table_name
        
        if success:
            success_count += 1
        else:
            print(f"è­¦å‘Š: å­æ•°æ®é›† {subdataset_name} çš„åˆ†è¡¨åˆ›å»ºå¤±è´¥")
    
    print(f"æ‰¹é‡åˆ›å»ºå®Œæˆ: æˆåŠŸ {success_count}/{len(subdataset_groups)} ä¸ªåˆ†è¡¨")
    return table_mapping

def filter_partition_tables(tables: List[str], exclude_view: str = None, exclude_defect_tables: bool = True) -> List[str]:
    """è¿‡æ»¤å‡ºçœŸæ­£çš„åˆ†è¡¨ï¼Œæ’é™¤ä¸»è¡¨ã€è§†å›¾ã€ä¸´æ—¶è¡¨ç­‰
    
    Args:
        tables: è¡¨ååˆ—è¡¨
        exclude_view: è¦æ’é™¤çš„è§†å›¾åç§°
        exclude_defect_tables: æ˜¯å¦æ’é™¤é—®é¢˜å•æ•°æ®è¡¨
        
    Returns:
        è¿‡æ»¤åçš„åˆ†è¡¨åˆ—è¡¨
    """
    filtered = []
    
    for table in tables:
        # æ’é™¤ä¸»è¡¨
        if table == 'clips_bbox':
            continue
            
        # æ’é™¤æŒ‡å®šçš„è§†å›¾ï¼ˆé¿å…å¾ªç¯å¼•ç”¨ï¼‰
        if exclude_view and table == exclude_view:
            continue
            
        # æ’é™¤åŒ…å«ç‰¹å®šå…³é”®è¯çš„è¡¨
        exclude_keywords = ['unified', 'temp', 'backup', 'test', 'tmp']
        if any(keyword in table.lower() for keyword in exclude_keywords):
            continue
            
        # åªåŒ…å«åˆ†è¡¨æ ¼å¼çš„è¡¨ï¼ˆå¿…é¡»ä»¥clips_bbox_å¼€å¤´ï¼‰
        if not (table.startswith('clips_bbox_') and table != 'clips_bbox'):
            continue
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé—®é¢˜å•æ•°æ®è¡¨ï¼ˆç®€åŒ–å®ç°ï¼ŒåŸºäºè¡¨åæ¨æ–­ï¼‰
        if exclude_defect_tables:
            try:
                from sqlalchemy import create_engine, text
                eng = create_engine(LOCAL_DSN, future=True)
                with eng.connect() as conn:
                    # æ£€æŸ¥è¡¨æ˜¯å¦åŒ…å«data_typeå­—æ®µä¸”å€¼ä¸º'defect'
                    check_defect_sql = text(f"""
                        SELECT EXISTS (
                            SELECT 1 FROM information_schema.columns 
                            WHERE table_name = '{table}' 
                            AND column_name = 'data_type'
                        );
                    """)
                    
                    has_data_type = conn.execute(check_defect_sql).scalar()
                    
                    if has_data_type:
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«é—®é¢˜å•æ•°æ®
                        check_defect_data_sql = text(f"""
                            SELECT EXISTS (
                                SELECT 1 FROM {table} 
                                WHERE data_type = 'defect' 
                                LIMIT 1
                            );
                        """)
                        
                        has_defect_data = conn.execute(check_defect_data_sql).scalar()
                        
                        if has_defect_data:
                            print(f"æ’é™¤é—®é¢˜å•æ•°æ®è¡¨: {table}")
                            continue
                            
            except Exception as e:
                # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œè®°å½•ä½†ä¸å½±å“è¿‡æ»¤
                print(f"æ£€æŸ¥è¡¨ {table} çš„æ•°æ®ç±»å‹æ—¶å‡ºé”™: {str(e)}")
        
        filtered.append(table)
    
    return filtered

def list_bbox_tables(eng) -> List[str]:
    """åˆ—å‡ºæ‰€æœ‰bboxç›¸å…³çš„è¡¨
    
    Args:
        eng: æ•°æ®åº“å¼•æ“
        
    Returns:
        bboxè¡¨ååˆ—è¡¨
    """
    list_tables_sql = text("""
        SELECT table_name 
        FROM information_schema.tables 
        WHERE table_schema = 'public' 
        AND table_name LIKE 'clips_bbox%'
        ORDER BY table_name;
    """)
    
    try:
        with eng.connect() as conn:
            result = conn.execute(list_tables_sql)
            tables = [row[0] for row in result.fetchall()]
            return tables
    except Exception as e:
        print(f"åˆ—å‡ºbboxè¡¨å¤±è´¥: {str(e)}")
        return []

def create_unified_view(eng, view_name: str = 'clips_bbox_unified') -> bool:
    """åˆ›å»ºç»Ÿä¸€è§†å›¾ï¼Œèšåˆæ‰€æœ‰åˆ†è¡¨æ•°æ®
    
    Args:
        eng: æ•°æ®åº“å¼•æ“
        view_name: ç»Ÿä¸€è§†å›¾åç§°
        
    Returns:
        åˆ›å»ºæ˜¯å¦æˆåŠŸ
    """
    try:
        # è·å–æ‰€æœ‰bboxç›¸å…³è¡¨
        all_tables = list_bbox_tables(eng)
        if not all_tables:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•bboxè¡¨ï¼Œæ— æ³•åˆ›å»ºç»Ÿä¸€è§†å›¾")
            return False
        
        # è¿‡æ»¤å‡ºçœŸæ­£çš„åˆ†è¡¨ï¼Œæ’é™¤è§†å›¾ã€ä¸»è¡¨ç­‰
        bbox_tables = filter_partition_tables(all_tables, exclude_view=view_name)
        if not bbox_tables:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†è¡¨ï¼Œæ— æ³•åˆ›å»ºç»Ÿä¸€è§†å›¾")
            print(f"å¯ç”¨çš„è¡¨: {all_tables}")
            return False
        
        # æ„å»ºUNION ALLæŸ¥è¯¢
        union_parts = []
        for table_name in bbox_tables:
            # æå–å­æ•°æ®é›†åç§°ï¼ˆå»æ‰clips_bbox_å‰ç¼€ï¼‰
            subdataset_name = table_name.replace('clips_bbox_', '') if table_name.startswith('clips_bbox_') else table_name
            
            union_part = f"""
            SELECT 
                {table_name}.id,
                {table_name}.scene_token,
                {table_name}.data_name,
                {table_name}.event_id,
                {table_name}.city_id,
                {table_name}.timestamp,
                {table_name}.all_good,
                {table_name}.geometry,
                '{subdataset_name}' as subdataset_name,
                '{table_name}' as source_table
            FROM {table_name}
            """
            union_parts.append(union_part)
        
        # ç»„åˆå®Œæ•´çš„è§†å›¾æŸ¥è¯¢
        view_query = "UNION ALL\n".join(union_parts)
        
        # å…ˆåˆ é™¤ç°æœ‰è§†å›¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        drop_view_sql = text(f"DROP VIEW IF EXISTS {view_name};")
        
        # åˆ›å»ºæ–°è§†å›¾
        create_view_sql = text(f"""
            CREATE OR REPLACE VIEW {view_name} AS
            {view_query};
        """)
        
        with eng.connect() as conn:
            conn.execute(drop_view_sql)
            conn.execute(create_view_sql)
            conn.commit()
        
        print(f"æˆåŠŸåˆ›å»ºç»Ÿä¸€è§†å›¾ {view_name}ï¼ŒåŒ…å« {len(bbox_tables)} ä¸ªåˆ†è¡¨:")
        for table in bbox_tables:
            print(f"  - {table}")
        
        return True
        
    except Exception as e:
        print(f"åˆ›å»ºç»Ÿä¸€è§†å›¾å¤±è´¥: {str(e)}")
        
        # æä¾›è°ƒè¯•ä¿¡æ¯
        try:
            print(f"è°ƒè¯•ä¿¡æ¯:")
            print(f"  - æ‰¾åˆ°çš„è¡¨æ•°é‡: {len(bbox_tables) if 'bbox_tables' in locals() else 'N/A'}")
            if 'bbox_tables' in locals() and bbox_tables:
                print(f"  - è¡¨åˆ—è¡¨: {', '.join(bbox_tables)}")
            
            # æ˜¾ç¤ºç”Ÿæˆçš„æŸ¥è¯¢ï¼ˆå‰100ä¸ªå­—ç¬¦ï¼‰
            if 'view_query' in locals():
                query_preview = view_query[:200] + "..." if len(view_query) > 200 else view_query
                print(f"  - ç”Ÿæˆçš„æŸ¥è¯¢é¢„è§ˆ: {query_preview}")
                
        except Exception as debug_e:
            print(f"  - æ— æ³•æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯: {str(debug_e)}")
        
        return False

def create_qgis_compatible_unified_view(eng, view_name: str = 'clips_bbox_unified_qgis') -> bool:
    """
    åˆ›å»ºQGISå…¼å®¹çš„ç»Ÿä¸€è§†å›¾ï¼Œå¸¦å…¨å±€å”¯ä¸€ID
    
    Args:
        eng: SQLAlchemy engine
        view_name: è§†å›¾åç§°
        
    Returns:
        bool: åˆ›å»ºæ˜¯å¦æˆåŠŸ
    """
    try:
        # è·å–åˆ†è¡¨åˆ—è¡¨
        all_tables = list_bbox_tables(eng)
        bbox_tables = filter_partition_tables(all_tables, exclude_view=view_name)
        
        if not bbox_tables:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†è¡¨ï¼Œæ— æ³•åˆ›å»ºQGISå…¼å®¹çš„ç»Ÿä¸€è§†å›¾")
            return False
        
        print(f"æ­£åœ¨ä¸º {len(bbox_tables)} ä¸ªåˆ†è¡¨åˆ›å»ºQGISå…¼å®¹çš„ç»Ÿä¸€è§†å›¾...")
        
        # æ„å»ºå¸¦ROW_NUMBERçš„UNIONæŸ¥è¯¢
        union_parts = []
        for table_name in bbox_tables:
            subdataset_name = table_name.replace('clips_bbox_', '') if table_name.startswith('clips_bbox_') else table_name
            
            union_part = f"""
            SELECT 
                {table_name}.id as original_id,
                {table_name}.scene_token,
                {table_name}.data_name,
                {table_name}.event_id,
                {table_name}.city_id,
                {table_name}.timestamp,
                {table_name}.all_good,
                {table_name}.geometry,
                '{subdataset_name}' as subdataset_name,
                '{table_name}' as source_table
            FROM {table_name}
            """
            union_parts.append(union_part)
        
        # åŒ…è£…åœ¨ROW_NUMBERä¸­åˆ›å»ºå…¨å±€å”¯ä¸€ID
        inner_query = "UNION ALL\n".join(union_parts)
        
        view_query = f"""
        SELECT 
            ROW_NUMBER() OVER (ORDER BY source_table, original_id) as qgis_id,
            original_id,
            scene_token,
            data_name,
            event_id,
            city_id,
            timestamp,
            all_good,
            geometry,
            subdataset_name,
            source_table
        FROM (
            {inner_query}
        ) as unified_data
        """
        
        # åˆ›å»ºè§†å›¾
        drop_view_sql = text(f"DROP VIEW IF EXISTS {view_name};")
        create_view_sql = text(f"CREATE OR REPLACE VIEW {view_name} AS {view_query};")
        
        with eng.connect() as conn:
            conn.execute(drop_view_sql)
            conn.execute(create_view_sql)
            conn.commit()
        
        print(f"âœ… æˆåŠŸåˆ›å»ºQGISå…¼å®¹çš„ç»Ÿä¸€è§†å›¾ {view_name}")
        print(f"ğŸ“‹ åœ¨QGISä¸­åŠ è½½æ—¶ï¼Œè¯·é€‰æ‹© 'qgis_id' ä½œä¸ºä¸»é”®åˆ—")
        print(f"ğŸ” è§†å›¾åŒ…å«ä»¥ä¸‹åˆ†è¡¨: {', '.join(bbox_tables)}")
        
        return True
        
    except Exception as e:
        print(f"åˆ›å»ºQGISå…¼å®¹ç»Ÿä¸€è§†å›¾å¤±è´¥: {str(e)}")
        return False

def create_materialized_unified_view(eng, view_name: str = 'clips_bbox_unified_mat') -> bool:
    """
    åˆ›å»ºç‰©åŒ–è§†å›¾ï¼Œæä¾›æ›´å¥½çš„QGISæ€§èƒ½
    
    Args:
        eng: SQLAlchemy engine
        view_name: ç‰©åŒ–è§†å›¾åç§°
        
    Returns:
        bool: åˆ›å»ºæ˜¯å¦æˆåŠŸ
    """
    try:
        # è·å–åˆ†è¡¨åˆ—è¡¨
        all_tables = list_bbox_tables(eng)
        bbox_tables = filter_partition_tables(all_tables, exclude_view=view_name)
        
        if not bbox_tables:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ†è¡¨ï¼Œæ— æ³•åˆ›å»ºç‰©åŒ–è§†å›¾")
            return False
        
        print(f"æ­£åœ¨ä¸º {len(bbox_tables)} ä¸ªåˆ†è¡¨åˆ›å»ºç‰©åŒ–è§†å›¾...")
        
        # æ„å»ºUNIONæŸ¥è¯¢
        union_parts = []
        for table_name in bbox_tables:
            subdataset_name = table_name.replace('clips_bbox_', '') if table_name.startswith('clips_bbox_') else table_name
            
            union_part = f"""
            SELECT 
                {table_name}.id as original_id,
                {table_name}.scene_token,
                {table_name}.data_name,
                {table_name}.event_id,
                {table_name}.city_id,
                {table_name}.timestamp,
                {table_name}.all_good,
                {table_name}.geometry,
                '{subdataset_name}' as subdataset_name,
                '{table_name}' as source_table
            FROM {table_name}
            """
            union_parts.append(union_part)
        
        inner_query = "UNION ALL\n".join(union_parts)
        
        # åˆ›å»ºç‰©åŒ–è§†å›¾SQL
        drop_view_sql = text(f"DROP MATERIALIZED VIEW IF EXISTS {view_name};")
        
        create_mat_view_sql = text(f"""
            CREATE MATERIALIZED VIEW {view_name} AS
            SELECT 
                ROW_NUMBER() OVER (ORDER BY source_table, original_id) as qgis_id,
                original_id,
                scene_token,
                data_name,
                event_id,
                city_id,
                timestamp,
                all_good,
                geometry,
                subdataset_name,
                source_table
            FROM (
                {inner_query}
            ) as unified_data;
        """)
        
        # åˆ›å»ºç´¢å¼•
        create_index_sql = text(f"""
            CREATE UNIQUE INDEX IF NOT EXISTS {view_name}_qgis_id_idx 
            ON {view_name} (qgis_id);
        """)
        
        create_spatial_index_sql = text(f"""
            CREATE INDEX IF NOT EXISTS {view_name}_geom_idx 
            ON {view_name} USING GIST (geometry);
        """)
        
        with eng.connect() as conn:
            conn.execute(drop_view_sql)
            conn.execute(create_mat_view_sql)
            conn.execute(create_index_sql)
            conn.execute(create_spatial_index_sql)
            conn.commit()
        
        print(f"âœ… æˆåŠŸåˆ›å»ºç‰©åŒ–è§†å›¾ {view_name}")
        print(f"ğŸ“‹ åœ¨QGISä¸­ä½¿ç”¨ 'qgis_id' ä½œä¸ºä¸»é”®åˆ—")
        print(f"ğŸ’¡ æç¤ºï¼šæ•°æ®æ›´æ–°åéœ€è¦åˆ·æ–°ç‰©åŒ–è§†å›¾ï¼šREFRESH MATERIALIZED VIEW {view_name};")
        print(f"ğŸ” ç‰©åŒ–è§†å›¾åŒ…å«ä»¥ä¸‹åˆ†è¡¨: {', '.join(bbox_tables)}")
        
        return True
        
    except Exception as e:
        print(f"åˆ›å»ºç‰©åŒ–è§†å›¾å¤±è´¥: {str(e)}")
        return False

def refresh_materialized_view(eng, view_name: str = 'clips_bbox_unified_mat') -> bool:
    """
    åˆ·æ–°ç‰©åŒ–è§†å›¾
    
    Args:
        eng: SQLAlchemy engine
        view_name: ç‰©åŒ–è§†å›¾åç§°
        
    Returns:
        bool: åˆ·æ–°æ˜¯å¦æˆåŠŸ
    """
    try:
        refresh_sql = text(f"REFRESH MATERIALIZED VIEW {view_name};")
        
        with eng.connect() as conn:
            print(f"æ­£åœ¨åˆ·æ–°ç‰©åŒ–è§†å›¾ {view_name}...")
            conn.execute(refresh_sql)
            conn.commit()
        
        print(f"âœ… ç‰©åŒ–è§†å›¾ {view_name} åˆ·æ–°å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"åˆ·æ–°ç‰©åŒ–è§†å›¾å¤±è´¥: {str(e)}")
        return False

def maintain_unified_view(eng, view_name: str = 'clips_bbox_unified') -> bool:
    """ç»´æŠ¤ç»Ÿä¸€è§†å›¾ï¼Œç¡®ä¿åŒ…å«æ‰€æœ‰å½“å‰çš„åˆ†è¡¨
    
    Args:
        eng: æ•°æ®åº“å¼•æ“  
        view_name: ç»Ÿä¸€è§†å›¾åç§°
        
    Returns:
        ç»´æŠ¤æ˜¯å¦æˆåŠŸ
    """
    try:
        # æ£€æŸ¥è§†å›¾æ˜¯å¦å­˜åœ¨
        check_view_sql = text(f"""
            SELECT EXISTS (
                SELECT FROM information_schema.views 
                WHERE table_schema = 'public' 
                AND table_name = '{view_name}'
            );
        """)
        
        with eng.connect() as conn:
            result = conn.execute(check_view_sql)
            view_exists = result.scalar()
        
        if not view_exists:
            print(f"è§†å›¾ {view_name} ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°è§†å›¾")
            return create_unified_view(eng, view_name)
        else:
            print(f"è§†å›¾ {view_name} å·²å­˜åœ¨ï¼Œé‡æ–°åˆ›å»ºä»¥åŒ…å«æœ€æ–°çš„åˆ†è¡¨")
            return create_unified_view(eng, view_name)
        
    except Exception as e:
        print(f"ç»´æŠ¤ç»Ÿä¸€è§†å›¾å¤±è´¥: {str(e)}")
        return False

def process_subdataset_parallel(args):
    """å¹¶è¡Œå¤„ç†å•ä¸ªå­æ•°æ®é›†çš„åŒ…è£…å‡½æ•°
    
    Args:
        args: (subdataset_name, scene_ids, table_name, batch_size, insert_batch_size, work_dir, dsn, metadata)
        
    Returns:
        (subdataset_name, processed_count, inserted_count, success)
    """
    subdataset_name, scene_ids, table_name, batch_size, insert_batch_size, work_dir, dsn, metadata = args
    
    try:
        # ä¸ºæ¯ä¸ªè¿›ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥
        from sqlalchemy import create_engine
        eng = create_engine(dsn, future=True)
        
        # åˆ›å»ºç‹¬ç«‹çš„è¿›åº¦è·Ÿè¸ªå™¨
        sub_work_dir = f"{work_dir}/{subdataset_name}"
        sub_tracker = LightweightProgressTracker(sub_work_dir)
        
        # è·å–éœ€è¦å¤„ç†çš„åœºæ™¯ID
        remaining_scene_ids = sub_tracker.get_remaining_tokens(scene_ids)
        
        if not remaining_scene_ids:
            print(f"  ğŸ”„ [{subdataset_name}] æ‰€æœ‰åœºæ™¯å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡")
            return subdataset_name, 0, 0, True
        
        print(f"  ğŸš€ [{subdataset_name}] å¼€å§‹å¤„ç† {len(remaining_scene_ids)} ä¸ªåœºæ™¯")
        
        # å¤„ç†å½“å‰å­æ•°æ®é›†çš„æ•°æ®
        processed_count, inserted_count = process_subdataset_scenes(
            eng, remaining_scene_ids, table_name, batch_size, insert_batch_size, sub_tracker, metadata
        )
        
        print(f"  âœ… [{subdataset_name}] å®Œæˆ: å¤„ç† {processed_count} ä¸ªï¼Œæ’å…¥ {inserted_count} æ¡è®°å½•")
        
        return subdataset_name, processed_count, inserted_count, True
        
    except Exception as e:
        print(f"  âŒ [{subdataset_name}] å¤„ç†å¤±è´¥: {str(e)}")
        return subdataset_name, 0, 0, False

def run_with_partitioning_parallel(input_path, batch=1000, insert_batch=1000, work_dir="./bbox_import_logs", 
                                 create_unified_view_flag=True, maintain_view_only=False, max_workers=None):
    """ä½¿ç”¨å¹¶è¡Œåˆ†è¡¨æ¨¡å¼è¿è¡Œè¾¹ç•Œæ¡†å¤„ç†
    
    Args:
        input_path: è¾“å…¥æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        batch: å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch: æ’å…¥æ‰¹æ¬¡å¤§å°  
        work_dir: å·¥ä½œç›®å½•
        create_unified_view_flag: æ˜¯å¦åˆ›å»ºç»Ÿä¸€è§†å›¾
        maintain_view_only: æ˜¯å¦åªç»´æŠ¤è§†å›¾ï¼ˆä¸å¤„ç†æ•°æ®ï¼‰
        max_workers: æœ€å¤§å¹¶è¡Œworkeræ•°é‡ï¼ŒNoneä¸ºè‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
    """
    global interrupted
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()
    
    # ç¡®å®šå¹¶è¡Œworkeræ•°é‡
    if max_workers is None:
        # æ™ºèƒ½é»˜è®¤å€¼ï¼šCPUæ ¸å¿ƒæ•° * 1.5ï¼Œä½†ä¸è¶…è¿‡16ï¼ˆå¯é€šè¿‡å‚æ•°è¦†ç›–ï¼‰
        cpu_count = mp.cpu_count()
        max_workers = min(int(cpu_count * 1.5), 16)
        print(f"ğŸ” æ£€æµ‹åˆ° {cpu_count} ä¸ªCPUæ ¸å¿ƒï¼Œé»˜è®¤ä½¿ç”¨ {max_workers} ä¸ªworkers")
    else:
        print(f"ğŸ¯ ç”¨æˆ·æŒ‡å®šä½¿ç”¨ {max_workers} ä¸ªworkers")
    
    print(f"=== å¹¶è¡Œåˆ†è¡¨æ¨¡å¼å¤„ç†å¼€å§‹ ===")
    print(f"è¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"å·¥ä½œç›®å½•: {work_dir}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch}")
    print(f"æ’å…¥æ‰¹æ¬¡å¤§å°: {insert_batch}")
    print(f"å¹¶è¡Œworkeræ•°: {max_workers}")
    print(f"åˆ›å»ºç»Ÿä¸€è§†å›¾: {create_unified_view_flag}")
    print(f"ä»…ç»´æŠ¤è§†å›¾: {maintain_view_only}")
    
    eng = create_engine(LOCAL_DSN, future=True)
    
    # å¦‚æœåªæ˜¯ç»´æŠ¤è§†å›¾
    if maintain_view_only:
        print("\n=== ç»´æŠ¤ç»Ÿä¸€è§†å›¾æ¨¡å¼ ===")
        success = maintain_unified_view(eng)
        if success:
            print("âœ… ç»Ÿä¸€è§†å›¾ç»´æŠ¤å®Œæˆ")
        else:
            print("âŒ ç»Ÿä¸€è§†å›¾ç»´æŠ¤å¤±è´¥")
        return
    
    try:
        # æ­¥éª¤1: æŒ‰å­æ•°æ®é›†åˆ†ç»„åœºæ™¯
        print("\n=== æ­¥éª¤1: åˆ†ç»„åœºæ™¯æ•°æ® ===")
        scene_groups = group_scenes_by_subdataset(input_path)
        
        if not scene_groups:
            print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åœºæ™¯åˆ†ç»„æ•°æ®")
            return
        
        print(f"æ‰¾åˆ° {len(scene_groups)} ä¸ªå­æ•°æ®é›†")
        
        # æ­¥éª¤2: æ‰¹é‡åˆ›å»ºåˆ†è¡¨
        print("\n=== æ­¥éª¤2: åˆ›å»ºåˆ†è¡¨ ===")
        table_mapping = batch_create_tables_for_subdatasets(eng, scene_groups)
        
        # æ­¥éª¤3: å¹¶è¡Œå¤„ç†æ¯ä¸ªå­æ•°æ®é›†
        print(f"\n=== æ­¥éª¤3: å¹¶è¡Œåˆ†è¡¨æ•°æ®å¤„ç† ({max_workers} workers) ===")
        
        # å‡†å¤‡å¹¶è¡Œä»»åŠ¡å‚æ•°
        task_args = []
        for subdataset_name, subdataset_info in scene_groups.items():
            scene_ids = subdataset_info['scene_ids']
            metadata = subdataset_info.get('metadata', {})
            table_name = table_mapping[subdataset_name]
            task_args.append((
                subdataset_name, scene_ids, table_name, 
                batch, insert_batch, work_dir, LOCAL_DSN, metadata
            ))
        
        # æ‰§è¡Œå¹¶è¡Œå¤„ç†
        total_processed = 0
        total_inserted = 0
        completed_count = 0
        failed_count = 0
        
        print(f"å¯åŠ¨ {len(task_args)} ä¸ªå¹¶è¡Œä»»åŠ¡...")
        
        start_time = time.time()
        
        # ä½¿ç”¨ProcessPoolExecutorè¿›è¡Œå¹¶è¡Œå¤„ç†
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_subdataset = {
                executor.submit(process_subdataset_parallel, args): args[0] 
                for args in task_args
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_subdataset):
                if interrupted:
                    print("\nâš ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢å‰©ä½™ä»»åŠ¡...")
                    executor.shutdown(wait=False)
                    break
                
                subdataset_name = future_to_subdataset[future]
                try:
                    result_name, processed, inserted, success = future.result()
                    completed_count += 1
                    
                    if success:
                        total_processed += processed
                        total_inserted += inserted
                        print(f"âœ… [{completed_count}/{len(task_args)}] {subdataset_name}: {processed}å¤„ç†/{inserted}æ’å…¥")
                    else:
                        failed_count += 1
                        print(f"âŒ [{completed_count}/{len(task_args)}] {subdataset_name}: å¤„ç†å¤±è´¥")
                        
                except Exception as e:
                    failed_count += 1
                    print(f"âŒ [{completed_count}/{len(task_args)}] {subdataset_name}: å¼‚å¸¸ - {str(e)}")
        
        processing_time = time.time() - start_time
        
        # æ­¥éª¤4: åˆ›å»ºç»Ÿä¸€è§†å›¾ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if create_unified_view_flag and not interrupted:
            print("\n=== æ­¥éª¤4: åˆ›å»ºç»Ÿä¸€è§†å›¾ ===")
            success = create_unified_view(eng)
            if success:
                print("âœ… ç»Ÿä¸€è§†å›¾åˆ›å»ºå®Œæˆ")
            else:
                print("âŒ ç»Ÿä¸€è§†å›¾åˆ›å»ºå¤±è´¥")
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print(f"\n=== å¹¶è¡Œåˆ†è¡¨å¤„ç†å®Œæˆ ===")
        print(f"å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
        print(f"æ€»è®¡å¤„ç†: {total_processed} æ¡è®°å½•")
        print(f"æ€»è®¡æ’å…¥: {total_inserted} æ¡è®°å½•")
        print(f"æˆåŠŸå­æ•°æ®é›†: {completed_count - failed_count}/{len(scene_groups)}")
        if failed_count > 0:
            print(f"å¤±è´¥å­æ•°æ®é›†: {failed_count}")
        
        if interrupted:
            print("âš ï¸  å¤„ç†è¢«ä¸­æ–­ï¼Œéƒ¨åˆ†æ•°æ®å¯èƒ½æœªå®Œæˆ")
        else:
            print("âœ… å¹¶è¡Œåˆ†è¡¨å¤„ç†å®Œæˆ")
            
        # è®¡ç®—æ€§èƒ½æå‡ä¼°ç®—
        if processing_time > 0:
            estimated_sequential_time = processing_time * max_workers
            speedup = estimated_sequential_time / processing_time
            print(f"ğŸš€ é¢„è®¡æ€§èƒ½æå‡: {speedup:.1f}x (ç›¸æ¯”é¡ºåºå¤„ç†)")
            
    except KeyboardInterrupt:
        print(f"\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        interrupted = True
        interrupt_flag.set()
        interrupt_flag.set()
        interrupt_flag.set()
    except Exception as e:
        print(f"\nå¹¶è¡Œåˆ†è¡¨å¤„ç†é‡åˆ°é”™è¯¯: {str(e)}")
    finally:
        print(f"\næ—¥å¿—å’Œè¿›åº¦æ–‡ä»¶ä¿å­˜åœ¨: {work_dir}")

def run_with_partitioning(input_path, batch=1000, insert_batch=1000, work_dir="./bbox_import_logs", 
                         create_unified_view_flag=True, maintain_view_only=False, use_parallel=False, 
                         max_workers=None):
    """ä½¿ç”¨åˆ†è¡¨æ¨¡å¼è¿è¡Œè¾¹ç•Œæ¡†å¤„ç†ï¼ˆæ”¯æŒå¹¶è¡Œå’Œé¡ºåºæ¨¡å¼ï¼‰
    
    Args:
        input_path: è¾“å…¥æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        batch: å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch: æ’å…¥æ‰¹æ¬¡å¤§å°  
        work_dir: å·¥ä½œç›®å½•
        create_unified_view_flag: æ˜¯å¦åˆ›å»ºç»Ÿä¸€è§†å›¾
        maintain_view_only: æ˜¯å¦åªç»´æŠ¤è§†å›¾ï¼ˆä¸å¤„ç†æ•°æ®ï¼‰
        use_parallel: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œå¤„ç†æ¨¡å¼
        max_workers: æœ€å¤§å¹¶è¡Œworkeræ•°é‡ï¼ŒNoneä¸ºè‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
    """
    if use_parallel:
        # ä½¿ç”¨å¹¶è¡Œæ¨¡å¼
        return run_with_partitioning_parallel(
            input_path, batch, insert_batch, work_dir, 
            create_unified_view_flag, maintain_view_only, max_workers
        )
    else:
        # ä½¿ç”¨é¡ºåºæ¨¡å¼ï¼ˆåŸå§‹å®ç°ï¼‰
        return run_with_partitioning_sequential(
            input_path, batch, insert_batch, work_dir, 
            create_unified_view_flag, maintain_view_only
        )

def run_with_partitioning_sequential(input_path, batch=1000, insert_batch=1000, work_dir="./bbox_import_logs", 
                                   create_unified_view_flag=True, maintain_view_only=False):
    """ä½¿ç”¨é¡ºåºåˆ†è¡¨æ¨¡å¼è¿è¡Œè¾¹ç•Œæ¡†å¤„ç†ï¼ˆåŸå§‹å®ç°ï¼‰
    
    Args:
        input_path: è¾“å…¥æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        batch: å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch: æ’å…¥æ‰¹æ¬¡å¤§å°  
        work_dir: å·¥ä½œç›®å½•
        create_unified_view_flag: æ˜¯å¦åˆ›å»ºç»Ÿä¸€è§†å›¾
        maintain_view_only: æ˜¯å¦åªç»´æŠ¤è§†å›¾ï¼ˆä¸å¤„ç†æ•°æ®ï¼‰
    """
    global interrupted
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()
    
    print(f"=== åˆ†è¡¨æ¨¡å¼å¤„ç†å¼€å§‹ ===")
    print(f"è¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"å·¥ä½œç›®å½•: {work_dir}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch}")
    print(f"æ’å…¥æ‰¹æ¬¡å¤§å°: {insert_batch}")
    print(f"åˆ›å»ºç»Ÿä¸€è§†å›¾: {create_unified_view_flag}")
    print(f"ä»…ç»´æŠ¤è§†å›¾: {maintain_view_only}")
    
    eng = create_engine(LOCAL_DSN, future=True)
    
    # å¦‚æœåªæ˜¯ç»´æŠ¤è§†å›¾
    if maintain_view_only:
        print("\n=== ç»´æŠ¤ç»Ÿä¸€è§†å›¾æ¨¡å¼ ===")
        success = maintain_unified_view(eng)
        if success:
            print("âœ… ç»Ÿä¸€è§†å›¾ç»´æŠ¤å®Œæˆ")
        else:
            print("âŒ ç»Ÿä¸€è§†å›¾ç»´æŠ¤å¤±è´¥")
        return
    
    try:
        # æ­¥éª¤1: æŒ‰å­æ•°æ®é›†åˆ†ç»„åœºæ™¯
        print("\n=== æ­¥éª¤1: åˆ†ç»„åœºæ™¯æ•°æ® ===")
        scene_groups = group_scenes_by_subdataset(input_path)
        
        if not scene_groups:
            print("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åœºæ™¯åˆ†ç»„æ•°æ®")
            return
        
        # æ­¥éª¤2: æ‰¹é‡åˆ›å»ºåˆ†è¡¨
        print("\n=== æ­¥éª¤2: åˆ›å»ºåˆ†è¡¨ ===")
        table_mapping = batch_create_tables_for_subdatasets(eng, scene_groups)
        
        # æ­¥éª¤3: åˆ†åˆ«å¤„ç†æ¯ä¸ªå­æ•°æ®é›†
        print("\n=== æ­¥éª¤3: åˆ†è¡¨æ•°æ®å¤„ç† ===")
        total_processed = 0
        total_inserted = 0
        
        for i, (subdataset_name, subdataset_info) in enumerate(scene_groups.items(), 1):
            scene_ids = subdataset_info['scene_ids']
            metadata = subdataset_info.get('metadata', {})
            if interrupted:
                print(f"\nç¨‹åºè¢«ä¸­æ–­ï¼Œå·²å¤„ç† {i-1}/{len(scene_groups)} ä¸ªå­æ•°æ®é›†")
                break
                
            table_name = table_mapping[subdataset_name]
            print(f"\n[{i}/{len(scene_groups)}] å¤„ç†å­æ•°æ®é›†: {subdataset_name}")
            print(f"  - ç›®æ ‡è¡¨: {table_name}")
            print(f"  - åœºæ™¯æ•°: {len(scene_ids)}")
            
            # ä¸ºæ¯ä¸ªå­æ•°æ®é›†åˆ›å»ºç‹¬ç«‹çš„è¿›åº¦è·Ÿè¸ªå™¨
            sub_work_dir = f"{work_dir}/{subdataset_name}"
            sub_tracker = LightweightProgressTracker(sub_work_dir)
            
            try:
                # è·å–éœ€è¦å¤„ç†çš„åœºæ™¯ID
                remaining_scene_ids = sub_tracker.get_remaining_tokens(scene_ids)
                
                if not remaining_scene_ids:
                    print(f"  - å­æ•°æ®é›† {subdataset_name} æ‰€æœ‰åœºæ™¯å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡")
                    continue
                
                print(f"  - éœ€è¦å¤„ç†: {len(remaining_scene_ids)} ä¸ªåœºæ™¯")
                
                # å¤„ç†å½“å‰å­æ•°æ®é›†çš„æ•°æ®
                sub_processed, sub_inserted = process_subdataset_scenes(
                    eng, remaining_scene_ids, table_name, batch, insert_batch, sub_tracker, metadata
                )
                
                total_processed += sub_processed
                total_inserted += sub_inserted
                
                print(f"  - å®Œæˆ: å¤„ç† {sub_processed} ä¸ªï¼Œæ’å…¥ {sub_inserted} æ¡è®°å½•")
                
            except Exception as e:
                print(f"  - å¤„ç†å­æ•°æ®é›† {subdataset_name} å¤±è´¥: {str(e)}")
                continue
        
        # æ­¥éª¤4: åˆ›å»ºç»Ÿä¸€è§†å›¾ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if create_unified_view_flag and not interrupted:
            print("\n=== æ­¥éª¤4: åˆ›å»ºç»Ÿä¸€è§†å›¾ ===")
            success = create_unified_view(eng)
            if success:
                print("âœ… ç»Ÿä¸€è§†å›¾åˆ›å»ºå®Œæˆ")
            else:
                print("âŒ ç»Ÿä¸€è§†å›¾åˆ›å»ºå¤±è´¥")
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print(f"\n=== åˆ†è¡¨å¤„ç†å®Œæˆ ===")
        print(f"æ€»è®¡å¤„ç†: {total_processed} æ¡è®°å½•")
        print(f"æ€»è®¡æ’å…¥: {total_inserted} æ¡è®°å½•")
        print(f"å¤„ç†å­æ•°æ®é›†: {len(scene_groups)} ä¸ª")
        
        if interrupted:
            print("âš ï¸  å¤„ç†è¢«ä¸­æ–­ï¼Œéƒ¨åˆ†æ•°æ®å¯èƒ½æœªå®Œæˆ")
        else:
            print("âœ… åˆ†è¡¨å¤„ç†å…¨éƒ¨æˆåŠŸå®Œæˆ")
            
    except KeyboardInterrupt:
        print(f"\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        interrupted = True
    except Exception as e:
        print(f"\nåˆ†è¡¨å¤„ç†é‡åˆ°é”™è¯¯: {str(e)}")
    finally:
        print(f"\næ—¥å¿—å’Œè¿›åº¦æ–‡ä»¶ä¿å­˜åœ¨: {work_dir}")

def process_subdataset_scenes(eng, scene_ids, table_name, batch_size, insert_batch_size, tracker, metadata=None):
    """å¤„ç†å•ä¸ªå­æ•°æ®é›†çš„åœºæ™¯æ•°æ®
    
    Args:
        eng: æ•°æ®åº“å¼•æ“
        scene_ids: åœºæ™¯IDåˆ—è¡¨
        table_name: ç›®æ ‡è¡¨å
        batch_size: å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch_size: æ’å…¥æ‰¹æ¬¡å¤§å°
        tracker: è¿›åº¦è·Ÿè¸ªå™¨
        metadata: å­æ•°æ®é›†å…ƒæ•°æ®ï¼Œç”¨äºæ·»åŠ é¢å¤–å­—æ®µ
        
    Returns:
        (processed_count, inserted_count) å…ƒç»„
    """
    processed_count = 0
    inserted_count = 0
    
    try:
        for batch_num, token_batch in enumerate(chunk(scene_ids, batch_size), 1):
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                print(f"    æ‰¹æ¬¡å¤„ç†è¢«ä¸­æ–­ï¼Œå·²å¤„ç† {batch_num-1} ä¸ªæ‰¹æ¬¡")
                break
            
            print(f"    [æ‰¹æ¬¡ {batch_num}] å¤„ç† {len(token_batch)} ä¸ªåœºæ™¯")
            
            # è¿‡æ»¤å·²å¤„ç†çš„è®°å½•
            existing_in_progress = tracker.check_tokens_exist(token_batch)
            token_batch = [token for token in token_batch if token not in existing_in_progress]
            
            if not token_batch:
                print(f"    [æ‰¹æ¬¡ {batch_num}] æ‰€æœ‰æ•°æ®å·²å¤„ç†ï¼Œè·³è¿‡")
                continue
            
            if existing_in_progress:
                print(f"    [æ‰¹æ¬¡ {batch_num}] è·³è¿‡ {len(existing_in_progress)} ä¸ªå·²å¤„ç†çš„è®°å½•")
            
            # è·å–å…ƒæ•°æ®
            try:
                meta = fetch_meta(token_batch)
                if meta.empty:
                    print(f"    [æ‰¹æ¬¡ {batch_num}] æ²¡æœ‰æ‰¾åˆ°å…ƒæ•°æ®ï¼Œè·³è¿‡")
                    for token in token_batch:
                        tracker.save_failed_record(token, "æ— æ³•è·å–å…ƒæ•°æ®", batch_num, "fetch_meta")
                    continue
                
                print(f"    [æ‰¹æ¬¡ {batch_num}] è·å–åˆ° {len(meta)} æ¡å…ƒæ•°æ®")
                
            except Exception as e:
                print(f"    [æ‰¹æ¬¡ {batch_num}] è·å–å…ƒæ•°æ®å¤±è´¥: {str(e)}")
                for token in token_batch:
                    tracker.save_failed_record(token, f"è·å–å…ƒæ•°æ®å¼‚å¸¸: {str(e)}", batch_num, "fetch_meta")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                break
            
            # è·å–è¾¹ç•Œæ¡†å’Œå‡ ä½•å¯¹è±¡
            try:
                bbox_gdf = fetch_bbox_with_geometry(
                    meta.data_name.tolist(), eng, point_table=POINT_TABLE
                )
                if bbox_gdf.empty:
                    print(f"    [æ‰¹æ¬¡ {batch_num}] æ²¡æœ‰æ‰¾åˆ°è¾¹ç•Œæ¡†æ•°æ®ï¼Œè·³è¿‡")
                    for token in meta.scene_token:
                        tracker.save_failed_record(token, "æ— æ³•è·å–è¾¹ç•Œæ¡†æ•°æ®", batch_num, "fetch_bbox")
                    continue
                
                print(f"    [æ‰¹æ¬¡ {batch_num}] è·å–åˆ° {len(bbox_gdf)} æ¡è¾¹ç•Œæ¡†æ•°æ®")
                
            except Exception as e:
                print(f"    [æ‰¹æ¬¡ {batch_num}] è·å–è¾¹ç•Œæ¡†å¤±è´¥: {str(e)}")
                for token in meta.scene_token:
                    tracker.save_failed_record(token, f"è·å–è¾¹ç•Œæ¡†å¼‚å¸¸: {str(e)}", batch_num, "fetch_bbox")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                break
            
                        # åˆå¹¶æ•°æ®
            try:
                merged = meta.merge(bbox_gdf, left_on='data_name', right_on='dataset_name', how='inner')
                if merged.empty:
                    print(f"    [æ‰¹æ¬¡ {batch_num}] åˆå¹¶åæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                    for token in meta.scene_token:
                        tracker.save_failed_record(token, "å…ƒæ•°æ®ä¸è¾¹ç•Œæ¡†æ•°æ®æ— æ³•åŒ¹é…", batch_num, "data_merge")
                    continue
                    
                print(f"    [æ‰¹æ¬¡ {batch_num}] åˆå¹¶åå¾—åˆ° {len(merged)} æ¡è®°å½•")
                
                # åˆ›å»ºåŸºç¡€å­—æ®µçš„æ•°æ®
                base_columns = ['scene_token', 'data_name', 'event_id', 'city_id', 'timestamp', 'all_good']
                final_data = merged[base_columns].copy()
                
                # æ·»åŠ é¢å¤–å­—æ®µï¼ˆå¦‚æœæœ‰metadataï¼‰
                if metadata:
                    data_type = metadata.get('data_type', 'standard')
                    final_data['data_type'] = data_type
                    
                    if data_type == 'defect':
                        # è·å–scene_attributes
                        scene_attributes = metadata.get('scene_attributes', {})
                        
                        # ä¸ºæ¯ä¸ªåœºæ™¯æ·»åŠ ç‰¹å®šå±æ€§
                        for idx, scene_token in enumerate(final_data['scene_token']):
                            scene_attrs = scene_attributes.get(scene_token, {})
                            
                            # æ·»åŠ åŸºç¡€é—®é¢˜å•å­—æ®µï¼ˆå¸¦ç±»å‹è½¬æ¢ï¼‰
                            final_data.loc[idx, 'original_url'] = str(scene_attrs.get('original_url', ''))
                            
                            # æ·»åŠ å…¶ä»–è‡ªå®šä¹‰å­—æ®µï¼ˆå¸¦ç±»å‹è½¬æ¢ï¼‰
                            system_fields = {'data_type', 'original_url', 'data_name'}
                            for key, value in scene_attrs.items():
                                if key not in system_fields and not key.startswith('data_'):
                                    # æ ¹æ®å­—æ®µåæ¨æ–­é¢„æœŸç±»å‹å¹¶è½¬æ¢
                                    converted_value = convert_value_to_expected_type(key, value)
                                    final_data.loc[idx, key] = converted_value
                                    
                        print(f"    [æ‰¹æ¬¡ {batch_num}] æ·»åŠ äº†é—®é¢˜å•ç‰¹å®šå­—æ®µï¼ŒåŒ…å« {len(scene_attributes)} ä¸ªåœºæ™¯çš„å±æ€§")
                else:
                    # å‘åå…¼å®¹ï¼šæ·»åŠ é»˜è®¤data_type
                    final_data['data_type'] = 'standard'
                
                # åˆ›å»ºæœ€ç»ˆçš„GeoDataFrame
                final_gdf = gpd.GeoDataFrame(
                    final_data, 
                    geometry=merged['geometry'], 
                    crs=4326
                )
                
            except Exception as e:
                print(f"    [æ‰¹æ¬¡ {batch_num}] æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
                for token in meta.scene_token:
                    tracker.save_failed_record(token, f"æ•°æ®åˆå¹¶å¼‚å¸¸: {str(e)}", batch_num, "data_merge")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                break
            
            # æ‰¹é‡æ’å…¥åˆ°æŒ‡å®šè¡¨
            try:
                batch_inserted = batch_insert_to_postgis(
                    final_gdf, eng, 
                    table_name=table_name,  # ä½¿ç”¨æŒ‡å®šçš„åˆ†è¡¨åç§°
                    batch_size=insert_batch_size, 
                    tracker=tracker, 
                    batch_num=batch_num
                )
                inserted_count += batch_inserted
                processed_count += len(final_gdf)
                
                print(f"    [æ‰¹æ¬¡ {batch_num}] å®Œæˆï¼Œæ’å…¥ {batch_inserted} æ¡è®°å½•åˆ° {table_name}")
                
            except Exception as e:
                print(f"    [æ‰¹æ¬¡ {batch_num}] æ’å…¥æ•°æ®åº“å¤±è´¥: {str(e)}")
                for token in final_gdf.scene_token:
                    tracker.save_failed_record(token, f"æ‰¹é‡æ’å…¥å¼‚å¸¸: {str(e)}", batch_num, "batch_insert")
                continue
        
        return processed_count, inserted_count
        
    except Exception as e:
        print(f"    å¤„ç†å­æ•°æ®é›†åœºæ™¯å¤±è´¥: {str(e)}")
        return processed_count, inserted_count
    finally:
        # ä¿å­˜è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯
        tracker.finalize()

def run(input_path, batch=1000, insert_batch=1000, create_table=False, retry_failed=False, work_dir="./bbox_import_logs", show_stats=False):
    """ä¸»è¿è¡Œå‡½æ•°
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        batch: å¤„ç†æ‰¹æ¬¡å¤§å°
        insert_batch: æ’å…¥æ‰¹æ¬¡å¤§å°
        create_table: æ˜¯å¦åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        retry_failed: æ˜¯å¦åªé‡è¯•å¤±è´¥çš„æ•°æ®
        work_dir: å·¥ä½œç›®å½•ï¼Œç”¨äºå­˜å‚¨æ—¥å¿—å’Œè¿›åº¦æ–‡ä»¶
        show_stats: æ˜¯å¦æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯å¹¶é€€å‡º
    """
    global interrupted
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    setup_signal_handlers()
    
    # æ£€æŸ¥Parquetæ”¯æŒ
    if not PARQUET_AVAILABLE:
        print("è­¦å‘Š: æœªå®‰è£…pyarrowï¼Œå°†ä½¿ç”¨é™çº§çš„æ–‡æœ¬æ–‡ä»¶æ¨¡å¼")
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªå™¨
    tracker = LightweightProgressTracker(work_dir)
    
    # å¦‚æœåªæ˜¯æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    if show_stats:
        stats = tracker.get_statistics()
        print("\n=== å¤„ç†ç»Ÿè®¡ä¿¡æ¯ ===")
        print(f"æˆåŠŸå¤„ç†: {stats['success_count']} ä¸ªåœºæ™¯")
        print(f"å¤±è´¥åœºæ™¯: {stats['failed_count']} ä¸ª")
        
        if stats['failed_by_step']:
            print("\næŒ‰æ­¥éª¤åˆ†ç±»çš„å¤±è´¥ç»Ÿè®¡:")
            for step, count in stats['failed_by_step'].items():
                print(f"  {step}: {count} ä¸ª")
        
        return
    
    print(f"å¼€å§‹å¤„ç†è¾“å…¥æ–‡ä»¶: {input_path}")
    print(f"å·¥ä½œç›®å½•: {work_dir}")
    
    # æ™ºèƒ½åŠ è½½scene_idåˆ—è¡¨
    try:
        if retry_failed:
            scene_ids = tracker.load_failed_tokens()
            print(f"é‡è¯•æ¨¡å¼ï¼šåŠ è½½äº† {len(scene_ids)} ä¸ªå¤±è´¥çš„scene_id")
        else:
            all_scene_ids = load_scene_ids(input_path)
            scene_ids = tracker.get_remaining_tokens(all_scene_ids)
    except Exception as e:
        print(f"åŠ è½½è¾“å…¥æ–‡ä»¶å¤±è´¥: {str(e)}")
        return
    
    if not scene_ids:
        print("æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„scene_id")
        return
    
    eng = create_engine(LOCAL_DSN, future=True)
    
    # åˆ›å»ºè¡¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if create_table:
        if not create_table_if_not_exists(eng):
            print("åˆ›å»ºè¡¨å¤±è´¥ï¼Œé€€å‡º")
            return
    
    total_processed = 0
    total_inserted = 0
    
    print(f"å¼€å§‹å¤„ç† {len(scene_ids)} ä¸ªåœºæ™¯ï¼Œæ‰¹æ¬¡å¤§å°: {batch}")
    print("ä½¿ç”¨åŸå§‹è¾¹ç•Œæ¡†æ•°æ®ï¼Œä¸è¿›è¡Œç¼“å†²åŒºæ‰©å±•")
    
    try:
        for batch_num, token_batch in enumerate(chunk(scene_ids, batch), 1):
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                print(f"\nç¨‹åºè¢«ä¸­æ–­ï¼Œå·²å¤„ç† {batch_num-1} ä¸ªæ‰¹æ¬¡")
                break
                
            print(f"[æ‰¹æ¬¡ {batch_num}] å¤„ç† {len(token_batch)} ä¸ªåœºæ™¯")
            
            # åªæ£€æŸ¥è¿›åº¦è·Ÿè¸ªå™¨ä¸­çš„è®°å½•ï¼ˆå†…å­˜ä¸­çš„æˆåŠŸç¼“å­˜ï¼‰
            existing_in_progress = tracker.check_tokens_exist(token_batch)
            token_batch = [token for token in token_batch if token not in existing_in_progress]
            
            if not token_batch:
                print(f"[æ‰¹æ¬¡ {batch_num}] æ‰€æœ‰æ•°æ®å·²åœ¨è¿›åº¦ä¸­æ ‡è®°ä¸ºå¤„ç†è¿‡ï¼Œè·³è¿‡")
                continue
            
            if existing_in_progress:
                print(f"[æ‰¹æ¬¡ {batch_num}] è·³è¿‡ {len(existing_in_progress)} ä¸ªå·²å¤„ç†çš„è®°å½•")
            
            # è·å–å…ƒæ•°æ®
            try:
                meta = fetch_meta(token_batch)
                if meta.empty:
                    print(f"[æ‰¹æ¬¡ {batch_num}] æ²¡æœ‰æ‰¾åˆ°å…ƒæ•°æ®ï¼Œè·³è¿‡")
                    # è®°å½•è·å–å…ƒæ•°æ®å¤±è´¥çš„tokens
                    for token in token_batch:
                        tracker.save_failed_record(token, "æ— æ³•è·å–å…ƒæ•°æ®", batch_num, "fetch_meta")
                    continue
                    
                print(f"[æ‰¹æ¬¡ {batch_num}] è·å–åˆ° {len(meta)} æ¡å…ƒæ•°æ®")
                
            except Exception as e:
                print(f"[æ‰¹æ¬¡ {batch_num}] è·å–å…ƒæ•°æ®å¤±è´¥: {str(e)}")
                for token in token_batch:
                    tracker.save_failed_record(token, f"è·å–å…ƒæ•°æ®å¼‚å¸¸: {str(e)}", batch_num, "fetch_meta")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                print(f"\nç¨‹åºè¢«ä¸­æ–­ï¼Œæ­£åœ¨å¤„ç†æ‰¹æ¬¡ {batch_num}")
                break
            
            # è·å–è¾¹ç•Œæ¡†å’Œå‡ ä½•å¯¹è±¡ï¼ˆç›´æ¥ä»PostGISè·å–åŸå§‹å‡ ä½•ï¼‰
            try:
                bbox_gdf = fetch_bbox_with_geometry(
                    meta.data_name.tolist(), eng, point_table=POINT_TABLE
                )
                if bbox_gdf.empty:
                    print(f"[æ‰¹æ¬¡ {batch_num}] æ²¡æœ‰æ‰¾åˆ°è¾¹ç•Œæ¡†æ•°æ®ï¼Œè·³è¿‡")
                    # è®°å½•è·å–è¾¹ç•Œæ¡†å¤±è´¥çš„tokens
                    for token in meta.scene_token:
                        tracker.save_failed_record(token, "æ— æ³•è·å–è¾¹ç•Œæ¡†æ•°æ®", batch_num, "fetch_bbox")
                    continue
                    
                print(f"[æ‰¹æ¬¡ {batch_num}] è·å–åˆ° {len(bbox_gdf)} æ¡è¾¹ç•Œæ¡†æ•°æ®")
                
            except Exception as e:
                print(f"[æ‰¹æ¬¡ {batch_num}] è·å–è¾¹ç•Œæ¡†å¤±è´¥: {str(e)}")
                for token in meta.scene_token:
                    tracker.save_failed_record(token, f"è·å–è¾¹ç•Œæ¡†å¼‚å¸¸: {str(e)}", batch_num, "fetch_bbox")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                print(f"\nç¨‹åºè¢«ä¸­æ–­ï¼Œæ­£åœ¨å¤„ç†æ‰¹æ¬¡ {batch_num}")
                break
            
            # åˆå¹¶æ•°æ®
            try:
                merged = meta.merge(bbox_gdf, left_on='data_name', right_on='dataset_name', how='inner')
                if merged.empty:
                    print(f"[æ‰¹æ¬¡ {batch_num}] åˆå¹¶åæ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                    # è®°å½•åˆå¹¶å¤±è´¥çš„tokens
                    for token in meta.scene_token:
                        tracker.save_failed_record(token, "å…ƒæ•°æ®ä¸è¾¹ç•Œæ¡†æ•°æ®æ— æ³•åŒ¹é…", batch_num, "data_merge")
                    continue
                    
                print(f"[æ‰¹æ¬¡ {batch_num}] åˆå¹¶åå¾—åˆ° {len(merged)} æ¡è®°å½•")
                
                # åˆ›å»ºæœ€ç»ˆçš„GeoDataFrame
                final_gdf = gpd.GeoDataFrame(
                    merged[['scene_token', 'data_name', 'event_id', 'city_id', 'timestamp', 'all_good']], 
                    geometry=merged['geometry'], 
                    crs=4326
                )
                
            except Exception as e:
                print(f"[æ‰¹æ¬¡ {batch_num}] æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}")
                for token in meta.scene_token:
                    tracker.save_failed_record(token, f"æ•°æ®åˆå¹¶å¼‚å¸¸: {str(e)}", batch_num, "data_merge")
                continue
            
            # æ£€æŸ¥ä¸­æ–­ä¿¡å·
            if interrupted:
                print(f"\nç¨‹åºè¢«ä¸­æ–­ï¼Œæ­£åœ¨å¤„ç†æ‰¹æ¬¡ {batch_num}")
                break
            
            # æ‰¹é‡æ’å…¥æ•°æ®åº“
            try:
                batch_inserted = batch_insert_to_postgis(
                    final_gdf, eng, 
                    batch_size=insert_batch, 
                    tracker=tracker, 
                    batch_num=batch_num
                )
                total_inserted += batch_inserted
                total_processed += len(final_gdf)
                
                print(f"[æ‰¹æ¬¡ {batch_num}] å®Œæˆï¼Œæ’å…¥ {batch_inserted} æ¡è®°å½•")
                print(f"[ç´¯è®¡è¿›åº¦] å·²å¤„ç†: {total_processed}, å·²æ’å…¥: {total_inserted}")
                
                # æ¯10ä¸ªæ‰¹æ¬¡ä¿å­˜ä¸€æ¬¡è¿›åº¦
                if batch_num % 10 == 0:
                    tracker.save_progress(len(scene_ids), total_processed, total_inserted, batch_num)
                
            except Exception as e:
                print(f"[æ‰¹æ¬¡ {batch_num}] æ’å…¥æ•°æ®åº“å¤±è´¥: {str(e)}")
                for token in final_gdf.scene_token:
                    tracker.save_failed_record(token, f"æ‰¹é‡æ’å…¥å¼‚å¸¸: {str(e)}", batch_num, "batch_insert")
                continue
                
    except KeyboardInterrupt:
        print(f"\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        interrupted = True
    except Exception as e:
        print(f"\nç¨‹åºé‡åˆ°æœªé¢„æœŸçš„é”™è¯¯: {str(e)}")
    finally:
        # æœ€ç»ˆä¿å­˜è¿›åº¦å’Œåˆ·æ–°ç¼“å†²åŒº
        tracker.finalize()
        tracker.save_progress(len(scene_ids), total_processed, total_inserted, batch_num if 'batch_num' in locals() else 0)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        stats = tracker.get_statistics()
        
        if interrupted:
            print(f"ç¨‹åºä¼˜é›…é€€å‡ºï¼å·²å¤„ç†: {total_processed} æ¡è®°å½•ï¼ŒæˆåŠŸæ’å…¥: {total_inserted} æ¡è®°å½•")
        else:
            print(f"å¤„ç†å®Œæˆï¼æ€»è®¡å¤„ç†: {total_processed} æ¡è®°å½•ï¼ŒæˆåŠŸæ’å…¥: {total_inserted} æ¡è®°å½•")
        
        print(f"\n=== æœ€ç»ˆç»Ÿè®¡ ===")
        print(f"æˆåŠŸå¤„ç†: {stats['success_count']} ä¸ªåœºæ™¯")
        print(f"å¤±è´¥åœºæ™¯: {stats['failed_count']} ä¸ª")
        
        print(f"\nçŠ¶æ€æ–‡ä»¶ä½ç½®:")
        print(f"- æˆåŠŸè®°å½•: {tracker.success_file}")
        print(f"- å¤±è´¥è®°å½•: {tracker.failed_file}")
        print(f"- è¿›åº¦æ–‡ä»¶: {tracker.progress_file}")

def build_parser() -> argparse.ArgumentParser:
    """å…¼å®¹æ—§è„šæœ¬çš„å‘½ä»¤è¡Œè§£æå™¨ã€‚"""

    parser = argparse.ArgumentParser(description='ä»æ•°æ®é›†æ–‡ä»¶ç”Ÿæˆè¾¹ç•Œæ¡†æ•°æ®')
    parser.add_argument('--input', required=True, help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒJSON/Parquet/æ–‡æœ¬æ ¼å¼ï¼‰')
    parser.add_argument('--batch', type=int, default=1000, help='å¤„ç†æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--insert-batch', type=int, default=1000, help='æ’å…¥æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--create-table', action='store_true', help='åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ã€‚é»˜è®¤å‡è®¾è¡¨å·²é€šè¿‡SQLè„šæœ¬åˆ›å»º')
    parser.add_argument('--retry-failed', action='store_true', help='æ˜¯å¦åªé‡è¯•å¤±è´¥çš„æ•°æ®')
    parser.add_argument('--work-dir', default='./bbox_import_logs', help='å·¥ä½œç›®å½•ï¼Œç”¨äºå­˜å‚¨æ—¥å¿—å’Œè¿›åº¦æ–‡ä»¶')
    parser.add_argument('--show-stats', action='store_true', help='æ˜¾ç¤ºå¤„ç†ç»Ÿè®¡ä¿¡æ¯å¹¶é€€å‡º')
    return parser


def main(argv: list[str] | None = None) -> int:
    """æš´éœ²ä¸€ä¸ªå…¥å£ä»¥ä¾›æ–°çš„ CLI ä»£ç†è°ƒç”¨ã€‚"""

    parser = build_parser()
    args = parser.parse_args(argv)
    run(
        args.input,
        args.batch,
        args.insert_batch,
        args.create_table,
        args.retry_failed,
        args.work_dir,
        args.show_stats,
    )
    return 0


if __name__ == '__main__':
    raise SystemExit(main())
