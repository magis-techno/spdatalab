"""Gridè½¨è¿¹èšç±»æ¨¡å—

åŸºäºcity_hotspotsè¡¨çš„çƒ­ç‚¹gridï¼Œå¯¹æ¯ä¸ª200mÃ—200måŒºåŸŸå†…çš„é«˜è´¨é‡è½¨è¿¹è¿›è¡Œèšç±»åˆ†æã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»city_hotspotsè¡¨åŠ è½½çƒ­ç‚¹grid
2. æŸ¥è¯¢gridå†…çš„é«˜è´¨é‡è½¨è¿¹ç‚¹ï¼ˆworkstage=2ï¼‰
3. æŒ‰è·ç¦»ä¼˜å…ˆ+æ—¶é•¿ä¸Šé™ç­–ç•¥åˆ‡åˆ†è½¨è¿¹æ®µ
4. æå–10ç»´ç‰¹å¾å‘é‡ï¼ˆé€Ÿåº¦ã€åŠ é€Ÿåº¦ã€èˆªå‘ã€å½¢æ€ï¼‰
5. DBSCANèšç±»åˆ†æ
6. ä¿å­˜ç»“æœåˆ°æ•°æ®åº“

ä½¿ç”¨ç¤ºä¾‹ï¼š
    clusterer = GridTrajectoryClusterer()
    results = clusterer.process_all_grids(city_id='A72', max_grids=5)
"""

from __future__ import annotations
import logging
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import warnings

import numpy as np
import pandas as pd
import geopandas as gpd
from shapely.geometry import LineString, Point
from shapely import wkt
from sqlalchemy import create_engine, text
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# å¯¼å…¥é«˜æ€§èƒ½polygonæŸ¥è¯¢å™¨
from spdatalab.dataset.polygon_trajectory_query import (
    HighPerformancePolygonTrajectoryQuery,
    PolygonTrajectoryConfig
)

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# æ•°æ®åº“é…ç½®
LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"
POINT_TABLE = "public.ddi_data_points"

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class ClusterConfig:
    """èšç±»é…ç½®å‚æ•°"""
    # æ•°æ®åº“é…ç½®
    local_dsn: str = LOCAL_DSN
    point_table: str = POINT_TABLE
    
    # è½¨è¿¹æŸ¥è¯¢é…ç½®
    query_limit: int = 50000          # æ¯ä¸ªgridçš„è½¨è¿¹ç‚¹æŸ¥è¯¢é™åˆ¶
    
    # è½¨è¿¹åˆ‡åˆ†é…ç½®ï¼ˆè·ç¦»ä¼˜å…ˆ+æ—¶é•¿ä¸Šé™ï¼‰
    min_distance: float = 50.0        # ä¸»åˆ‡åˆ†ï¼š50ç±³/æ®µ
    max_duration: float = 15.0        # å¼ºåˆ¶åˆ‡åˆ†ï¼š15ç§’ä¸Šé™
    min_points: int = 5               # æœ€å°‘ç‚¹æ•°
    time_gap_threshold: float = 3.0   # æ—¶é—´é—´éš”>3ç§’æ–­å¼€
    
    # è´¨é‡è¿‡æ»¤é…ç½®
    min_movement: float = 10.0        # æœ€å°ç§»åŠ¨è·ç¦»ï¼ˆç±³ï¼‰
    max_jump: float = 100.0           # æœ€å¤§ç‚¹é—´è·ï¼ˆç±³ï¼‰
    max_speed: float = 30.0           # æœ€å¤§åˆç†é€Ÿåº¦ï¼ˆm/sï¼‰
    
    # èšç±»é…ç½®
    eps: float = 0.4                  # DBSCANè·ç¦»é˜ˆå€¼
    min_samples: int = 3              # DBSCANæœ€å°æ ·æœ¬æ•°
    
    # æ€§èƒ½é…ç½®
    batch_size: int = 100             # æ‰¹é‡ä¿å­˜å¤§å°


@dataclass
class TrajectorySegment:
    """è½¨è¿¹æ®µæ•°æ®ç»“æ„"""
    dataset_name: str
    segment_index: int
    points: pd.DataFrame              # åŒ…å«lon, lat, timestamp, twist_linear, yawç­‰
    
    # åŸºæœ¬ç»Ÿè®¡
    start_time: int
    end_time: int
    duration: float
    point_count: int
    
    # ç‰¹å¾ï¼ˆç¨åè®¡ç®—ï¼‰
    features: Optional[np.ndarray] = None
    quality_flag: str = 'unknown'
    
    # å‡ ä½•
    geometry: Optional[LineString] = None


class GridTrajectoryClusterer:
    """Gridè½¨è¿¹èšç±»å™¨
    
    æ ¸å¿ƒæµç¨‹ï¼š
    1. åŠ è½½çƒ­ç‚¹grids
    2. æŸ¥è¯¢è½¨è¿¹ç‚¹
    3. åˆ‡åˆ†è½¨è¿¹æ®µ
    4. ç‰¹å¾æå–
    5. DBSCANèšç±»
    6. ä¿å­˜ç»“æœ
    """
    
    def __init__(self, config: Optional[ClusterConfig] = None):
        """åˆå§‹åŒ–èšç±»å™¨
        
        Args:
            config: èšç±»é…ç½®å‚æ•°
        """
        self.config = config or ClusterConfig()
        self.engine = create_engine(
            self.config.local_dsn,
            future=True,
            pool_pre_ping=True
        )
        self.scaler = StandardScaler()
        
        # åˆå§‹åŒ–é«˜æ€§èƒ½æŸ¥è¯¢å™¨
        query_config = PolygonTrajectoryConfig(
            limit_per_polygon=self.config.query_limit,
            fetch_complete_trajectories=False,  # åªæŸ¥è¯¢ç›¸äº¤çš„ç‚¹
            batch_threshold=1,  # å•ä¸ªgridä½¿ç”¨æ‰¹é‡ç­–ç•¥
            enable_speed_stats=False,
            enable_avp_stats=False
        )
        self.trajectory_query = HighPerformancePolygonTrajectoryQuery(query_config)
        
        logger.info("ğŸš€ GridTrajectoryClusterer åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   æŸ¥è¯¢é™åˆ¶: {self.config.query_limit}ç‚¹/grid")
        logger.info(f"   åˆ‡åˆ†ç­–ç•¥: {self.config.min_distance}ç±³/{self.config.max_duration}ç§’")
        logger.info(f"   èšç±»å‚æ•°: eps={self.config.eps}, min_samples={self.config.min_samples}")
    
    def load_hotspot_grids(
        self, 
        city_id: Optional[str] = None, 
        limit: Optional[int] = None,
        grid_ids: Optional[List[int]] = None
    ) -> pd.DataFrame:
        """ä»city_hotspotsè¡¨åŠ è½½çƒ­ç‚¹grid
        
        Args:
            city_id: åŸå¸‚IDè¿‡æ»¤
            limit: é™åˆ¶æ•°é‡
            grid_ids: æŒ‡å®šgrid IDåˆ—è¡¨
            
        Returns:
            åŒ…å«gridä¿¡æ¯çš„DataFrame
        """
        logger.info("ğŸ“Š åŠ è½½çƒ­ç‚¹grids...")
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        where_conditions = []
        params = {}
        
        if city_id:
            where_conditions.append("city_id = :city_id")
            params['city_id'] = city_id
        
        if grid_ids:
            where_conditions.append("id = ANY(:grid_ids)")
            params['grid_ids'] = grid_ids
        
        where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
        limit_clause = f"LIMIT {limit}" if limit else ""
        
        sql = text(f"""
            SELECT 
                id as grid_id,
                city_id,
                analysis_id,
                bbox_count,
                subdataset_count,
                scene_count,
                grid_coords,
                ST_AsText(geometry) as geometry_wkt
            FROM city_hotspots
            WHERE {where_clause}
            ORDER BY bbox_count DESC
            {limit_clause};
        """)
        
        with self.engine.connect() as conn:
            grids_df = pd.read_sql(sql, conn, params=params)
        
        if grids_df.empty:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„grid")
            return grids_df
        
        # è½¬æ¢å‡ ä½•
        grids_df['geometry'] = grids_df['geometry_wkt'].apply(wkt.loads)
        
        logger.info(f"âœ… åŠ è½½äº† {len(grids_df)} ä¸ªçƒ­ç‚¹grid")
        if city_id:
            logger.info(f"   åŸå¸‚: {city_id}")
        logger.info(f"   bboxæ•°é‡èŒƒå›´: {grids_df['bbox_count'].min()}-{grids_df['bbox_count'].max()}")
        
        return grids_df
    
    def query_trajectory_points(self, grid_geometry, grid_id: int = 0) -> pd.DataFrame:
        """æŸ¥è¯¢gridå†…çš„é«˜è´¨é‡è½¨è¿¹ç‚¹ï¼ˆä½¿ç”¨é«˜æ€§èƒ½æŸ¥è¯¢å™¨ï¼‰
        
        Args:
            grid_geometry: Gridçš„å‡ ä½•å¯¹è±¡ï¼ˆPolygonï¼‰
            grid_id: Grid IDï¼ˆç”¨äºæ ‡è¯†ï¼‰
            
        Returns:
            è½¨è¿¹ç‚¹DataFrame
        """
        logger.debug("ğŸ” æŸ¥è¯¢gridå†…çš„è½¨è¿¹ç‚¹ï¼ˆä½¿ç”¨é«˜æ€§èƒ½æŸ¥è¯¢å™¨ï¼‰...")
        
        # å°†gridåŒ…è£…æˆpolygonæ ¼å¼
        polygon_data = [{
            'id': f'grid_{grid_id}',
            'geometry': grid_geometry,
            'properties': {'grid_id': grid_id}
        }]
        
        try:
            # ä½¿ç”¨é«˜æ€§èƒ½æŸ¥è¯¢å™¨ï¼ˆå¤ç”¨æ‰€æœ‰ä¼˜åŒ–ç­–ç•¥ï¼‰
            points_df, stats = self.trajectory_query.query_intersecting_trajectory_points(polygon_data)
            
            if not points_df.empty:
                # é‡å‘½ååˆ—ä»¥åŒ¹é…åç»­å¤„ç†
                points_df = points_df.rename(columns={
                    'longitude': 'lon',
                    'latitude': 'lat',
                    'twist_linear': 'twist_linear'  # ä¿æŒåŸå
                })
                
                # æ·»åŠ vehicle_idåˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                if 'vehicle_id' not in points_df.columns:
                    points_df['vehicle_id'] = None
                
                logger.debug(f"   æŸ¥è¯¢åˆ° {len(points_df)} ä¸ªé«˜è´¨é‡è½¨è¿¹ç‚¹")
                logger.debug(f"   æ¶‰åŠè½¨è¿¹æ•°: {points_df['dataset_name'].nunique()}")
                logger.debug(f"   æŸ¥è¯¢ç”¨æ—¶: {stats['query_time']:.2f}s")
            else:
                logger.debug("   æœªæ‰¾åˆ°è½¨è¿¹ç‚¹")
            
            return points_df
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢è½¨è¿¹ç‚¹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()
    
    def segment_trajectories(
        self, 
        points_df: pd.DataFrame
    ) -> List[TrajectorySegment]:
        """æŒ‰è·ç¦»ä¼˜å…ˆ+æ—¶é•¿ä¸Šé™ç­–ç•¥åˆ‡åˆ†è½¨è¿¹æ®µ
        
        Args:
            points_df: è½¨è¿¹ç‚¹DataFrame
            
        Returns:
            è½¨è¿¹æ®µåˆ—è¡¨
        """
        if points_df.empty:
            return []
        
        logger.debug("âœ‚ï¸ åˆ‡åˆ†è½¨è¿¹æ®µ...")
        
        all_segments = []
        
        # æŒ‰dataset_nameåˆ†ç»„
        for dataset_name, group in points_df.groupby('dataset_name'):
            # æŒ‰timestampæ’åº
            group = group.sort_values('timestamp').reset_index(drop=True)
            
            if len(group) < self.config.min_points:
                continue
            
            # åˆ‡åˆ†è¯¥è½¨è¿¹
            segments = self._segment_single_trajectory(dataset_name, group)
            all_segments.extend(segments)
        
        logger.debug(f"   åˆ‡åˆ†å¾—åˆ° {len(all_segments)} ä¸ªè½¨è¿¹æ®µ")
        
        return all_segments
    
    def _segment_single_trajectory(
        self, 
        dataset_name: str, 
        points: pd.DataFrame
    ) -> List[TrajectorySegment]:
        """åˆ‡åˆ†å•æ¡è½¨è¿¹
        
        å®ç°è·ç¦»ä¼˜å…ˆ+æ—¶é•¿ä¸Šé™ç­–ç•¥ï¼š
        1. ç´¯è®¡è·ç¦»è¾¾åˆ°min_distance â†’ åˆ‡åˆ†
        2. æ—¶é•¿è¶…è¿‡max_duration â†’ å¼ºåˆ¶åˆ‡åˆ†
        3. æ—¶é—´é—´éš”>time_gap_threshold â†’ æ–­å¼€
        """
        segments = []
        current_segment_points = []
        cumulative_distance = 0.0
        segment_index = 0
        
        for i in range(len(points)):
            current_point = points.iloc[i]
            current_segment_points.append(current_point)
            
            if i == 0:
                segment_start_time = current_point['timestamp']
                continue
            
            prev_point = points.iloc[i - 1]
            
            # è®¡ç®—ä¸ä¸Šä¸€ç‚¹çš„è·ç¦»
            dist = self._haversine_distance(
                prev_point['lat'], prev_point['lon'],
                current_point['lat'], current_point['lon']
            )
            cumulative_distance += dist
            
            # è®¡ç®—æ—¶é•¿
            duration = current_point['timestamp'] - segment_start_time
            
            # è®¡ç®—æ—¶é—´é—´éš”
            time_gap = current_point['timestamp'] - prev_point['timestamp']
            
            # åˆ¤æ–­åˆ‡åˆ†æ¡ä»¶
            should_split = False
            
            # æ¡ä»¶1ï¼šæ—¶é—´é—´éš”è¿‡å¤§ï¼ˆè½¨è¿¹æ–­å¼€ï¼‰
            if time_gap > self.config.time_gap_threshold:
                should_split = True
                logger.debug(f"   æ—¶é—´é—´éš”æ–­å¼€: {time_gap:.1f}ç§’")
            
            # æ¡ä»¶2ï¼šè·ç¦»è¾¾æ ‡ï¼ˆä¸»åˆ‡åˆ†ï¼‰
            elif cumulative_distance >= self.config.min_distance:
                should_split = True
                logger.debug(f"   è·ç¦»è¾¾æ ‡åˆ‡åˆ†: {cumulative_distance:.1f}ç±³")
            
            # æ¡ä»¶3ï¼šæ—¶é•¿è¶…é™ï¼ˆå¼ºåˆ¶åˆ‡åˆ†ï¼‰
            elif duration >= self.config.max_duration:
                should_split = True
                logger.debug(f"   æ—¶é•¿è¶…é™åˆ‡åˆ†: {duration:.1f}ç§’")
            
            if should_split:
                # ä¿å­˜å½“å‰æ®µï¼ˆä¸åŒ…æ‹¬è§¦å‘ç‚¹ï¼‰
                segment_points = current_segment_points[:-1]
                if len(segment_points) >= self.config.min_points:
                    segment = self._create_segment(
                        dataset_name, 
                        segment_index, 
                        segment_points
                    )
                    segments.append(segment)
                    segment_index += 1
                
                # å¼€å§‹æ–°æ®µ
                current_segment_points = [current_point]
                cumulative_distance = 0.0
                segment_start_time = current_point['timestamp']
        
        # å¤„ç†æœ€åä¸€æ®µ
        if len(current_segment_points) >= self.config.min_points:
            segment = self._create_segment(
                dataset_name,
                segment_index,
                current_segment_points
            )
            segments.append(segment)
        
        return segments
    
    def _create_segment(
        self, 
        dataset_name: str, 
        segment_index: int, 
        points_list: List
    ) -> TrajectorySegment:
        """åˆ›å»ºè½¨è¿¹æ®µå¯¹è±¡"""
        points_df = pd.DataFrame(points_list)
        
        start_time = int(points_df['timestamp'].min())
        end_time = int(points_df['timestamp'].max())
        duration = (end_time - start_time) / 1.0  # ç§’
        
        # åˆ›å»ºLineStringå‡ ä½•
        coords = list(zip(points_df['lon'], points_df['lat']))
        geometry = LineString(coords) if len(coords) >= 2 else None
        
        return TrajectorySegment(
            dataset_name=dataset_name,
            segment_index=segment_index,
            points=points_df,
            start_time=start_time,
            end_time=end_time,
            duration=duration,
            point_count=len(points_df),
            geometry=geometry
        )
    
    def filter_segment_quality(
        self, 
        segment: TrajectorySegment
    ) -> Tuple[bool, str]:
        """è´¨é‡è¿‡æ»¤ï¼šåŸåœ°ä¸åŠ¨ã€GPSè·³ç‚¹ã€é€Ÿåº¦å¼‚å¸¸
        
        Returns:
            (is_valid, reason)
        """
        points = segment.points
        
        # 1. æœ€å°‘ç‚¹æ•°æ£€æŸ¥
        if len(points) < self.config.min_points:
            return False, "insufficient_points"
        
        # 2. ç§»åŠ¨è·ç¦»æ£€æŸ¥ï¼ˆè¿‡æ»¤åŸåœ°ä¸åŠ¨ï¼‰
        total_distance = self._calculate_total_distance(points)
        if total_distance < self.config.min_movement:
            return False, "stationary"
        
        # 3. GPSè·³ç‚¹æ£€æŸ¥
        max_jump = self._calculate_max_consecutive_distance(points)
        if max_jump > self.config.max_jump:
            return False, "gps_jump"
        
        # 4. é€Ÿåº¦åˆç†æ€§æ£€æŸ¥
        if 'twist_linear' in points.columns:
            avg_speed = points['twist_linear'].mean()
            if avg_speed > self.config.max_speed:
                return False, "excessive_speed"
        
        return True, "valid"
    
    def extract_features(self, segment: TrajectorySegment) -> np.ndarray:
        """æå–10ç»´ç‰¹å¾å‘é‡
        
        ç‰¹å¾åŒ…æ‹¬ï¼š
        - é€Ÿåº¦ç‰¹å¾ï¼ˆ4ç»´ï¼‰ï¼šavg, std, max, min
        - åŠ é€Ÿåº¦ç‰¹å¾ï¼ˆ2ç»´ï¼‰ï¼šavg, std
        - èˆªå‘è§’ç‰¹å¾ï¼ˆ2ç»´ï¼‰ï¼šchange_rate, std
        - å½¢æ€ç‰¹å¾ï¼ˆ2ç»´ï¼‰ï¼šdirection_cos, direction_sin
        
        Returns:
            10ç»´ç‰¹å¾å‘é‡
        """
        points = segment.points
        
        # é€Ÿåº¦ç‰¹å¾
        speeds = points['twist_linear'].values
        avg_speed = np.mean(speeds)
        std_speed = np.std(speeds)
        max_speed = np.max(speeds)
        min_speed = np.min(speeds)
        
        # åŠ é€Ÿåº¦ç‰¹å¾ï¼ˆå·®åˆ†è®¡ç®—ï¼‰
        if len(speeds) > 1:
            time_diffs = np.diff(points['timestamp'].values)
            time_diffs = np.maximum(time_diffs, 0.1)  # é¿å…é™¤é›¶
            accelerations = np.diff(speeds) / time_diffs
            avg_acceleration = np.mean(accelerations)
            std_acceleration = np.std(accelerations)
        else:
            avg_acceleration = 0.0
            std_acceleration = 0.0
        
        # èˆªå‘è§’ç‰¹å¾
        if 'yaw' in points.columns and len(points) > 1:
            yaws = points['yaw'].values
            time_diffs = np.diff(points['timestamp'].values)
            time_diffs = np.maximum(time_diffs, 0.1)
            yaw_changes = np.diff(yaws) / time_diffs
            yaw_change_rate = np.mean(np.abs(yaw_changes))
            std_yaw = np.std(yaws)
        else:
            yaw_change_rate = 0.0
            std_yaw = 0.0
        
        # è½¨è¿¹å½¢æ€ç‰¹å¾
        start_point = points.iloc[0]
        end_point = points.iloc[-1]
        
        delta_lng = end_point['lon'] - start_point['lon']
        delta_lat = end_point['lat'] - start_point['lat']
        
        # èµ·ç»ˆç‚¹æ–¹å‘å‘é‡
        angle = np.arctan2(delta_lat, delta_lng)
        direction_cos = np.cos(angle)
        direction_sin = np.sin(angle)
        
        # ç»„è£…ç‰¹å¾å‘é‡ï¼ˆ10ç»´ï¼‰
        features = np.array([
            avg_speed,
            std_speed,
            max_speed,
            min_speed,
            avg_acceleration,
            std_acceleration,
            yaw_change_rate,
            std_yaw,
            direction_cos,
            direction_sin
        ])
        
        return features
    
    def perform_clustering(
        self, 
        segments: List[TrajectorySegment]
    ) -> np.ndarray:
        """DBSCANèšç±»
        
        Args:
            segments: è½¨è¿¹æ®µåˆ—è¡¨ï¼ˆå·²æå–ç‰¹å¾ï¼‰
            
        Returns:
            èšç±»æ ‡ç­¾æ•°ç»„
        """
        if not segments:
            return np.array([])
        
        logger.info("ğŸ”¬ æ‰§è¡ŒDBSCANèšç±»...")
        
        # æå–ç‰¹å¾çŸ©é˜µ
        features_list = [seg.features for seg in segments if seg.features is not None]
        
        if not features_list:
            logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆç‰¹å¾ï¼Œè·³è¿‡èšç±»")
            return np.array([-1] * len(segments))
        
        features_matrix = np.vstack(features_list)
        
        logger.info(f"   ç‰¹å¾çŸ©é˜µ: {features_matrix.shape}")
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        features_scaled = self.scaler.fit_transform(features_matrix)
        
        # DBSCANèšç±»
        dbscan = DBSCAN(
            eps=self.config.eps,
            min_samples=self.config.min_samples,
            metric='euclidean'
        )
        
        labels = dbscan.fit_predict(features_scaled)
        
        # ç»Ÿè®¡èšç±»ç»“æœ
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        
        logger.info(f"âœ… èšç±»å®Œæˆ:")
        logger.info(f"   èšç±»æ•°é‡: {n_clusters}")
        logger.info(f"   å™ªå£°ç‚¹: {n_noise} ({n_noise/len(labels)*100:.1f}%)")
        
        return labels
    
    def generate_behavior_labels(
        self, 
        segments: List[TrajectorySegment],
        labels: np.ndarray
    ) -> Dict[int, Dict]:
        """æ ¹æ®èšç±»ä¸­å¿ƒç‰¹å¾ç”Ÿæˆè¡Œä¸ºæ ‡ç­¾
        
        Returns:
            {cluster_label: {centroid, speed_range, behavior_label}}
        """
        cluster_info = {}
        
        unique_labels = set(labels)
        
        for label in unique_labels:
            # è·å–è¯¥èšç±»çš„æ‰€æœ‰æ®µ
            cluster_segments = [seg for seg, l in zip(segments, labels) if l == label]
            
            if not cluster_segments:
                continue
            
            # è®¡ç®—èšç±»ä¸­å¿ƒï¼ˆç‰¹å¾å¹³å‡å€¼ï¼‰
            features = np.vstack([seg.features for seg in cluster_segments])
            centroid = np.mean(features, axis=0)
            
            # æå–å…³é”®ç‰¹å¾
            avg_speed = centroid[0]
            avg_accel = centroid[4]
            yaw_change = centroid[6]
            direction_cos = centroid[8]
            direction_sin = centroid[9]
            
            # ç”Ÿæˆé€Ÿåº¦èŒƒå›´æ ‡ç­¾
            if avg_speed < 2:
                speed_range = "æä½é€Ÿ(0-2m/s)"
            elif avg_speed < 5:
                speed_range = "ä½é€Ÿ(2-5m/s)"
            elif avg_speed < 10:
                speed_range = "ä¸­é€Ÿ(5-10m/s)"
            elif avg_speed < 15:
                speed_range = "è¾ƒå¿«(10-15m/s)"
            else:
                speed_range = f"å¿«é€Ÿ({avg_speed:.1f}m/s)"
            
            # ç”Ÿæˆè¡Œä¸ºæ ‡ç­¾
            if label == -1:
                behavior_label = "å™ªå£°/å¼‚å¸¸"
            elif yaw_change > 0.3:
                behavior_label = "è½¬å¼¯/å˜é“"
            elif avg_accel < -1:
                behavior_label = "å‡é€Ÿ/åˆ¹è½¦"
            elif avg_accel > 1:
                behavior_label = "åŠ é€Ÿ"
            elif avg_speed < 3:
                behavior_label = "ç¼“æ…¢ç§»åŠ¨"
            else:
                behavior_label = "ç›´è¡Œé€šè¿‡"
            
            cluster_info[int(label)] = {
                'centroid': centroid,
                'speed_range': speed_range,
                'behavior_label': behavior_label,
                'segment_count': len(cluster_segments),
                'centroid_avg_speed': float(avg_speed),
                'centroid_avg_acceleration': float(avg_accel),
                'centroid_yaw_change_rate': float(yaw_change),
                'centroid_direction_cos': float(direction_cos),
                'centroid_direction_sin': float(direction_sin)
            }
        
        return cluster_info
    
    def save_results(
        self,
        grid_id: int,
        city_id: str,
        analysis_id: str,
        segments: List[TrajectorySegment],
        labels: np.ndarray,
        cluster_info: Dict
    ):
        """ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
        
        ä¿å­˜åˆ°ä¸¤ä¸ªè¡¨ï¼š
        1. grid_trajectory_segments - æ¯ä¸ªè½¨è¿¹æ®µ
        2. grid_clustering_summary - èšç±»ç»Ÿè®¡
        """
        logger.info("ğŸ’¾ ä¿å­˜ç»“æœåˆ°æ•°æ®åº“...")
        
        if not segments:
            logger.warning("âš ï¸ æ²¡æœ‰è½¨è¿¹æ®µå¯ä¿å­˜")
            return
        
        # å‡†å¤‡è½¨è¿¹æ®µæ•°æ®
        segments_data = []
        
        for segment, label in zip(segments, labels):
            # è®¡ç®—é¢å¤–çš„å½¢æ€ç‰¹å¾
            total_distance = self._calculate_total_distance(segment.points)
            straight_distance = self._haversine_distance(
                segment.points.iloc[0]['lat'], segment.points.iloc[0]['lon'],
                segment.points.iloc[-1]['lat'], segment.points.iloc[-1]['lon']
            )
            curvature = total_distance / max(straight_distance, 0.001)
            
            # æå–ç‰¹å¾å€¼
            features = segment.features
            
            segments_data.append({
                'grid_id': grid_id,
                'city_id': city_id,
                'analysis_id': analysis_id,
                'dataset_name': segment.dataset_name,
                'segment_index': segment.segment_index,
                'start_time': segment.start_time,
                'end_time': segment.end_time,
                'duration': segment.duration,
                'point_count': segment.point_count,
                'avg_speed': float(features[0]) if features is not None else None,
                'std_speed': float(features[1]) if features is not None else None,
                'max_speed': float(features[2]) if features is not None else None,
                'min_speed': float(features[3]) if features is not None else None,
                'avg_acceleration': float(features[4]) if features is not None else None,
                'std_acceleration': float(features[5]) if features is not None else None,
                'avg_yaw': float(segment.points['yaw'].mean()) if 'yaw' in segment.points.columns else None,
                'yaw_change_rate': float(features[6]) if features is not None else None,
                'std_yaw': float(features[7]) if features is not None else None,
                'trajectory_length_m': total_distance,
                'direction_cos': float(features[8]) if features is not None else None,
                'direction_sin': float(features[9]) if features is not None else None,
                'curvature': curvature,
                'quality_flag': segment.quality_flag,
                'cluster_label': int(label),
                'geometry': segment.geometry.wkt if segment.geometry else None
            })
        
        # æ‰¹é‡ä¿å­˜è½¨è¿¹æ®µ
        self._bulk_insert_segments(segments_data)
        
        # ä¿å­˜èšç±»ç»Ÿè®¡
        self._save_cluster_summary(grid_id, city_id, analysis_id, cluster_info)
        
        logger.info(f"âœ… ä¿å­˜å®Œæˆ: {len(segments_data)} ä¸ªè½¨è¿¹æ®µ, {len(cluster_info)} ä¸ªèšç±»")
    
    def _bulk_insert_segments(self, segments_data: List[Dict]):
        """æ‰¹é‡æ’å…¥è½¨è¿¹æ®µ"""
        if not segments_data:
            return
        
        sql = text("""
            INSERT INTO grid_trajectory_segments (
                grid_id, city_id, analysis_id, dataset_name, segment_index,
                start_time, end_time, duration, point_count,
                avg_speed, std_speed, max_speed, min_speed,
                avg_acceleration, std_acceleration,
                avg_yaw, yaw_change_rate, std_yaw,
                trajectory_length_m, direction_cos, direction_sin, curvature,
                quality_flag, cluster_label,
                geometry
            ) VALUES (
                :grid_id, :city_id, :analysis_id, :dataset_name, :segment_index,
                :start_time, :end_time, :duration, :point_count,
                :avg_speed, :std_speed, :max_speed, :min_speed,
                :avg_acceleration, :std_acceleration,
                :avg_yaw, :yaw_change_rate, :std_yaw,
                :trajectory_length_m, :direction_cos, :direction_sin, :curvature,
                :quality_flag, :cluster_label,
                ST_GeomFromText(:geometry, 4326)
            );
        """)
        
        with self.engine.connect() as conn:
            conn.execute(sql, segments_data)
            conn.commit()
    
    def _save_cluster_summary(
        self, 
        grid_id: int, 
        city_id: str, 
        analysis_id: str,
        cluster_info: Dict
    ):
        """ä¿å­˜èšç±»ç»Ÿè®¡"""
        if not cluster_info:
            return
        
        summary_data = []
        for label, info in cluster_info.items():
            summary_data.append({
                'grid_id': grid_id,
                'city_id': city_id,
                'analysis_id': analysis_id,
                'cluster_label': label,
                'segment_count': info['segment_count'],
                'centroid_avg_speed': info['centroid_avg_speed'],
                'centroid_avg_acceleration': info['centroid_avg_acceleration'],
                'centroid_yaw_change_rate': info['centroid_yaw_change_rate'],
                'centroid_direction_cos': info['centroid_direction_cos'],
                'centroid_direction_sin': info['centroid_direction_sin'],
                'speed_range': info['speed_range'],
                'behavior_label': info['behavior_label']
            })
        
        sql = text("""
            INSERT INTO grid_clustering_summary (
                grid_id, city_id, analysis_id, cluster_label, segment_count,
                centroid_avg_speed, centroid_avg_acceleration, centroid_yaw_change_rate,
                centroid_direction_cos, centroid_direction_sin,
                speed_range, behavior_label
            ) VALUES (
                :grid_id, :city_id, :analysis_id, :cluster_label, :segment_count,
                :centroid_avg_speed, :centroid_avg_acceleration, :centroid_yaw_change_rate,
                :centroid_direction_cos, :centroid_direction_sin,
                :speed_range, :behavior_label
            );
        """)
        
        with self.engine.connect() as conn:
            conn.execute(sql, summary_data)
            conn.commit()
    
    def process_single_grid(self, grid_row: pd.Series) -> Dict:
        """å¤„ç†å•ä¸ªgridçš„å®Œæ•´æµç¨‹
        
        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        grid_id = grid_row['grid_id']
        city_id = grid_row['city_id']
        analysis_id = grid_row['analysis_id']
        geometry = grid_row['geometry']
        
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸ¯ å¤„ç†Grid #{grid_id} (åŸå¸‚: {city_id})")
        logger.info(f"{'='*60}")
        
        start_time = time.time()
        stats = {
            'grid_id': grid_id,
            'city_id': city_id,
            'success': False,
            'error': None
        }
        
        try:
            # 1. æŸ¥è¯¢è½¨è¿¹ç‚¹ï¼ˆä½¿ç”¨é«˜æ€§èƒ½æŸ¥è¯¢å™¨ï¼‰
            points_df = self.query_trajectory_points(geometry, grid_id)
            stats['total_points'] = len(points_df)
            stats['trajectory_count'] = points_df['dataset_name'].nunique() if not points_df.empty else 0
            
            if points_df.empty:
                logger.warning("âš ï¸ æ²¡æœ‰è½¨è¿¹ç‚¹ï¼Œè·³è¿‡")
                stats['error'] = 'no_points'
                return stats
            
            logger.info(f"ğŸ“Š è½¨è¿¹ç‚¹ç»Ÿè®¡: {len(points_df)} ä¸ªç‚¹, {stats['trajectory_count']} æ¡è½¨è¿¹")
            
            # 2. åˆ‡åˆ†è½¨è¿¹æ®µ
            segments = self.segment_trajectories(points_df)
            stats['total_segments'] = len(segments)
            
            if not segments:
                logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆè½¨è¿¹æ®µ")
                stats['error'] = 'no_segments'
                return stats
            
            logger.info(f"âœ‚ï¸ åˆ‡åˆ†ç»“æœ: {len(segments)} ä¸ªè½¨è¿¹æ®µ")
            
            # 3. è´¨é‡è¿‡æ»¤å’Œç‰¹å¾æå–
            valid_segments = []
            quality_stats = {}
            
            for segment in segments:
                is_valid, reason = self.filter_segment_quality(segment)
                segment.quality_flag = reason
                
                quality_stats[reason] = quality_stats.get(reason, 0) + 1
                
                if is_valid:
                    # æå–ç‰¹å¾
                    segment.features = self.extract_features(segment)
                    valid_segments.append(segment)
            
            stats['valid_segments'] = len(valid_segments)
            stats['quality_stats'] = quality_stats
            
            logger.info(f"âœ… æœ‰æ•ˆè½¨è¿¹æ®µ: {len(valid_segments)} ({len(valid_segments)/len(segments)*100:.1f}%)")
            logger.info(f"ğŸ“‹ è´¨é‡ç»Ÿè®¡: {quality_stats}")
            
            if not valid_segments:
                logger.warning("âš ï¸ æ²¡æœ‰é€šè¿‡è´¨é‡è¿‡æ»¤çš„è½¨è¿¹æ®µ")
                stats['error'] = 'no_valid_segments'
                return stats
            
            # 4. èšç±»
            labels = self.perform_clustering(valid_segments)
            stats['cluster_labels'] = labels.tolist()
            
            # 5. ç”Ÿæˆè¡Œä¸ºæ ‡ç­¾
            cluster_info = self.generate_behavior_labels(valid_segments, labels)
            stats['cluster_info'] = cluster_info
            
            # æ‰“å°èšç±»è¯¦æƒ…
            logger.info(f"\nğŸ“Š èšç±»è¯¦æƒ…:")
            for label in sorted(cluster_info.keys()):
                info = cluster_info[label]
                logger.info(f"   ç°‡{label}: {info['segment_count']}æ®µ | "
                          f"{info['behavior_label']} | {info['speed_range']}")
            
            # 6. ä¿å­˜ç»“æœ
            self.save_results(
                grid_id, city_id, analysis_id,
                valid_segments, labels, cluster_info
            )
            
            stats['success'] = True
            stats['elapsed_time'] = time.time() - start_time
            
            logger.info(f"\nâœ… Grid #{grid_id} å¤„ç†å®Œæˆ (è€—æ—¶: {stats['elapsed_time']:.2f}ç§’)")
            
        except Exception as e:
            logger.error(f"âŒ Grid #{grid_id} å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            stats['error'] = str(e)
        
        return stats
    
    def process_all_grids(
        self,
        city_id: Optional[str] = None,
        max_grids: Optional[int] = None,
        grid_ids: Optional[List[int]] = None
    ) -> pd.DataFrame:
        """æ‰¹é‡å¤„ç†å¤šä¸ªgrid
        
        Returns:
            å¤„ç†ç»Ÿè®¡DataFrame
        """
        logger.info("\n" + "="*70)
        logger.info("ğŸš€ æ‰¹é‡Gridè½¨è¿¹èšç±»åˆ†æ")
        logger.info("="*70)
        
        # åŠ è½½grids
        grids_df = self.load_hotspot_grids(city_id, max_grids, grid_ids)
        
        if grids_df.empty:
            logger.error("âŒ æ²¡æœ‰å¯å¤„ç†çš„grid")
            return pd.DataFrame()
        
        logger.info(f"\nğŸ“‹ å‡†å¤‡å¤„ç† {len(grids_df)} ä¸ªgrid")
        
        # æ‰¹é‡å¤„ç†
        all_stats = []
        start_time = time.time()
        
        for idx, grid_row in grids_df.iterrows():
            logger.info(f"\n[{idx+1}/{len(grids_df)}] å¼€å§‹å¤„ç†...")
            
            stats = self.process_single_grid(grid_row)
            all_stats.append(stats)
        
        # æ±‡æ€»ç»Ÿè®¡
        total_time = time.time() - start_time
        stats_df = pd.DataFrame(all_stats)
        
        success_count = stats_df['success'].sum()
        
        logger.info("\n" + "="*70)
        logger.info("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ")
        logger.info("="*70)
        logger.info(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        logger.info(f"æˆåŠŸ: {success_count}/{len(stats_df)}")
        logger.info(f"å¤±è´¥: {len(stats_df) - success_count}")
        
        if success_count > 0:
            successful_stats = stats_df[stats_df['success'] == True]
            logger.info(f"\næˆåŠŸgridç»Ÿè®¡:")
            logger.info(f"  å¹³å‡è½¨è¿¹ç‚¹æ•°: {successful_stats['total_points'].mean():.0f}")
            logger.info(f"  å¹³å‡è½¨è¿¹æ®µæ•°: {successful_stats['total_segments'].mean():.0f}")
            logger.info(f"  å¹³å‡æœ‰æ•ˆæ®µæ•°: {successful_stats['valid_segments'].mean():.0f}")
        
        return stats_df
    
    # ==================== è¾…åŠ©å‡½æ•° ====================
    
    @staticmethod
    def _haversine_distance(lat1, lon1, lat2, lon2) -> float:
        """è®¡ç®—ä¸¤ç‚¹é—´çš„Haversineè·ç¦»ï¼ˆç±³ï¼‰"""
        R = 6371000  # åœ°çƒåŠå¾„ï¼ˆç±³ï¼‰
        
        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        
        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
        c = 2 * np.arcsin(np.sqrt(a))
        
        return R * c
    
    def _calculate_total_distance(self, points: pd.DataFrame) -> float:
        """è®¡ç®—è½¨è¿¹æ€»è·ç¦»ï¼ˆç±³ï¼‰"""
        if len(points) < 2:
            return 0.0
        
        total_dist = 0.0
        for i in range(1, len(points)):
            dist = self._haversine_distance(
                points.iloc[i-1]['lat'], points.iloc[i-1]['lon'],
                points.iloc[i]['lat'], points.iloc[i]['lon']
            )
            total_dist += dist
        
        return total_dist
    
    def _calculate_max_consecutive_distance(self, points: pd.DataFrame) -> float:
        """è®¡ç®—æœ€å¤§è¿ç»­ç‚¹é—´è·ï¼ˆç±³ï¼‰"""
        if len(points) < 2:
            return 0.0
        
        max_dist = 0.0
        for i in range(1, len(points)):
            dist = self._haversine_distance(
                points.iloc[i-1]['lat'], points.iloc[i-1]['lon'],
                points.iloc[i]['lat'], points.iloc[i]['lon']
            )
            max_dist = max(max_dist, dist)
        
        return max_dist







