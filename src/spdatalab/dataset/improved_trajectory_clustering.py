"""æ”¹è¿›çš„è½¨è¿¹èšç±»æ¨¡å—

åŸºäºè°ƒç ”ç»“æœçš„æ”¹è¿›å®ç°ï¼š
1. æ”¹è¿›çš„ç›¸ä¼¼åº¦åº¦é‡ï¼ˆFrÃ©chetè·ç¦»ã€Hausdorffè·ç¦»ï¼‰
2. å¢å¼ºçš„ç‰¹å¾æå–ï¼ˆç©ºé—´å½¢æ€ç‰¹å¾ï¼‰
3. è‡ªé€‚åº”åˆ†æ®µç­–ç•¥
4. å¤šç§èšç±»ç®—æ³•å¯¹æ¯”

ä¸»è¦æ”¹è¿›ï¼š
- ä½¿ç”¨FrÃ©chetè·ç¦»æ›¿ä»£æ¬§æ°è·ç¦»
- æ·»åŠ ç©ºé—´å½¢æ€ç‰¹å¾ï¼ˆæ›²ç‡ã€æ›²æŠ˜åº¦ç­‰ï¼‰
- å®ç°è‡ªé€‚åº”åˆ†æ®µ
- æä¾›TRACLUSç®—æ³•å®ç°
"""

from __future__ import annotations
import logging
import numpy as np
import pandas as pd
from typing import List, Tuple, Optional
from dataclasses import dataclass
from shapely.geometry import LineString, Point
from scipy.spatial.distance import directed_hausdorff, euclidean
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score

logger = logging.getLogger(__name__)


# ==================== ç›¸ä¼¼åº¦åº¦é‡å‡½æ•° ====================

def frechet_distance(traj1: np.ndarray, traj2: np.ndarray) -> float:
    """
    è®¡ç®—ä¸¤æ¡è½¨è¿¹çš„FrÃ©chetè·ç¦»ï¼ˆç¦»æ•£ç‰ˆæœ¬ï¼‰
    
    FrÃ©chetè·ç¦»è€ƒè™‘äº†è½¨è¿¹çš„é¡ºåºå’Œè¿ç»­æ€§ï¼Œæ¯”Hausdorffæ›´é€‚åˆè½¨è¿¹ç›¸ä¼¼åº¦ã€‚
    ä½¿ç”¨åŠ¨æ€è§„åˆ’ç®—æ³•è®¡ç®—ã€‚
    
    Args:
        traj1: è½¨è¿¹1çš„åæ ‡æ•°ç»„ (N, 2)
        traj2: è½¨è¿¹2çš„åæ ‡æ•°ç»„ (M, 2)
        
    Returns:
        FrÃ©chetè·ç¦»ï¼ˆç±³ï¼‰
        
    å‚è€ƒï¼š
        Eiter & Mannila, "Computing discrete FrÃ©chet distance" (1994)
    """
    n, m = len(traj1), len(traj2)
    
    # è·ç¦»çŸ©é˜µ
    ca = np.full((n, m), -1.0)
    
    def c(i: int, j: int) -> float:
        """é€’å½’è®¡ç®—è€¦åˆè·ç¦»"""
        if ca[i, j] > -1:
            return ca[i, j]
        
        # ç‚¹ä¹‹é—´çš„æ¬§æ°è·ç¦»ï¼ˆç±³ï¼‰
        d = haversine_distance(
            traj1[i][1], traj1[i][0],
            traj2[j][1], traj2[j][0]
        )
        
        if i == 0 and j == 0:
            ca[i, j] = d
        elif i > 0 and j == 0:
            ca[i, j] = max(c(i-1, 0), d)
        elif i == 0 and j > 0:
            ca[i, j] = max(c(0, j-1), d)
        else:
            ca[i, j] = max(
                min(c(i-1, j), c(i-1, j-1), c(i, j-1)),
                d
            )
        
        return ca[i, j]
    
    return c(n-1, m-1)


def hausdorff_distance_trajectory(traj1: np.ndarray, traj2: np.ndarray) -> float:
    """
    è®¡ç®—ä¸¤æ¡è½¨è¿¹çš„Hausdorffè·ç¦»
    
    Args:
        traj1: è½¨è¿¹1çš„åæ ‡æ•°ç»„ (N, 2) [lon, lat]
        traj2: è½¨è¿¹2çš„åæ ‡æ•°ç»„ (M, 2) [lon, lat]
        
    Returns:
        Hausdorffè·ç¦»ï¼ˆç±³ï¼‰
    """
    # ä½¿ç”¨scipyçš„directed_hausdorffï¼ˆè¿”å›åƒç´ è·ç¦»ï¼‰
    # è¿™é‡Œæˆ‘ä»¬éœ€è¦è½¬æ¢ä¸ºå®é™…è·ç¦»
    
    max_dist_12 = 0.0
    for p1 in traj1:
        min_dist = min(
            haversine_distance(p1[1], p1[0], p2[1], p2[0]) 
            for p2 in traj2
        )
        max_dist_12 = max(max_dist_12, min_dist)
    
    max_dist_21 = 0.0
    for p2 in traj2:
        min_dist = min(
            haversine_distance(p2[1], p2[0], p1[1], p1[0]) 
            for p1 in traj1
        )
        max_dist_21 = max(max_dist_21, min_dist)
    
    return max(max_dist_12, max_dist_21)


def perpendicular_distance(point: np.ndarray, line_start: np.ndarray, line_end: np.ndarray) -> float:
    """
    è®¡ç®—ç‚¹åˆ°çº¿æ®µçš„å‚ç›´è·ç¦»ï¼ˆTRACLUSä¸­ä½¿ç”¨ï¼‰
    
    Args:
        point: ç‚¹åæ ‡ [lon, lat]
        line_start: çº¿æ®µèµ·ç‚¹ [lon, lat]
        line_end: çº¿æ®µç»ˆç‚¹ [lon, lat]
        
    Returns:
        å‚ç›´è·ç¦»ï¼ˆç±³ï¼‰
    """
    # å‘é‡
    line_vec = line_end - line_start
    point_vec = point - line_start
    
    # çº¿æ®µé•¿åº¦
    line_len = np.linalg.norm(line_vec)
    
    if line_len < 1e-10:  # é€€åŒ–ä¸ºç‚¹
        return haversine_distance(point[1], point[0], line_start[1], line_start[0])
    
    # æŠ•å½±é•¿åº¦
    line_unitvec = line_vec / line_len
    proj_len = np.dot(point_vec, line_unitvec)
    
    # æŠ•å½±ç‚¹
    if proj_len < 0:
        # æŠ•å½±åœ¨èµ·ç‚¹ä¹‹å‰
        closest = line_start
    elif proj_len > line_len:
        # æŠ•å½±åœ¨ç»ˆç‚¹ä¹‹å
        closest = line_end
    else:
        # æŠ•å½±åœ¨çº¿æ®µä¸Š
        closest = line_start + proj_len * line_unitvec
    
    return haversine_distance(point[1], point[0], closest[1], closest[0])


def parallel_distance(point: np.ndarray, line_start: np.ndarray, line_end: np.ndarray) -> float:
    """
    è®¡ç®—ç‚¹åˆ°çº¿æ®µçš„å¹³è¡Œè·ç¦»ï¼ˆTRACLUSä¸­ä½¿ç”¨ï¼‰
    
    Args:
        point: ç‚¹åæ ‡ [lon, lat]
        line_start: çº¿æ®µèµ·ç‚¹ [lon, lat]
        line_end: çº¿æ®µç»ˆç‚¹ [lon, lat]
        
    Returns:
        å¹³è¡Œè·ç¦»ï¼ˆç±³ï¼‰
    """
    line_vec = line_end - line_start
    point_vec = point - line_start
    
    line_len = np.linalg.norm(line_vec)
    
    if line_len < 1e-10:
        return 0.0
    
    # æŠ•å½±é•¿åº¦
    line_unitvec = line_vec / line_len
    proj_len = np.dot(point_vec, line_unitvec)
    
    # å¹³è¡Œè·ç¦»
    if proj_len < 0:
        return abs(proj_len)
    elif proj_len > line_len:
        return proj_len - line_len
    else:
        return 0.0


def segment_distance(seg1_coords: np.ndarray, seg2_coords: np.ndarray) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªè½¨è¿¹æ®µçš„è·ç¦»ï¼ˆTRACLUSé£æ ¼ï¼‰
    
    ç»¼åˆè€ƒè™‘ï¼š
    1. å‚ç›´è·ç¦»ï¼ˆå½¢çŠ¶ç›¸ä¼¼åº¦ï¼‰
    2. å¹³è¡Œè·ç¦»ï¼ˆä½ç½®å¯¹é½åº¦ï¼‰
    3. è§’åº¦è·ç¦»ï¼ˆæ–¹å‘ç›¸ä¼¼åº¦ï¼‰
    
    Args:
        seg1_coords: è½¨è¿¹æ®µ1çš„åæ ‡ (N, 2)
        seg2_coords: è½¨è¿¹æ®µ2çš„åæ ‡ (M, 2)
        
    Returns:
        æ®µè·ç¦»ï¼ˆç±³ï¼‰
    """
    if len(seg1_coords) < 2 or len(seg2_coords) < 2:
        return float('inf')
    
    # è®¡ç®—seg1æ¯ä¸ªç‚¹åˆ°seg2çš„å‚ç›´å’Œå¹³è¡Œè·ç¦»
    perp_dists = []
    para_dists = []
    
    seg2_start = seg2_coords[0]
    seg2_end = seg2_coords[-1]
    
    for point in seg1_coords:
        perp_dist = perpendicular_distance(point, seg2_start, seg2_end)
        para_dist = parallel_distance(point, seg2_start, seg2_end)
        perp_dists.append(perp_dist)
        para_dists.append(para_dist)
    
    # åå‘è®¡ç®—seg2æ¯ä¸ªç‚¹åˆ°seg1çš„è·ç¦»
    seg1_start = seg1_coords[0]
    seg1_end = seg1_coords[-1]
    
    for point in seg2_coords:
        perp_dist = perpendicular_distance(point, seg1_start, seg1_end)
        para_dist = parallel_distance(point, seg1_start, seg1_end)
        perp_dists.append(perp_dist)
        para_dists.append(para_dist)
    
    # å¹³å‡è·ç¦»
    avg_perp = np.mean(perp_dists)
    avg_para = np.mean(para_dists)
    
    # è§’åº¦è·ç¦»
    angle_dist = angle_difference(seg1_coords, seg2_coords)
    
    # ç»„åˆè·ç¦»ï¼ˆæƒé‡å¯è°ƒï¼‰
    return 0.6 * avg_perp + 0.3 * avg_para + 0.1 * angle_dist


def angle_difference(seg1_coords: np.ndarray, seg2_coords: np.ndarray) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªè½¨è¿¹æ®µçš„è§’åº¦å·®å¼‚
    
    Args:
        seg1_coords: è½¨è¿¹æ®µ1çš„åæ ‡ (N, 2)
        seg2_coords: è½¨è¿¹æ®µ2çš„åæ ‡ (M, 2)
        
    Returns:
        è§’åº¦å·®å¼‚ï¼ˆå¼§åº¦è½¬æ¢ä¸ºè·ç¦»å•ä½ï¼‰
    """
    # è®¡ç®—æ–¹å‘å‘é‡
    vec1 = seg1_coords[-1] - seg1_coords[0]
    vec2 = seg2_coords[-1] - seg2_coords[0]
    
    # å½’ä¸€åŒ–
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    
    if norm1 < 1e-10 or norm2 < 1e-10:
        return 0.0
    
    vec1 = vec1 / norm1
    vec2 = vec2 / norm2
    
    # ä½™å¼¦ç›¸ä¼¼åº¦
    cos_sim = np.dot(vec1, vec2)
    cos_sim = np.clip(cos_sim, -1.0, 1.0)
    
    # è§’åº¦å·®ï¼ˆå¼§åº¦ï¼‰
    angle_diff = np.arccos(cos_sim)
    
    # è½¬æ¢ä¸ºè·ç¦»å•ä½ï¼ˆä¹˜ä»¥å¹³å‡é•¿åº¦ï¼‰
    avg_len = (norm1 + norm2) / 2
    return angle_diff * avg_len * 10  # æ”¾å¤§ç³»æ•°


# ==================== å¢å¼ºç‰¹å¾æå– ====================

def extract_enhanced_features(segment_coords: np.ndarray, segment_attrs: dict) -> np.ndarray:
    """
    æå–å¢å¼ºçš„17ç»´ç‰¹å¾å‘é‡
    
    æ–°å¢ç‰¹å¾ï¼š
    - æ›²ç‡ï¼ˆcurvatureï¼‰
    - æ›²æŠ˜åº¦ï¼ˆtortuosityï¼‰
    - æ–¹å‘ç†µï¼ˆdirection_entropyï¼‰
    - é€Ÿåº¦ç†µï¼ˆspeed_entropyï¼‰
    - åœè½¦æ¯”ä¾‹ï¼ˆstop_ratioï¼‰
    - åŠ é€Ÿåº¦å³°å€¼æ•°ï¼ˆaccel_peaksï¼‰
    - è½¨è¿¹åŒ…ç»œé¢ç§¯ï¼ˆenvelope_areaï¼‰
    
    Args:
        segment_coords: è½¨è¿¹æ®µåæ ‡ (N, 2) [lon, lat]
        segment_attrs: å±æ€§å­—å…¸ï¼ˆé€Ÿåº¦ã€æ—¶é—´ç­‰ï¼‰
        
    Returns:
        17ç»´ç‰¹å¾å‘é‡
    """
    features = []
    
    # ===== åŸæœ‰çš„åŸºç¡€ç‰¹å¾ï¼ˆ10ç»´ï¼‰=====
    # é€Ÿåº¦ç‰¹å¾ï¼ˆ4ç»´ï¼‰
    speeds = segment_attrs.get('speeds', [])
    if len(speeds) > 0:
        features.append(np.mean(speeds))   # avg_speed
        features.append(np.std(speeds))    # std_speed
        features.append(np.max(speeds))    # max_speed
        features.append(np.min(speeds))    # min_speed
    else:
        features.extend([0.0, 0.0, 0.0, 0.0])
    
    # åŠ é€Ÿåº¦ç‰¹å¾ï¼ˆ2ç»´ï¼‰
    accelerations = segment_attrs.get('accelerations', [])
    if len(accelerations) > 0:
        features.append(np.mean(accelerations))  # avg_accel
        features.append(np.std(accelerations))   # std_accel
    else:
        features.extend([0.0, 0.0])
    
    # èˆªå‘è§’ç‰¹å¾ï¼ˆ2ç»´ï¼‰
    yaw_changes = segment_attrs.get('yaw_changes', [])
    yaws = segment_attrs.get('yaws', [])
    if len(yaw_changes) > 0:
        features.append(np.mean(np.abs(yaw_changes)))  # yaw_change_rate
    else:
        features.append(0.0)
    if len(yaws) > 0:
        features.append(np.std(yaws))  # std_yaw
    else:
        features.append(0.0)
    
    # æ–¹å‘ç‰¹å¾ï¼ˆ2ç»´ï¼‰
    delta_lng = segment_coords[-1][0] - segment_coords[0][0]
    delta_lat = segment_coords[-1][1] - segment_coords[0][1]
    angle = np.arctan2(delta_lat, delta_lng)
    features.append(np.cos(angle))  # direction_cos
    features.append(np.sin(angle))  # direction_sin
    
    # ===== æ–°å¢ç©ºé—´å½¢æ€ç‰¹å¾ï¼ˆ7ç»´ï¼‰=====
    
    # 1. æ›²ç‡ï¼ˆcurvatureï¼‰ï¼šè½¨è¿¹å¼¯æ›²ç¨‹åº¦
    curvature = calculate_curvature(segment_coords)
    features.append(curvature)
    
    # 2. æ›²æŠ˜åº¦ï¼ˆtortuosityï¼‰ï¼šå®é™…é•¿åº¦ / ç›´çº¿è·ç¦»
    tortuosity = calculate_tortuosity(segment_coords)
    features.append(tortuosity)
    
    # 3. æ–¹å‘ç†µï¼ˆdirection_entropyï¼‰ï¼šæ–¹å‘å˜åŒ–çš„å¤æ‚åº¦
    direction_entropy = calculate_direction_entropy(segment_coords)
    features.append(direction_entropy)
    
    # 4. é€Ÿåº¦ç†µï¼ˆspeed_entropyï¼‰ï¼šé€Ÿåº¦å˜åŒ–çš„å¤æ‚åº¦
    if len(speeds) > 0:
        speed_entropy = calculate_value_entropy(speeds)
    else:
        speed_entropy = 0.0
    features.append(speed_entropy)
    
    # 5. åœè½¦æ¯”ä¾‹ï¼ˆstop_ratioï¼‰ï¼šé€Ÿåº¦<1m/sçš„æ—¶é—´å æ¯”
    if len(speeds) > 0:
        stop_ratio = np.sum(np.array(speeds) < 1.0) / len(speeds)
    else:
        stop_ratio = 0.0
    features.append(stop_ratio)
    
    # 6. åŠ é€Ÿåº¦å³°å€¼æ•°ï¼ˆaccel_peaksï¼‰ï¼šæ˜¾è‘—åŠ å‡é€Ÿæ¬¡æ•°
    if len(accelerations) > 1:
        accel_peaks = count_peaks(accelerations, threshold=2.0)
    else:
        accel_peaks = 0
    features.append(float(accel_peaks))
    
    # 7. è½¨è¿¹åŒ…ç»œé¢ç§¯ï¼ˆenvelope_areaï¼‰ï¼šè½¨è¿¹ä¸ç›´çº¿çš„é¢ç§¯
    envelope_area = calculate_envelope_area(segment_coords)
    features.append(envelope_area)
    
    return np.array(features)


def calculate_curvature(coords: np.ndarray) -> float:
    """
    è®¡ç®—è½¨è¿¹çš„å¹³å‡æ›²ç‡
    
    ä½¿ç”¨ä¸‰ç‚¹æ³•è®¡ç®—æ›²ç‡ï¼šå¯¹æ¯3ä¸ªè¿ç»­ç‚¹ï¼Œè®¡ç®—å¤–æ¥åœ†åŠå¾„çš„å€’æ•°
    
    Args:
        coords: è½¨è¿¹åæ ‡ (N, 2)
        
    Returns:
        å¹³å‡æ›²ç‡
    """
    if len(coords) < 3:
        return 0.0
    
    curvatures = []
    
    for i in range(len(coords) - 2):
        p1, p2, p3 = coords[i], coords[i+1], coords[i+2]
        
        # ä¸‰è§’å½¢ä¸‰è¾¹é•¿
        a = np.linalg.norm(p2 - p1)
        b = np.linalg.norm(p3 - p2)
        c = np.linalg.norm(p3 - p1)
        
        # åŠå‘¨é•¿
        s = (a + b + c) / 2
        
        # é¢ç§¯ï¼ˆæµ·ä¼¦å…¬å¼ï¼‰
        area_sq = s * (s - a) * (s - b) * (s - c)
        
        if area_sq > 0:
            area = np.sqrt(area_sq)
            # æ›²ç‡ = 4 * é¢ç§¯ / (a * b * c)
            if a * b * c > 0:
                curvature = 4 * area / (a * b * c)
                curvatures.append(curvature)
    
    return np.mean(curvatures) if curvatures else 0.0


def calculate_tortuosity(coords: np.ndarray) -> float:
    """
    è®¡ç®—æ›²æŠ˜åº¦ï¼šå®é™…è·¯å¾„é•¿åº¦ / ç›´çº¿è·ç¦»
    
    å€¼è¶Šå¤§è¡¨ç¤ºè½¨è¿¹è¶Šæ›²æŠ˜
    - ç›´çº¿ï¼š~1.0
    - å¼¯æ›²ï¼š1.5-2.0
    - éå¸¸æ›²æŠ˜ï¼š>2.0
    
    Args:
        coords: è½¨è¿¹åæ ‡ (N, 2)
        
    Returns:
        æ›²æŠ˜åº¦
    """
    if len(coords) < 2:
        return 1.0
    
    # å®é™…è·¯å¾„é•¿åº¦
    path_length = 0.0
    for i in range(1, len(coords)):
        path_length += haversine_distance(
            coords[i-1][1], coords[i-1][0],
            coords[i][1], coords[i][0]
        )
    
    # ç›´çº¿è·ç¦»
    straight_dist = haversine_distance(
        coords[0][1], coords[0][0],
        coords[-1][1], coords[-1][0]
    )
    
    if straight_dist < 1.0:  # é¿å…é™¤é›¶
        return 1.0
    
    return path_length / straight_dist


def calculate_direction_entropy(coords: np.ndarray) -> float:
    """
    è®¡ç®—æ–¹å‘ç†µï¼šæ–¹å‘å˜åŒ–çš„å¤æ‚åº¦
    
    å°†æ–¹å‘åˆ†ä¸º8ä¸ªbinï¼ˆN, NE, E, SE, S, SW, W, NWï¼‰ï¼Œè®¡ç®—ç†µ
    
    Args:
        coords: è½¨è¿¹åæ ‡ (N, 2)
        
    Returns:
        æ–¹å‘ç†µ
    """
    if len(coords) < 2:
        return 0.0
    
    # è®¡ç®—æ¯æ®µçš„æ–¹å‘
    directions = []
    for i in range(1, len(coords)):
        dx = coords[i][0] - coords[i-1][0]
        dy = coords[i][1] - coords[i-1][1]
        angle = np.arctan2(dy, dx)  # [-Ï€, Ï€]
        directions.append(angle)
    
    # åˆ†ä¸º8ä¸ªbin
    bins = np.linspace(-np.pi, np.pi, 9)
    hist, _ = np.histogram(directions, bins=bins)
    
    # è®¡ç®—ç†µ
    probs = hist / np.sum(hist)
    probs = probs[probs > 0]  # å»é™¤0æ¦‚ç‡
    entropy = -np.sum(probs * np.log2(probs))
    
    return entropy


def calculate_value_entropy(values: list) -> float:
    """
    è®¡ç®—æ•°å€¼åºåˆ—çš„ç†µï¼ˆç”¨äºé€Ÿåº¦ã€åŠ é€Ÿåº¦ç­‰ï¼‰
    
    Args:
        values: æ•°å€¼åˆ—è¡¨
        
    Returns:
        ç†µå€¼
    """
    if len(values) < 2:
        return 0.0
    
    # åˆ†ä¸º10ä¸ªbin
    hist, _ = np.histogram(values, bins=10)
    
    # è®¡ç®—ç†µ
    probs = hist / np.sum(hist)
    probs = probs[probs > 0]
    entropy = -np.sum(probs * np.log2(probs))
    
    return entropy


def count_peaks(values: list, threshold: float = 2.0) -> int:
    """
    è®¡ç®—å³°å€¼æ•°é‡ï¼ˆåŠ é€Ÿåº¦å³°å€¼ï¼‰
    
    Args:
        values: æ•°å€¼åˆ—è¡¨
        threshold: å³°å€¼é˜ˆå€¼
        
    Returns:
        å³°å€¼æ•°é‡
    """
    if len(values) < 3:
        return 0
    
    values = np.array(values)
    peaks = 0
    
    for i in range(1, len(values) - 1):
        # å±€éƒ¨æœ€å¤§å€¼ä¸”è¶…è¿‡é˜ˆå€¼
        if abs(values[i]) > threshold:
            if (values[i] > values[i-1] and values[i] > values[i+1]) or \
               (values[i] < values[i-1] and values[i] < values[i+1]):
                peaks += 1
    
    return peaks


def calculate_envelope_area(coords: np.ndarray) -> float:
    """
    è®¡ç®—è½¨è¿¹åŒ…ç»œé¢ç§¯ï¼šè½¨è¿¹ä¸èµ·ç»ˆç‚¹è¿çº¿å›´æˆçš„é¢ç§¯
    
    Args:
        coords: è½¨è¿¹åæ ‡ (N, 2)
        
    Returns:
        åŒ…ç»œé¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰
    """
    if len(coords) < 3:
        return 0.0
    
    # ä½¿ç”¨Shapelyè®¡ç®—å¤šè¾¹å½¢é¢ç§¯
    from shapely.geometry import Polygon
    
    # æ„å»ºå¤šè¾¹å½¢ï¼šè½¨è¿¹ç‚¹ + èµ·ç»ˆç‚¹è¿çº¿
    polygon_coords = list(coords) + [coords[0]]
    
    try:
        polygon = Polygon(polygon_coords)
        # æ³¨æ„ï¼šè¿™æ˜¯åº¦æ•°å•ä½ï¼Œéœ€è¦è½¬æ¢ä¸ºç±³
        # ç²—ç•¥è¿‘ä¼¼ï¼š1åº¦ â‰ˆ 111km
        area_deg = polygon.area
        area_m2 = area_deg * (111000 ** 2)
        return area_m2
    except:
        return 0.0


# ==================== è‡ªé€‚åº”åˆ†æ®µç­–ç•¥ ====================

def adaptive_segmentation(
    trajectory_coords: np.ndarray,
    trajectory_attrs: dict,
    min_points: int = 5
) -> List[Tuple[int, int]]:
    """
    è‡ªé€‚åº”è½¨è¿¹åˆ†æ®µï¼ˆåŸºäºç‰¹å¾ç‚¹æ£€æµ‹ï¼‰
    
    æ£€æµ‹ç‰¹å¾ç‚¹ï¼š
    1. é€Ÿåº¦çªå˜ç‚¹ï¼ˆåŠ é€Ÿ/å‡é€Ÿï¼‰
    2. èˆªå‘è§’çªå˜ç‚¹ï¼ˆè½¬å¼¯ï¼‰
    3. åœè½¦ç‚¹
    
    Args:
        trajectory_coords: è½¨è¿¹åæ ‡ (N, 2)
        trajectory_attrs: å±æ€§å­—å…¸ï¼ˆé€Ÿåº¦ã€æ—¶é—´æˆ³ç­‰ï¼‰
        min_points: æ¯æ®µæœ€å°‘ç‚¹æ•°
        
    Returns:
        åˆ†æ®µç´¢å¼•åˆ—è¡¨ [(start, end), ...]
    """
    n = len(trajectory_coords)
    
    if n < min_points:
        return []
    
    # æ£€æµ‹ç‰¹å¾ç‚¹
    feature_points = set([0, n-1])  # èµ·ç»ˆç‚¹å¿…é¡»åŒ…å«
    
    # 1. é€Ÿåº¦çªå˜ç‚¹
    speeds = trajectory_attrs.get('speeds', [])
    if len(speeds) > 2:
        speed_changes = np.abs(np.diff(speeds))
        speed_threshold = np.mean(speed_changes) + 2 * np.std(speed_changes)
        
        for i in range(1, len(speed_changes)):
            if speed_changes[i] > speed_threshold:
                feature_points.add(i)
    
    # 2. èˆªå‘è§’çªå˜ç‚¹
    if len(trajectory_coords) > 2:
        angles = []
        for i in range(1, len(trajectory_coords)):
            dx = trajectory_coords[i][0] - trajectory_coords[i-1][0]
            dy = trajectory_coords[i][1] - trajectory_coords[i-1][1]
            angle = np.arctan2(dy, dx)
            angles.append(angle)
        
        if len(angles) > 1:
            angle_changes = np.abs(np.diff(angles))
            # å¤„ç†è§’åº¦è·³å˜ï¼ˆ-Ï€åˆ°Ï€ï¼‰
            angle_changes = np.minimum(angle_changes, 2*np.pi - angle_changes)
            
            # è½¬å¼¯é˜ˆå€¼ï¼š30åº¦
            turn_threshold = np.radians(30)
            
            for i in range(len(angle_changes)):
                if angle_changes[i] > turn_threshold:
                    feature_points.add(i+1)
    
    # 3. åœè½¦ç‚¹
    if len(speeds) > 0:
        for i in range(len(speeds)):
            if speeds[i] < 0.5:  # å‡ ä¹åœæ­¢
                feature_points.add(i)
    
    # æ’åºç‰¹å¾ç‚¹
    feature_points = sorted(list(feature_points))
    
    # ç”Ÿæˆåˆ†æ®µï¼ˆç¡®ä¿æ¯æ®µè‡³å°‘min_pointsä¸ªç‚¹ï¼‰
    segments = []
    start_idx = 0
    
    for i in range(1, len(feature_points)):
        end_idx = feature_points[i]
        
        if end_idx - start_idx >= min_points:
            segments.append((start_idx, end_idx))
            start_idx = end_idx
        # å¦åˆ™å»¶ç»­å½“å‰æ®µ
    
    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåˆ†æ®µï¼Œè¿”å›æ•´æ¡è½¨è¿¹
    if not segments:
        segments = [(0, n-1)]
    
    return segments


# ==================== è¾…åŠ©å‡½æ•° ====================

def haversine_distance(lat1, lon1, lat2, lon2) -> float:
    """è®¡ç®—ä¸¤ç‚¹é—´çš„Haversineè·ç¦»ï¼ˆç±³ï¼‰"""
    R = 6371000  # åœ°çƒåŠå¾„ï¼ˆç±³ï¼‰
    
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    
    return R * c


# ==================== æ”¹è¿›çš„èšç±»å™¨ ====================

class ImprovedTrajectoryClusterer:
    """
    æ”¹è¿›çš„è½¨è¿¹èšç±»å™¨
    
    ä¸»è¦æ”¹è¿›ï¼š
    1. ä½¿ç”¨FrÃ©chetè·ç¦»æˆ–Hausdorffè·ç¦»
    2. å¢å¼ºçš„ç‰¹å¾æå–ï¼ˆ17ç»´ï¼‰
    3. è‡ªé€‚åº”åˆ†æ®µ
    4. å¤šç§èšç±»ç®—æ³•ï¼ˆDBSCANã€å±‚æ¬¡èšç±»ï¼‰
    """
    
    def __init__(
        self,
        distance_metric: str = 'frechet',  # 'frechet', 'hausdorff', 'segment'
        clustering_method: str = 'dbscan',  # 'dbscan', 'hierarchical'
        use_enhanced_features: bool = True,
        use_adaptive_segmentation: bool = False
    ):
        """
        åˆå§‹åŒ–æ”¹è¿›èšç±»å™¨
        
        Args:
            distance_metric: è·ç¦»åº¦é‡æ–¹æ³•
            clustering_method: èšç±»ç®—æ³•
            use_enhanced_features: æ˜¯å¦ä½¿ç”¨å¢å¼ºç‰¹å¾
            use_adaptive_segmentation: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”åˆ†æ®µ
        """
        self.distance_metric = distance_metric
        self.clustering_method = clustering_method
        self.use_enhanced_features = use_enhanced_features
        self.use_adaptive_segmentation = use_adaptive_segmentation
        self.scaler = StandardScaler()
        
        logger.info(f"ğŸš€ æ”¹è¿›èšç±»å™¨åˆå§‹åŒ–:")
        logger.info(f"   è·ç¦»åº¦é‡: {distance_metric}")
        logger.info(f"   èšç±»æ–¹æ³•: {clustering_method}")
        logger.info(f"   å¢å¼ºç‰¹å¾: {use_enhanced_features}")
        logger.info(f"   è‡ªé€‚åº”åˆ†æ®µ: {use_adaptive_segmentation}")
    
    def compute_distance_matrix(self, segments: List[dict]) -> np.ndarray:
        """
        è®¡ç®—è½¨è¿¹æ®µä¹‹é—´çš„è·ç¦»çŸ©é˜µ
        
        Args:
            segments: è½¨è¿¹æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«coordså’Œattrs
            
        Returns:
            è·ç¦»çŸ©é˜µ (N, N)
        """
        n = len(segments)
        dist_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(i+1, n):
                coords_i = segments[i]['coords']
                coords_j = segments[j]['coords']
                
                if self.distance_metric == 'frechet':
                    dist = frechet_distance(coords_i, coords_j)
                elif self.distance_metric == 'hausdorff':
                    dist = hausdorff_distance_trajectory(coords_i, coords_j)
                elif self.distance_metric == 'segment':
                    dist = segment_distance(coords_i, coords_j)
                else:
                    # é»˜è®¤ä½¿ç”¨æ¬§æ°è·ç¦»ï¼ˆåœ¨ç‰¹å¾ç©ºé—´ï¼‰
                    dist = euclidean(
                        segments[i]['features'],
                        segments[j]['features']
                    )
                
                dist_matrix[i, j] = dist
                dist_matrix[j, i] = dist
        
        return dist_matrix
    
    def cluster(
        self,
        segments: List[dict],
        eps: float = 0.8,
        min_samples: int = 5,
        n_clusters: Optional[int] = None
    ) -> np.ndarray:
        """
        æ‰§è¡Œèšç±»
        
        Args:
            segments: è½¨è¿¹æ®µåˆ—è¡¨
            eps: DBSCANçš„epså‚æ•°
            min_samples: DBSCANçš„min_sampleså‚æ•°
            n_clusters: å±‚æ¬¡èšç±»çš„èšç±»æ•°
            
        Returns:
            èšç±»æ ‡ç­¾æ•°ç»„
        """
        if not segments:
            return np.array([])
        
        logger.info(f"ğŸ”¬ æ‰§è¡Œæ”¹è¿›èšç±»ï¼ˆ{len(segments)}ä¸ªè½¨è¿¹æ®µï¼‰...")
        
        # æå–æˆ–ä½¿ç”¨é¢„è®¡ç®—çš„ç‰¹å¾
        features_list = [seg['features'] for seg in segments]
        features_matrix = np.vstack(features_list)
        
        # æ ‡å‡†åŒ–
        features_scaled = self.scaler.fit_transform(features_matrix)
        
        # æ ¹æ®æ–¹æ³•é€‰æ‹©èšç±»ç®—æ³•
        if self.clustering_method == 'dbscan':
            clusterer = DBSCAN(
                eps=eps,
                min_samples=min_samples,
                metric='euclidean'
            )
            labels = clusterer.fit_predict(features_scaled)
            
        elif self.clustering_method == 'hierarchical':
            if n_clusters is None:
                n_clusters = max(2, int(np.sqrt(len(segments))))
            
            clusterer = AgglomerativeClustering(
                n_clusters=n_clusters,
                linkage='ward'
            )
            labels = clusterer.fit_predict(features_scaled)
        
        else:
            raise ValueError(f"æœªçŸ¥èšç±»æ–¹æ³•: {self.clustering_method}")
        
        # ç»Ÿè®¡
        n_clusters_found = len(set(labels)) - (1 if -1 in labels else 0)
        n_noise = list(labels).count(-1)
        
        logger.info(f"âœ… èšç±»å®Œæˆ:")
        logger.info(f"   èšç±»æ•°: {n_clusters_found}")
        logger.info(f"   å™ªå£°ç‚¹: {n_noise} ({n_noise/len(labels)*100:.1f}%)")
        
        # è¯„ä¼°èšç±»è´¨é‡
        if n_clusters_found > 1 and n_noise < len(labels):
            try:
                silhouette = silhouette_score(features_scaled, labels)
                db_index = davies_bouldin_score(features_scaled, labels)
                logger.info(f"ğŸ“Š èšç±»è´¨é‡:")
                logger.info(f"   è½®å»“ç³»æ•°: {silhouette:.3f} (è¶Šå¤§è¶Šå¥½)")
                logger.info(f"   DBæŒ‡æ•°: {db_index:.3f} (è¶Šå°è¶Šå¥½)")
            except:
                pass
        
        return labels


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    
    # ç¤ºä¾‹æ•°æ®
    traj1 = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])
    traj2 = np.array([[0, 0.1], [1, 1.1], [2, 2.1], [3, 3.1]])
    
    # 1. è®¡ç®—FrÃ©chetè·ç¦»
    dist = frechet_distance(traj1, traj2)
    print(f"FrÃ©chetè·ç¦»: {dist}")
    
    # 2. æå–å¢å¼ºç‰¹å¾
    segment_attrs = {
        'speeds': [5, 6, 7, 8],
        'accelerations': [0.5, 0.3, 0.2],
        'yaws': [0, 0.1, 0.2, 0.3],
        'yaw_changes': [0.1, 0.1, 0.1]
    }
    features = extract_enhanced_features(traj1, segment_attrs)
    print(f"å¢å¼ºç‰¹å¾ï¼ˆ17ç»´ï¼‰: {features}")
    
    # 3. åˆ›å»ºæ”¹è¿›èšç±»å™¨
    clusterer = ImprovedTrajectoryClusterer(
        distance_metric='frechet',
        clustering_method='dbscan',
        use_enhanced_features=True
    )
    
    # 4. æ‰§è¡Œèšç±»
    segments = [
        {'coords': traj1, 'features': features, 'attrs': segment_attrs},
        {'coords': traj2, 'features': features, 'attrs': segment_attrs},
    ]
    labels = clusterer.cluster(segments, eps=50.0, min_samples=2)
    print(f"èšç±»æ ‡ç­¾: {labels}")


if __name__ == '__main__':
    example_usage()

