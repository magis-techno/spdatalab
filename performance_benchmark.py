#!/usr/bin/env python3
"""
é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•

å¯¹æ¯”ä¸åŒé…ç½®å‚æ•°ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯ä¼˜åŒ–æ•ˆæœ
"""

import json
import logging
import tempfile
import time
from pathlib import Path
from typing import List, Dict

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_benchmark_polygons(count: int) -> str:
    """åˆ›å»ºæ€§èƒ½æµ‹è¯•ç”¨çš„polygoné›†åˆ"""
    # åˆ›å»ºåŒ—äº¬åœ°åŒºçš„å¤šä¸ªæµ‹è¯•polygon
    base_coords = [
        [116.3, 39.9], [116.4, 39.9], [116.4, 40.0], [116.3, 40.0], [116.3, 39.9]
    ]
    
    features = []
    for i in range(count):
        # åœ¨åŸºç¡€åæ ‡ä¸Šæ·»åŠ å°åç§»ï¼Œåˆ›å»ºä¸åŒçš„polygon
        offset_x = (i % 10) * 0.01  # ç»åº¦åç§»
        offset_y = (i // 10) * 0.01  # çº¬åº¦åç§»
        
        coords = [[x + offset_x, y + offset_y] for x, y in base_coords]
        
        feature = {
            "type": "Feature",
            "properties": {
                "id": f"benchmark_area_{i}",
                "name": f"æµ‹è¯•åŒºåŸŸ{i}"
            },
            "geometry": {
                "type": "Polygon",
                "coordinates": [coords]
            }
        }
        features.append(feature)
    
    polygon_collection = {
        "type": "FeatureCollection",
        "features": features
    }
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.geojson', delete=False, encoding='utf-8') as f:
        json.dump(polygon_collection, f, ensure_ascii=False, indent=2)
        temp_file = f.name
    
    logger.info(f"åˆ›å»ºäº†åŒ…å« {count} ä¸ªpolygonçš„æµ‹è¯•æ–‡ä»¶: {temp_file}")
    return temp_file

def run_performance_test(geojson_file: str, config_name: str, config: 'PolygonTrajectoryConfig') -> Dict:
    """è¿è¡Œå•ä¸ªæ€§èƒ½æµ‹è¯•"""
    from src.spdatalab.dataset.polygon_trajectory_query import process_polygon_trajectory_query
    
    logger.info(f"å¼€å§‹æ€§èƒ½æµ‹è¯•: {config_name}")
    logger.info(f"é…ç½®: batch_threshold={config.batch_threshold}, chunk_size={config.chunk_size}")
    
    start_time = time.time()
    
    # åªå†™å…¥æ•°æ®åº“ï¼Œä¸å¯¼å‡ºæ–‡ä»¶
    table_name = f"benchmark_{config_name.lower().replace(' ', '_')}"
    
    stats = process_polygon_trajectory_query(
        geojson_file=geojson_file,
        output_table=table_name,
        config=config
    )
    
    total_time = time.time() - start_time
    
    return {
        'config_name': config_name,
        'total_time': total_time,
        'success': stats.get('success', False),
        'query_stats': stats.get('query_stats', {}),
        'build_stats': stats.get('build_stats', {}),
        'save_stats': stats.get('save_stats', {}),
        'polygon_count': stats.get('polygon_count', 0)
    }

def run_benchmark_suite():
    """è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    from src.spdatalab.dataset.polygon_trajectory_query import PolygonTrajectoryConfig
    
    logger.info("=" * 80)
    logger.info("ğŸš€ å¼€å§‹é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢åŸºå‡†æµ‹è¯•")
    logger.info("=" * 80)
    
    # æµ‹è¯•ä¸åŒæ•°é‡çš„polygon
    polygon_counts = [5, 25, 100]  # å°ã€ä¸­ã€å¤§è§„æ¨¡æµ‹è¯•
    
    # æµ‹è¯•é…ç½®ç»„åˆ
    test_configs = [
        {
            'name': 'é»˜è®¤é…ç½®',
            'config': PolygonTrajectoryConfig()
        },
        {
            'name': 'å°æ‰¹é‡é«˜é¢‘',
            'config': PolygonTrajectoryConfig(
                batch_threshold=10,
                chunk_size=5,
                batch_insert_size=200
            )
        },
        {
            'name': 'å¤§æ‰¹é‡ä½é¢‘',
            'config': PolygonTrajectoryConfig(
                batch_threshold=100,
                chunk_size=50,
                batch_insert_size=2000
            )
        },
        {
            'name': 'è¶…é«˜æ€§èƒ½',
            'config': PolygonTrajectoryConfig(
                batch_threshold=200,
                chunk_size=100,
                batch_insert_size=5000,
                limit_per_polygon=50000
            )
        }
    ]
    
    all_results = []
    
    for polygon_count in polygon_counts:
        logger.info(f"\nğŸ” æµ‹è¯•è§„æ¨¡: {polygon_count} ä¸ªpolygon")
        logger.info("-" * 60)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        geojson_file = create_benchmark_polygons(polygon_count)
        
        try:
            scale_results = []
            
            for test_config in test_configs:
                try:
                    result = run_performance_test(
                        geojson_file=geojson_file,
                        config_name=test_config['name'],
                        config=test_config['config']
                    )
                    result['polygon_count'] = polygon_count
                    scale_results.append(result)
                    
                    # è¾“å‡ºå•ä¸ªæµ‹è¯•ç»“æœ
                    if result['success']:
                        query_time = result['query_stats'].get('query_time', 0)
                        total_points = result['query_stats'].get('total_points', 0)
                        strategy = result['query_stats'].get('strategy', 'unknown')
                        
                        logger.info(f"âœ… {test_config['name']}: {result['total_time']:.2f}s "
                                   f"(æŸ¥è¯¢: {query_time:.2f}s, {total_points:,} ç‚¹, {strategy})")
                    else:
                        logger.error(f"âŒ {test_config['name']}: æµ‹è¯•å¤±è´¥")
                        
                except Exception as e:
                    logger.error(f"âŒ {test_config['name']} æµ‹è¯•å‡ºé”™: {e}")
                    scale_results.append({
                        'config_name': test_config['name'],
                        'polygon_count': polygon_count,
                        'success': False,
                        'error': str(e)
                    })
            
            all_results.extend(scale_results)
            
        finally:
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            Path(geojson_file).unlink(missing_ok=True)
    
    # è¾“å‡ºç»¼åˆåˆ†æ
    print_benchmark_analysis(all_results)
    
    return all_results

def print_benchmark_analysis(results: List[Dict]):
    """è¾“å‡ºåŸºå‡†æµ‹è¯•åˆ†ææŠ¥å‘Š"""
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•åˆ†ææŠ¥å‘Š")
    logger.info("=" * 80)
    
    # æŒ‰polygonæ•°é‡åˆ†ç»„
    by_scale = {}
    for result in results:
        if result.get('success', False):
            scale = result['polygon_count']
            if scale not in by_scale:
                by_scale[scale] = []
            by_scale[scale].append(result)
    
    # å„è§„æ¨¡æ€§èƒ½å¯¹æ¯”
    for scale in sorted(by_scale.keys()):
        scale_results = by_scale[scale]
        logger.info(f"\nğŸ¯ {scale} ä¸ªpolygonæ€§èƒ½å¯¹æ¯”:")
        
        # æŒ‰æ€»æ—¶é—´æ’åº
        scale_results.sort(key=lambda x: x['total_time'])
        
        best_time = scale_results[0]['total_time']
        
        for i, result in enumerate(scale_results):
            config_name = result['config_name']
            total_time = result['total_time']
            speedup = best_time / total_time
            
            query_stats = result.get('query_stats', {})
            strategy = query_stats.get('strategy', 'unknown')
            total_points = query_stats.get('total_points', 0)
            
            rank_emoji = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "ğŸ“Š"][min(i, 3)]
            
            logger.info(f"   {rank_emoji} {config_name}: {total_time:.2f}s "
                       f"({speedup:.1f}x) - {strategy}, {total_points:,} ç‚¹")
    
    # ç­–ç•¥æ•ˆæœåˆ†æ
    logger.info(f"\nğŸ“ˆ æŸ¥è¯¢ç­–ç•¥æ•ˆæœåˆ†æ:")
    
    batch_results = [r for r in results if r.get('success') and 
                    r.get('query_stats', {}).get('strategy') == 'batch_query']
    chunked_results = [r for r in results if r.get('success') and 
                      r.get('query_stats', {}).get('strategy') == 'chunked_query']
    
    if batch_results:
        avg_batch_time = sum(r['query_stats']['query_time'] for r in batch_results) / len(batch_results)
        logger.info(f"   ğŸ”— æ‰¹é‡æŸ¥è¯¢å¹³å‡ç”¨æ—¶: {avg_batch_time:.2f}s ({len(batch_results)} æ¬¡æµ‹è¯•)")
    
    if chunked_results:
        avg_chunked_time = sum(r['query_stats']['query_time'] for r in chunked_results) / len(chunked_results)
        logger.info(f"   ğŸ“¦ åˆ†å—æŸ¥è¯¢å¹³å‡ç”¨æ—¶: {avg_chunked_time:.2f}s ({len(chunked_results)} æ¬¡æµ‹è¯•)")
    
    # æœ€ä½³é…ç½®æ¨è
    logger.info(f"\nğŸ† æœ€ä½³é…ç½®æ¨è:")
    
    for scale in sorted(by_scale.keys()):
        best_result = min(by_scale[scale], key=lambda x: x['total_time'])
        logger.info(f"   â€¢ {scale} ä¸ªpolygon: {best_result['config_name']} "
                   f"({best_result['total_time']:.2f}s)")
    
    logger.info("\n" + "=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥ä¾èµ–
        try:
            from src.spdatalab.dataset.polygon_trajectory_query import PolygonTrajectoryConfig
            logger.info("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            logger.error(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            return 1
        
        print("ğŸ§ª é«˜æ€§èƒ½Polygonè½¨è¿¹æŸ¥è¯¢åŸºå‡†æµ‹è¯•")
        print("\næ³¨æ„äº‹é¡¹:")
        print("â€¢ æ­¤æµ‹è¯•å°†åˆ›å»ºå¤šä¸ªæ•°æ®åº“è¡¨ç”¨äºæ€§èƒ½æµ‹è¯•")
        print("â€¢ æµ‹è¯•å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…")
        print("â€¢ ç¡®ä¿æ•°æ®åº“è¿æ¥æ­£å¸¸ä¸”æœ‰è¶³å¤Ÿç©ºé—´")
        
        user_input = input("\næ˜¯å¦å¼€å§‹åŸºå‡†æµ‹è¯•ï¼Ÿ(y/N): ")
        
        if user_input.lower() not in ['y', 'yes']:
            print("å–æ¶ˆåŸºå‡†æµ‹è¯•")
            return 0
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        results = run_benchmark_suite()
        
        # ä¿å­˜ç»“æœ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = f"benchmark_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ“‹ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        logger.info("ğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆï¼")
        return 0
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        return 1
    except Exception as e:
        logger.error(f"åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    exit(main()) 