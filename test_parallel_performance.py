#!/usr/bin/env python3
"""
å¹¶è¡Œå¤„ç†æ€§èƒ½æµ‹è¯•è„šæœ¬

ç”¨äºæµ‹è¯•å’Œå¯¹æ¯”é¡ºåºå¤„ç†ä¸å¹¶è¡Œå¤„ç†çš„æ€§èƒ½å·®å¼‚
"""

import time
import sys
import argparse
from pathlib import Path

def test_sequential_processing(dataset_file, batch_size=1000, insert_batch_size=1000):
    """æµ‹è¯•é¡ºåºå¤„ç†æ€§èƒ½"""
    print("ğŸ”„ å¼€å§‹é¡ºåºå¤„ç†æµ‹è¯•...")
    
    try:
        from src.spdatalab.dataset.bbox import run_with_partitioning
        
        start_time = time.time()
        
        run_with_partitioning(
            input_path=dataset_file,
            batch=batch_size,
            insert_batch=insert_batch_size,
            work_dir="./test_logs/sequential",
            create_unified_view_flag=True,
            maintain_view_only=False,
            use_parallel=False
        )
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f"âœ… é¡ºåºå¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f} ç§’")
        return processing_time
        
    except Exception as e:
        print(f"âŒ é¡ºåºå¤„ç†å¤±è´¥: {str(e)}")
        return None

def test_parallel_processing(dataset_file, batch_size=1000, insert_batch_size=1000, max_workers=None):
    """æµ‹è¯•å¹¶è¡Œå¤„ç†æ€§èƒ½"""
    print(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†æµ‹è¯• (workers: {max_workers or 'auto'})...")
    
    try:
        from src.spdatalab.dataset.bbox import run_with_partitioning
        
        start_time = time.time()
        
        run_with_partitioning(
            input_path=dataset_file,
            batch=batch_size,
            insert_batch=insert_batch_size,
            work_dir="./test_logs/parallel",
            create_unified_view_flag=True,
            maintain_view_only=False,
            use_parallel=True,
            max_workers=max_workers
        )
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f"âœ… å¹¶è¡Œå¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f} ç§’")
        return processing_time
        
    except Exception as e:
        print(f"âŒ å¹¶è¡Œå¤„ç†å¤±è´¥: {str(e)}")
        return None

def compare_performance(sequential_time, parallel_time):
    """æ¯”è¾ƒæ€§èƒ½å·®å¼‚"""
    if sequential_time and parallel_time:
        speedup = sequential_time / parallel_time
        time_saved = sequential_time - parallel_time
        
        print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
        print(f"  - é¡ºåºå¤„ç†: {sequential_time:.2f} ç§’")
        print(f"  - å¹¶è¡Œå¤„ç†: {parallel_time:.2f} ç§’")
        print(f"  - æ€§èƒ½æå‡: {speedup:.2f}x")
        print(f"  - èŠ‚çœæ—¶é—´: {time_saved:.2f} ç§’")
        
        if speedup > 1:
            print(f"ğŸ‰ å¹¶è¡Œå¤„ç†æ•ˆæœæ˜¾è‘—ï¼Œæå‡ {speedup:.1f} å€!")
        else:
            print(f"âš ï¸  å¹¶è¡Œå¤„ç†æœªæ˜¾ç¤ºæ˜æ˜¾ä¼˜åŠ¿ï¼Œå¯èƒ½æ•°æ®é‡å¤ªå°æˆ–å­˜åœ¨ç“¶é¢ˆ")
    else:
        print("âŒ æ— æ³•è¿›è¡Œæ€§èƒ½å¯¹æ¯”ï¼ŒæŸä¸ªæµ‹è¯•å¤±è´¥")

def main():
    parser = argparse.ArgumentParser(description="å¹¶è¡Œå¤„ç†æ€§èƒ½æµ‹è¯•")
    parser.add_argument('--dataset-file', required=True, help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--batch-size', type=int, default=1000, help='å¤„ç†æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--insert-batch-size', type=int, default=1000, help='æ’å…¥æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max-workers', type=int, help='æœ€å¤§å¹¶è¡Œworkeræ•°é‡')
    parser.add_argument('--test-mode', choices=['sequential', 'parallel', 'both'], 
                       default='both', help='æµ‹è¯•æ¨¡å¼')
    parser.add_argument('--skip-cleanup', action='store_true', help='è·³è¿‡æ¸…ç†æ­¥éª¤')
    
    args = parser.parse_args()
    
    # éªŒè¯æ•°æ®é›†æ–‡ä»¶å­˜åœ¨
    if not Path(args.dataset_file).exists():
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {args.dataset_file}")
        return
    
    print("ğŸ§ª å¹¶è¡Œå¤„ç†æ€§èƒ½æµ‹è¯•å¼€å§‹")
    print("=" * 60)
    print(f"æ•°æ®é›†æ–‡ä»¶: {args.dataset_file}")
    print(f"å¤„ç†æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"æ’å…¥æ‰¹æ¬¡å¤§å°: {args.insert_batch_size}")
    print(f"æµ‹è¯•æ¨¡å¼: {args.test_mode}")
    
    # åˆ›å»ºæµ‹è¯•æ—¥å¿—ç›®å½•
    Path("./test_logs/sequential").mkdir(parents=True, exist_ok=True)
    Path("./test_logs/parallel").mkdir(parents=True, exist_ok=True)
    
    sequential_time = None
    parallel_time = None
    
    # æ‰§è¡Œæµ‹è¯•
    if args.test_mode in ['sequential', 'both']:
        print("\n" + "="*60)
        sequential_time = test_sequential_processing(
            args.dataset_file, args.batch_size, args.insert_batch_size
        )
    
    if args.test_mode in ['parallel', 'both']:
        print("\n" + "="*60)
        parallel_time = test_parallel_processing(
            args.dataset_file, args.batch_size, args.insert_batch_size, args.max_workers
        )
    
    # æ¯”è¾ƒç»“æœ
    if args.test_mode == 'both':
        print("\n" + "="*60)
        compare_performance(sequential_time, parallel_time)
    
    print("\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main() 