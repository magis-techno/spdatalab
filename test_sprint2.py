#!/usr/bin/env python3
"""
Sprint 2 åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•åˆ†è¡¨æ¨¡å¼å’Œç»Ÿä¸€è§†å›¾åŠŸèƒ½ï¼š
1. åˆ†è¡¨æ¨¡å¼æ•°æ®å¤„ç†
2. ç»Ÿä¸€è§†å›¾åˆ›å»ºå’Œç»´æŠ¤
3. è·¨æ•°æ®é›†æŸ¥è¯¢éªŒè¯
"""

import sys
import os
sys.path.insert(0, 'src')

import argparse
from pathlib import Path
from sqlalchemy import create_engine, text
from src.spdatalab.dataset.bbox import (
    run_with_partitioning,
    create_unified_view,
    maintain_unified_view,
    list_bbox_tables,
    group_scenes_by_subdataset,
    batch_create_tables_for_subdatasets
)

# æ•°æ®åº“è¿æ¥é…ç½®
LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"

def test_partitioning_workflow(dataset_file: str, work_dir: str = "./test_sprint2_logs"):
    """æµ‹è¯•å®Œæ•´çš„åˆ†è¡¨å·¥ä½œæµç¨‹
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
        work_dir: å·¥ä½œç›®å½•
    """
    print("=" * 60)
    print("ğŸ¯ æµ‹è¯•Sprint 2: åˆ†è¡¨æ¨¡å¼å·¥ä½œæµç¨‹")
    print("=" * 60)
    
    try:
        # æ­¥éª¤1: æµ‹è¯•åœºæ™¯åˆ†ç»„åŠŸèƒ½
        print("\n=== æ­¥éª¤1: æµ‹è¯•åœºæ™¯åˆ†ç»„ ===")
        scene_groups = group_scenes_by_subdataset(dataset_file)
        
        if not scene_groups:
            print("âŒ åœºæ™¯åˆ†ç»„å¤±è´¥ï¼Œæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
            return False
        
        print(f"âœ… åœºæ™¯åˆ†ç»„æˆåŠŸï¼Œæ‰¾åˆ° {len(scene_groups)} ä¸ªå­æ•°æ®é›†")
        
        # æ˜¾ç¤ºåˆ†ç»„ç»Ÿè®¡
        total_scenes = sum(len(scenes) for scenes in scene_groups.values())
        print(f"   æ€»åœºæ™¯æ•°: {total_scenes}")
        
        # é™åˆ¶æµ‹è¯•è§„æ¨¡ï¼ˆåªå¤„ç†å‰3ä¸ªå­æ•°æ®é›†çš„å°‘é‡æ•°æ®ï¼‰
        limited_groups = {}
        for i, (name, scenes) in enumerate(scene_groups.items()):
            if i >= 3:  # åªå¤„ç†å‰3ä¸ªå­æ•°æ®é›†
                break
            # æ¯ä¸ªå­æ•°æ®é›†æœ€å¤šæµ‹è¯•10ä¸ªåœºæ™¯
            limited_scenes = scenes[:10]
            limited_groups[name] = limited_scenes
            print(f"   - {name}: {len(limited_scenes)} ä¸ªåœºæ™¯ï¼ˆæµ‹è¯•ç”¨ï¼‰")
        
        # æ­¥éª¤2: æµ‹è¯•åˆ†è¡¨åˆ›å»º
        print("\n=== æ­¥éª¤2: æµ‹è¯•åˆ†è¡¨åˆ›å»º ===")
        eng = create_engine(LOCAL_DSN, future=True)
        table_mapping = batch_create_tables_for_subdatasets(eng, list(limited_groups.keys()))
        
        if not table_mapping:
            print("âŒ åˆ†è¡¨åˆ›å»ºå¤±è´¥")
            return False
        
        print(f"âœ… åˆ†è¡¨åˆ›å»ºæˆåŠŸï¼Œåˆ›å»ºäº† {len(table_mapping)} ä¸ªåˆ†è¡¨")
        for subdataset, table_name in table_mapping.items():
            print(f"   {subdataset} -> {table_name}")
        
        # æ­¥éª¤3: æµ‹è¯•ç»Ÿä¸€è§†å›¾åˆ›å»º
        print("\n=== æ­¥éª¤3: æµ‹è¯•ç»Ÿä¸€è§†å›¾åˆ›å»º ===")
        view_success = create_unified_view(eng, "clips_bbox_unified_test")
        
        if view_success:
            print("âœ… ç»Ÿä¸€è§†å›¾åˆ›å»ºæˆåŠŸ")
        else:
            print("âŒ ç»Ÿä¸€è§†å›¾åˆ›å»ºå¤±è´¥")
            return False
        
        # æ­¥éª¤4: æµ‹è¯•è§†å›¾æŸ¥è¯¢
        print("\n=== æ­¥éª¤4: æµ‹è¯•ç»Ÿä¸€è§†å›¾æŸ¥è¯¢ ===")
        test_view_query(eng, "clips_bbox_unified_test")
        
        # æ­¥éª¤5: æµ‹è¯•è¡¨ç®¡ç†åŠŸèƒ½
        print("\n=== æ­¥éª¤5: æµ‹è¯•è¡¨ç®¡ç†åŠŸèƒ½ ===")
        bbox_tables = list_bbox_tables(eng)
        print(f"å½“å‰bboxè¡¨æ•°é‡: {len(bbox_tables)}")
        for table in bbox_tables:
            print(f"   - {table}")
        
        # æ­¥éª¤6: æµ‹è¯•è§†å›¾ç»´æŠ¤
        print("\n=== æ­¥éª¤6: æµ‹è¯•ç»Ÿä¸€è§†å›¾ç»´æŠ¤ ===")
        maintain_success = maintain_unified_view(eng, "clips_bbox_unified_test")
        
        if maintain_success:
            print("âœ… ç»Ÿä¸€è§†å›¾ç»´æŠ¤æˆåŠŸ")
        else:
            print("âŒ ç»Ÿä¸€è§†å›¾ç»´æŠ¤å¤±è´¥")
            return False
        
        print("\n" + "=" * 60)
        print("ğŸ‰ Sprint 2 æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_view_query(eng, view_name: str):
    """æµ‹è¯•ç»Ÿä¸€è§†å›¾æŸ¥è¯¢åŠŸèƒ½
    
    Args:
        eng: æ•°æ®åº“å¼•æ“
        view_name: è§†å›¾åç§°
    """
    try:
        # æµ‹è¯•åŸºæœ¬æŸ¥è¯¢
        query_sql = text(f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT subdataset_name) as subdataset_count,
                COUNT(DISTINCT source_table) as table_count
            FROM {view_name};
        """)
        
        with eng.connect() as conn:
            result = conn.execute(query_sql)
            row = result.fetchone()
            
            print(f"   è§†å›¾æŸ¥è¯¢ç»“æœ:")
            print(f"   - æ€»è®°å½•æ•°: {row[0]}")
            print(f"   - å­æ•°æ®é›†æ•°: {row[1]}")
            print(f"   - æºè¡¨æ•°: {row[2]}")
        
        # æµ‹è¯•åˆ†ç»„æŸ¥è¯¢
        group_query_sql = text(f"""
            SELECT 
                subdataset_name,
                source_table,
                COUNT(*) as record_count
            FROM {view_name}
            GROUP BY subdataset_name, source_table
            ORDER BY subdataset_name
            LIMIT 10;
        """)
        
        with eng.connect() as conn:
            result = conn.execute(group_query_sql)
            rows = result.fetchall()
            
            print(f"   æŒ‰å­æ•°æ®é›†åˆ†ç»„æŸ¥è¯¢ç»“æœ:")
            for row in rows:
                print(f"   - {row[0]}: {row[2]} æ¡è®°å½• (è¡¨: {row[1]})")
        
        print("âœ… ç»Ÿä¸€è§†å›¾æŸ¥è¯¢æµ‹è¯•æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ ç»Ÿä¸€è§†å›¾æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {str(e)}")

def test_small_scale_processing(dataset_file: str):
    """æµ‹è¯•å°è§„æ¨¡æ•°æ®çš„åˆ†è¡¨å¤„ç†
    
    Args:
        dataset_file: æ•°æ®é›†æ–‡ä»¶è·¯å¾„
    """
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•å°è§„æ¨¡åˆ†è¡¨æ•°æ®å¤„ç†")
    print("=" * 60)
    
    try:
        # ä½¿ç”¨åˆ†è¡¨æ¨¡å¼å¤„ç†å°è§„æ¨¡æ•°æ®
        run_with_partitioning(
            input_path=dataset_file,
            batch=10,  # å°æ‰¹æ¬¡
            insert_batch=10,  # å°æ’å…¥æ‰¹æ¬¡
            work_dir="./test_sprint2_processing",
            create_unified_view_flag=True,
            maintain_view_only=False
        )
        
        print("âœ… å°è§„æ¨¡åˆ†è¡¨å¤„ç†æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ å°è§„æ¨¡åˆ†è¡¨å¤„ç†æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def cleanup_test_resources():
    """æ¸…ç†æµ‹è¯•èµ„æº"""
    print("\n=== æ¸…ç†æµ‹è¯•èµ„æº ===")
    
    try:
        eng = create_engine(LOCAL_DSN, future=True)
        
        # åˆ é™¤æµ‹è¯•è§†å›¾
        with eng.connect() as conn:
            conn.execute(text("DROP VIEW IF EXISTS clips_bbox_unified_test;"))
            conn.commit()
        
        print("âœ… æµ‹è¯•èµ„æºæ¸…ç†å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ¸…ç†æµ‹è¯•èµ„æºå¤±è´¥: {str(e)}")

def main():
    parser = argparse.ArgumentParser(description="Sprint 2 åŠŸèƒ½æµ‹è¯•")
    parser.add_argument(
        '--dataset-file', 
        required=True,
        help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆJSON/Parquetæ ¼å¼ï¼‰'
    )
    parser.add_argument(
        '--test-processing', 
        action='store_true',
        help='æµ‹è¯•å°è§„æ¨¡æ•°æ®å¤„ç†ï¼ˆéœ€è¦å®é™…æ•°æ®åº“è¿æ¥ï¼‰'
    )
    parser.add_argument(
        '--cleanup', 
        action='store_true',
        help='æ¸…ç†æµ‹è¯•èµ„æº'
    )
    
    args = parser.parse_args()
    
    if args.cleanup:
        cleanup_test_resources()
        return
    
    if not Path(args.dataset_file).exists():
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {args.dataset_file}")
        sys.exit(1)
    
    # æµ‹è¯•æ ¸å¿ƒåŠŸèƒ½
    success = test_partitioning_workflow(args.dataset_file)
    
    if not success:
        print("\nâŒ Sprint 2 æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•å¤±è´¥")
        sys.exit(1)
    
    # å¯é€‰çš„æ•°æ®å¤„ç†æµ‹è¯•
    if args.test_processing:
        print("\n" + "ğŸ”„ å¼€å§‹æ•°æ®å¤„ç†æµ‹è¯•...")
        processing_success = test_small_scale_processing(args.dataset_file)
        
        if not processing_success:
            print("\nâŒ æ•°æ®å¤„ç†æµ‹è¯•å¤±è´¥")
            sys.exit(1)
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼")
    else:
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ --test-processing å‚æ•°å¯ä»¥æµ‹è¯•å®é™…çš„æ•°æ®å¤„ç†åŠŸèƒ½")
    
    print("\n" + "=" * 60)
    print("âœ… Sprint 2 æµ‹è¯•å®Œæˆ")
    print("=" * 60)

if __name__ == "__main__":
    main() 