#!/usr/bin/env python3
"""
åˆ†è¡¨åŠŸèƒ½æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ Sprint 1 çš„åŸºç¡€åŠŸèƒ½
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ° Python path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from sqlalchemy import create_engine, text
from src.spdatalab.dataset.bbox import (
    normalize_subdataset_name,
    get_table_name_for_subdataset,
    create_table_for_subdataset,
    group_scenes_by_subdataset,
    batch_create_tables_for_subdatasets
)

# æ•°æ®åº“è¿æ¥
LOCAL_DSN = "postgresql+psycopg://postgres:postgres@local_pg:5432/postgres"

def test_name_normalization():
    """æµ‹è¯•å­æ•°æ®é›†åç§°è§„èŒƒåŒ–"""
    print("=== æµ‹è¯•å­æ•°æ®é›†åç§°è§„èŒƒåŒ– ===")
    
    test_cases = [
        "GOD_E2E_lane_change_1_sub_ddi_2773412e2e_2025_05_18_11_07_59",
        "GOD_E2E_simple_dataset",
        "normal_dataset_name_2025_01_01_12_00_00",
        "dataset_with_sub_ddi_extra_info_2025_05_18_11_07_59",
        "short_name",
        "very_very_very_long_dataset_name_that_exceeds_normal_length_limits_2025_05_18_11_07_59",
        # æ–°å¢æµ‹è¯•ç”¨ä¾‹ï¼šåŒ…å«277...e2eæ¨¡å¼
        "GOD_E2E_lane_change_1_277736e2e_2025_05_18_11_07_59",
        "dataset_with_277abc123e2e_suffix",
        "normal_dataset_277abcde2e_with_timestamp_2025_01_01_12_00_00",
        "complex_GOD_E2E_name_277hash123e2e_sub_ddi_extra_2025_05_18_11_07_59"
    ]
    
    print("æµ‹è¯•ç”¨ä¾‹å’Œç»“æœ:")
    for i, original in enumerate(test_cases, 1):
        normalized = normalize_subdataset_name(original)
        table_name = get_table_name_for_subdataset(original)
        print(f"{i}. åŸå§‹: {original}")
        print(f"   è§„èŒƒåŒ–: {normalized}")
        print(f"   è¡¨å: {table_name}")
        print(f"   è¡¨åé•¿åº¦: {len(table_name)}")
        print()
    
    print("âœ… åç§°è§„èŒƒåŒ–æµ‹è¯•å®Œæˆ\n")

def test_table_creation():
    """æµ‹è¯•åˆ†è¡¨åˆ›å»ºåŠŸèƒ½"""
    print("=== æµ‹è¯•åˆ†è¡¨åˆ›å»ºåŠŸèƒ½ ===")
    
    try:
        eng = create_engine(LOCAL_DSN, future=True)
        
        # æµ‹è¯•ç”¨ä¾‹ï¼šåˆ›å»ºå‡ ä¸ªæµ‹è¯•åˆ†è¡¨
        test_subdatasets = [
            "GOD_E2E_test_dataset_1_2025_01_01_12_00_00",
            "test_dataset_2_sub_ddi_extra",
            "simple_test"
        ]
        
        print("åˆ›å»ºæµ‹è¯•åˆ†è¡¨:")
        results = {}
        for subdataset in test_subdatasets:
            print(f"åˆ›å»ºåˆ†è¡¨: {subdataset}")
            success, table_name = create_table_for_subdataset(eng, subdataset)
            results[subdataset] = (success, table_name)
            
        # éªŒè¯è¡¨æ˜¯å¦åˆ›å»ºæˆåŠŸ
        print("\néªŒè¯è¡¨åˆ›å»ºç»“æœ:")
        with eng.connect() as conn:
            for subdataset, (success, table_name) in results.items():
                if success:
                    # æ£€æŸ¥è¡¨ç»“æ„
                    check_sql = text(f"""
                        SELECT column_name, data_type 
                        FROM information_schema.columns 
                        WHERE table_name = '{table_name}' 
                        ORDER BY ordinal_position;
                    """)
                    result = conn.execute(check_sql)
                    columns = result.fetchall()
                    print(f"âœ… {table_name}: {len(columns)} åˆ—")
                    
                    # æ£€æŸ¥ç´¢å¼•
                    index_sql = text(f"""
                        SELECT indexname 
                        FROM pg_indexes 
                        WHERE tablename = '{table_name}';
                    """)
                    result = conn.execute(index_sql)
                    indexes = result.fetchall()
                    print(f"   ç´¢å¼•æ•°é‡: {len(indexes)}")
                else:
                    print(f"âŒ {subdataset}: åˆ›å»ºå¤±è´¥")
        
        print("âœ… åˆ†è¡¨åˆ›å»ºæµ‹è¯•å®Œæˆ\n")
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†è¡¨åˆ›å»ºæµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_scene_grouping(dataset_file: str = None):
    """æµ‹è¯•åœºæ™¯åˆ†ç»„åŠŸèƒ½"""
    print("=== æµ‹è¯•åœºæ™¯åˆ†ç»„åŠŸèƒ½ ===")
    
    if not dataset_file:
        print("âš ï¸  æœªæä¾›datasetæ–‡ä»¶ï¼Œè·³è¿‡åœºæ™¯åˆ†ç»„æµ‹è¯•")
        print("   ä½¿ç”¨æ–¹æ³•: python test_partitioning.py --dataset-file path/to/dataset.json")
        return True
    
    if not os.path.exists(dataset_file):
        print(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
        return False
    
    try:
        print(f"åŠ è½½æ•°æ®é›†æ–‡ä»¶: {dataset_file}")
        groups = group_scenes_by_subdataset(dataset_file)
        
        print(f"\nåˆ†ç»„ç»“æœç»Ÿè®¡:")
        print(f"å­æ•°æ®é›†æ•°é‡: {len(groups)}")
        
        total_scenes = sum(len(scenes) for scenes in groups.values())
        print(f"æ€»åœºæ™¯æ•°: {total_scenes}")
        
        # æ˜¾ç¤ºå‰5ä¸ªå­æ•°æ®é›†çš„è¯¦æƒ…
        print(f"\nå‰5ä¸ªå­æ•°æ®é›†è¯¦æƒ…:")
        for i, (name, scenes) in enumerate(list(groups.items())[:5]):
            normalized = normalize_subdataset_name(name)
            table_name = get_table_name_for_subdataset(name)
            print(f"{i+1}. {name}")
            print(f"   åœºæ™¯æ•°: {len(scenes)}")
            print(f"   è¡¨å: {table_name}")
        
        if len(groups) > 5:
            print(f"   ... è¿˜æœ‰ {len(groups) - 5} ä¸ªå­æ•°æ®é›†")
        
        print("âœ… åœºæ™¯åˆ†ç»„æµ‹è¯•å®Œæˆ\n")
        return True
        
    except Exception as e:
        print(f"âŒ åœºæ™¯åˆ†ç»„æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_batch_table_creation(dataset_file: str = None):
    """æµ‹è¯•æ‰¹é‡åˆ›å»ºåˆ†è¡¨åŠŸèƒ½"""
    print("=== æµ‹è¯•æ‰¹é‡åˆ›å»ºåˆ†è¡¨åŠŸèƒ½ ===")
    
    if not dataset_file:
        print("âš ï¸  æœªæä¾›datasetæ–‡ä»¶ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•")
        # ä½¿ç”¨æ¨¡æ‹Ÿçš„å­æ•°æ®é›†åç§°
        subdataset_names = [
            "GOD_E2E_mock_dataset_1_2025_01_01_12_00_00",
            "mock_dataset_2_sub_ddi_extra",
            "simple_mock_dataset"
        ]
    else:
        try:
            groups = group_scenes_by_subdataset(dataset_file)
            # åªå–å‰3ä¸ªå­æ•°æ®é›†è¿›è¡Œæµ‹è¯•
            subdataset_names = list(groups.keys())[:3]
            print(f"ä½¿ç”¨çœŸå®æ•°æ®é›†çš„å‰ {len(subdataset_names)} ä¸ªå­æ•°æ®é›†è¿›è¡Œæµ‹è¯•")
        except Exception as e:
            print(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            subdataset_names = [
                "GOD_E2E_mock_dataset_1_2025_01_01_12_00_00",
                "mock_dataset_2_sub_ddi_extra",
                "simple_mock_dataset"
            ]
    
    try:
        eng = create_engine(LOCAL_DSN, future=True)
        
        print(f"æ‰¹é‡åˆ›å»º {len(subdataset_names)} ä¸ªåˆ†è¡¨:")
        table_mapping = batch_create_tables_for_subdatasets(eng, subdataset_names)
        
        print(f"\næ‰¹é‡åˆ›å»ºç»“æœ:")
        for original_name, table_name in table_mapping.items():
            print(f"  {original_name} -> {table_name}")
        
        print("âœ… æ‰¹é‡åˆ›å»ºåˆ†è¡¨æµ‹è¯•å®Œæˆ\n")
        return True
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡åˆ›å»ºåˆ†è¡¨æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def cleanup_test_tables():
    """æ¸…ç†æµ‹è¯•äº§ç”Ÿçš„è¡¨"""
    print("=== æ¸…ç†æµ‹è¯•è¡¨ ===")
    
    try:
        eng = create_engine(LOCAL_DSN, future=True)
        
        # æŸ¥æ‰¾æ‰€æœ‰æµ‹è¯•ç›¸å…³çš„è¡¨
        with eng.connect() as conn:
            find_tables_sql = text("""
                SELECT table_name 
                FROM information_schema.tables 
                WHERE table_schema = 'public' 
                  AND (table_name LIKE 'clips_bbox_test_%' 
                       OR table_name LIKE 'clips_bbox_mock_%'
                       OR table_name LIKE 'clips_bbox_simple_test%')
                  AND table_type = 'BASE TABLE';
            """)
            
            result = conn.execute(find_tables_sql)
            test_tables = [row[0] for row in result.fetchall()]
            
            if test_tables:
                print(f"æ‰¾åˆ° {len(test_tables)} ä¸ªæµ‹è¯•è¡¨ï¼Œå¼€å§‹æ¸…ç†:")
                for table_name in test_tables:
                    drop_sql = text(f"DROP TABLE IF EXISTS {table_name} CASCADE;")
                    conn.execute(drop_sql)
                    print(f"  åˆ é™¤: {table_name}")
                
                conn.commit()
                print("âœ… æµ‹è¯•è¡¨æ¸…ç†å®Œæˆ")
            else:
                print("æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æµ‹è¯•è¡¨")
                
    except Exception as e:
        print(f"âŒ æ¸…ç†æµ‹è¯•è¡¨å¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='åˆ†è¡¨åŠŸèƒ½æµ‹è¯•è„šæœ¬')
    parser.add_argument('--dataset-file', help='æ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--cleanup', action='store_true', help='æ¸…ç†æµ‹è¯•è¡¨å¹¶é€€å‡º')
    parser.add_argument('--skip-db-tests', action='store_true', help='è·³è¿‡éœ€è¦æ•°æ®åº“çš„æµ‹è¯•')
    
    args = parser.parse_args()
    
    if args.cleanup:
        cleanup_test_tables()
        return
    
    print("ğŸš€ å¼€å§‹ Sprint 1 åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•1: åç§°è§„èŒƒåŒ–ï¼ˆä¸éœ€è¦æ•°æ®åº“ï¼‰
    test_name_normalization()
    
    if args.skip_db_tests:
        print("âš ï¸  è·³è¿‡æ•°æ®åº“ç›¸å…³æµ‹è¯•")
        return
    
    # æµ‹è¯•2: åˆ†è¡¨åˆ›å»º
    success1 = test_table_creation()
    
    # æµ‹è¯•3: åœºæ™¯åˆ†ç»„
    success2 = test_scene_grouping(args.dataset_file)
    
    # æµ‹è¯•4: æ‰¹é‡åˆ›å»ºåˆ†è¡¨
    success3 = test_batch_table_creation(args.dataset_file)
    
    # æ¸…ç†æµ‹è¯•è¡¨
    cleanup_test_tables()
    
    print("=" * 50)
    if all([success1, success2, success3]):
        print("ğŸ‰ Sprint 1 åŠŸèƒ½æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        print("âœ… å¯ä»¥è¿›å…¥ Sprint 1 éªŒæ”¶é˜¶æ®µ")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤åé‡æ–°æµ‹è¯•")
    
    print("=" * 50)

if __name__ == "__main__":
    main() 